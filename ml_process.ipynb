{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEC 478 Final Code Submission\n",
    "Grace Wang and Didi Zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import plot_tree\n",
    "import random\n",
    "from sklearn.impute import KNNImputer\n",
    "import math\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation for Morph Embeddings\n",
    "Use kNN Imputation to fill in missing pre- morph embeddings. (Takes a while to run, but we have provided the results file which can be loaded in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in training data on each ADP / potential synapse\n",
    "data = pd.read_csv(\"Data/train_data.csv\")\n",
    "\n",
    "#load in morph embedding df\n",
    "morph_embeddings = pd.read_csv(\"Data/morph_embeddings.csv\")\n",
    "\n",
    "# merge ADP data with morph embedding data\n",
    "full_data = (\n",
    "    data.merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# get only the morph embeddings back out (some pre-morph embeddings\n",
    "# will be null)\n",
    "morph_embed = full_data.filter(regex=\"_morph_emb_\")\n",
    "\n",
    "# complete the imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_morph_embed = imputer.fit_transform(morph_embed)\n",
    "imputed_morph_embed_df = pd.DataFrame(imputed_morph_embed)\n",
    "csv_imputed_morph_embed_df = imputed_morph_embed_df.copy()\n",
    "\n",
    "# save to CSV\n",
    "csv_imputed_morph_embed_df.to_csv('Data/imputed_morph_embed.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_morph_emb_0</th>\n",
       "      <th>pre_morph_emb_1</th>\n",
       "      <th>pre_morph_emb_2</th>\n",
       "      <th>pre_morph_emb_3</th>\n",
       "      <th>pre_morph_emb_4</th>\n",
       "      <th>pre_morph_emb_5</th>\n",
       "      <th>pre_morph_emb_6</th>\n",
       "      <th>pre_morph_emb_7</th>\n",
       "      <th>pre_morph_emb_8</th>\n",
       "      <th>pre_morph_emb_9</th>\n",
       "      <th>...</th>\n",
       "      <th>post_morph_emb_22</th>\n",
       "      <th>post_morph_emb_23</th>\n",
       "      <th>post_morph_emb_24</th>\n",
       "      <th>post_morph_emb_25</th>\n",
       "      <th>post_morph_emb_26</th>\n",
       "      <th>post_morph_emb_27</th>\n",
       "      <th>post_morph_emb_28</th>\n",
       "      <th>post_morph_emb_29</th>\n",
       "      <th>post_morph_emb_30</th>\n",
       "      <th>post_morph_emb_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.373316</td>\n",
       "      <td>0.209818</td>\n",
       "      <td>-0.546946</td>\n",
       "      <td>0.630883</td>\n",
       "      <td>0.832248</td>\n",
       "      <td>-0.983688</td>\n",
       "      <td>1.085743</td>\n",
       "      <td>-0.395466</td>\n",
       "      <td>-1.151271</td>\n",
       "      <td>-0.495176</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064851</td>\n",
       "      <td>-0.816273</td>\n",
       "      <td>-0.215224</td>\n",
       "      <td>-0.598382</td>\n",
       "      <td>0.545335</td>\n",
       "      <td>-0.525224</td>\n",
       "      <td>0.171648</td>\n",
       "      <td>1.022962</td>\n",
       "      <td>-0.645146</td>\n",
       "      <td>-0.687774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pre_morph_emb_0  pre_morph_emb_1  pre_morph_emb_2  pre_morph_emb_3  \\\n",
       "0         0.373316         0.209818        -0.546946         0.630883   \n",
       "1         0.373316         0.209818        -0.546946         0.630883   \n",
       "2         0.373316         0.209818        -0.546946         0.630883   \n",
       "3         0.373316         0.209818        -0.546946         0.630883   \n",
       "4         0.373316         0.209818        -0.546946         0.630883   \n",
       "\n",
       "   pre_morph_emb_4  pre_morph_emb_5  pre_morph_emb_6  pre_morph_emb_7  \\\n",
       "0         0.832248        -0.983688         1.085743        -0.395466   \n",
       "1         0.832248        -0.983688         1.085743        -0.395466   \n",
       "2         0.832248        -0.983688         1.085743        -0.395466   \n",
       "3         0.832248        -0.983688         1.085743        -0.395466   \n",
       "4         0.832248        -0.983688         1.085743        -0.395466   \n",
       "\n",
       "   pre_morph_emb_8  pre_morph_emb_9  ...  post_morph_emb_22  \\\n",
       "0        -1.151271        -0.495176  ...          -1.064851   \n",
       "1        -1.151271        -0.495176  ...          -1.064851   \n",
       "2        -1.151271        -0.495176  ...          -1.064851   \n",
       "3        -1.151271        -0.495176  ...          -1.064851   \n",
       "4        -1.151271        -0.495176  ...          -1.064851   \n",
       "\n",
       "   post_morph_emb_23  post_morph_emb_24  post_morph_emb_25  post_morph_emb_26  \\\n",
       "0          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "1          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "2          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "3          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "4          -0.816273          -0.215224          -0.598382           0.545335   \n",
       "\n",
       "   post_morph_emb_27  post_morph_emb_28  post_morph_emb_29  post_morph_emb_30  \\\n",
       "0          -0.525224           0.171648           1.022962          -0.645146   \n",
       "1          -0.525224           0.171648           1.022962          -0.645146   \n",
       "2          -0.525224           0.171648           1.022962          -0.645146   \n",
       "3          -0.525224           0.171648           1.022962          -0.645146   \n",
       "4          -0.525224           0.171648           1.022962          -0.645146   \n",
       "\n",
       "   post_morph_emb_31  \n",
       "0          -0.687774  \n",
       "1          -0.687774  \n",
       "2          -0.687774  \n",
       "3          -0.687774  \n",
       "4          -0.687774  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the resulting dataframe\n",
    "pd.read_csv('Data/imputed_morph_embed.csv').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(train, feature, imp_morph):\n",
    "    \"\"\"\n",
    "    Function that performs data cleaning and feature engineering for the data\n",
    "\n",
    "    inputs:\n",
    "    - train: path to training data\n",
    "    - feature: path to feature data\n",
    "    - imp_morph: path to imputed morph embedding data\n",
    "\n",
    "    outputs:\n",
    "    - data: cleaned and feature engineered data\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(train)\n",
    "\n",
    "    ############## CONCAT FEATURE WEIGHT DATA ##############\n",
    "    \n",
    "    #load in feature weight information for each neuron\n",
    "    feature_weights = pd.read_csv(feature)\n",
    "        \n",
    "    # make feature_weights into a numPy array\n",
    "    feature_weights[\"feature_weights\"] = (feature_weights.filter(regex=\"feature_weight_\").sort_index(axis=1)\n",
    "                                        .apply(lambda x: np.array(x), axis=1))\n",
    "\n",
    "    # delete the feature_weight_i columns\n",
    "    feature_weights.drop(\n",
    "        feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True)\n",
    "    \n",
    "    # update data with feature weight information for pre- and post- neurons\n",
    "    data = (\n",
    "    data.merge(\n",
    "        feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
    "        how=\"left\", \n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    ))\n",
    "    \n",
    "    ############## CONCAT IMPUTED MORPH EMBEDDINGS ################\n",
    "    # load in imputed morph embedding data\n",
    "    morph_embs = pd.read_csv(imp_morph)\n",
    "\n",
    "    # put all the morph embedding data into vectors\n",
    "    morph_embs[\"pre_morph_embeddings\"] = (morph_embs.filter(regex=\"pre_morph_emb_\").sort_index(axis=1)\n",
    "                                          .apply(lambda x: np.array(x), axis=1))\n",
    "    \n",
    "    morph_embs[\"post_morph_embeddings\"] = (morph_embs.filter(regex=\"post_morph_emb_\")\n",
    "                                           .sort_index(axis=1).apply(lambda x: np.array(x), axis=1))\n",
    "    \n",
    "    # drop the individual morph embedding columns\n",
    "    morph_embs.drop(morph_embs.filter(regex=\"_morph_emb_\").columns, axis=1, inplace=True)\n",
    "    morph_embs[\"ID\"] = data[\"ID\"]\n",
    "\n",
    "    # merge the main df with morph embeddings\n",
    "    data = data.merge(morph_embs, on=\"ID\")\n",
    "\n",
    "    ############## Preprocessing: ONE HOT ENCODE BRAIN AREA ##############\n",
    "    data = one_hot('pre_brain_area', data, '_pre')\n",
    "    data = one_hot('post_brain_area', data, '_post')\n",
    "\n",
    "\n",
    "    ############## Preprocessing: BRAIN COMPARTMENT GROUPING ##############\n",
    "    data = area_cols(data)\n",
    "\n",
    "    ############## FE: CALCULATE SIMILARITY BETWEEN PRE- AND POST- MORPH EMBEDDINGS ###################\n",
    "    data[\"me_similarity\"] = data.apply(row_morph_similarity, axis=1)\n",
    "    \n",
    "    ############## FE: CALCULATE SIMILARITY BETWEEN PRE- AND POST- FEATURE WEIGHTS ##############\n",
    "    data[\"fw_similarity\"] = data.apply(row_feature_similarity, axis=1)\n",
    "    \n",
    "    ############## FE: COMBINE COORDINATES INTO ARRAYS ##############\n",
    "    data = coord_column(data, \"axonal_coords\", \"axonal_coor_\")\n",
    "    data = coord_column(data, \"dendritic_coords\", \"dendritic_coor_\")\n",
    "    data = coord_column(data, \"pre_rf_coords\", \"pre_rf_[xy]\")\n",
    "    data = coord_column(data, \"post_rf_coords\", \"post_rf_[xy]\")\n",
    "    data = coord_column(data, \"pre_nucleus_coords\", \"pre_nucleus_[xyz]\")\n",
    "    data = coord_column(data, \"post_nucleus_coords\", \"post_nucleus_[xyz]\")\n",
    "    data = coord_column(data, \"pre_nucleus_xy\", \"pre_nucleus_[xy]\")\n",
    "    data = coord_column(data, \"post_nucleus_xy\", \"post_nucleus_[xy]\")\n",
    "\n",
    "    ############## FE: RF Distance ##############\n",
    "    data = coord_rf(data)\n",
    "    data[\"rf_distance\"] = data.apply(rfdistance, axis=1)\n",
    "\n",
    "\n",
    "    ############## FE: MINICOLUMNS ##############\n",
    "    data[\"minicol_dist\"] =  data[[\"pre_nucleus_xy\", \"post_nucleus_xy\"]].apply(\n",
    "    lambda x: math.dist(x[\"pre_nucleus_xy\"], x[\"post_nucleus_xy\"]), axis=1)\n",
    "\n",
    "    ############## FE: DISTANCE FROM PRE-SYNAPTIC NUCLEUS TO AXON ##############\n",
    "    data[\"nuclei_adp_dist\"] =  data[[\"pre_nucleus_coords\", \"axonal_coords\"]].apply(\n",
    "    lambda x: math.dist(x[\"pre_nucleus_coords\"], x[\"axonal_coords\"]), axis=1)\n",
    "\n",
    "    ############## STANDARDIZE ALL NUMERIC DATA #############\n",
    "    num_cols = data.select_dtypes(include='number').drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id'])\n",
    "    num_cols = num_cols.columns\n",
    "    for column in num_cols:\n",
    "        data[column] = StandardScaler().fit_transform(np.array(data[column]).reshape(-1, 1))\n",
    "    \n",
    "    # return processed data\n",
    "    return data\n",
    "\n",
    "def row_feature_similarity(row):\n",
    "    \"\"\"\n",
    "    Cosine similarity function for feature weight similarity\n",
    "\n",
    "    Inputs: row - a row of the dataframe containing feature weight information\n",
    "    Outputs: the cosine similarity between the pre and post feature weights\n",
    "    \"\"\"\n",
    "    pre = row[\"pre_feature_weights\"]\n",
    "    post = row[\"post_feature_weights\"]\n",
    "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
    "\n",
    "def row_morph_similarity(row):\n",
    "    \"\"\"\n",
    "    Morph embedding similarity function for feature weight similarity\n",
    "\n",
    "    Inputs: row - a row of the dataframe containing feature weight information\n",
    "    Outputs: the cosine similarity between the pre and post morph embeddings\n",
    "    \"\"\"\n",
    "    pre = row[\"pre_morph_embeddings\"]\n",
    "    post = row[\"post_morph_embeddings\"]\n",
    "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
    "\n",
    "\n",
    "def coord_column(df, new_col, old_cols):\n",
    "    \"\"\"\n",
    "    Function that combines coordinate data into an array of coordinates\n",
    "    Inputs:\n",
    "        - df: the data frame\n",
    "        - new_col: the new column created to store the coordinate array\n",
    "        - old_cols: the old coordinate columns to be combined to form the new column\n",
    "    Outputs:\n",
    "        - df: the updated data frame\n",
    "    \"\"\"\n",
    "    df[new_col] = (\n",
    "        df.filter(regex=old_cols)\n",
    "        .sort_index(axis=1)\n",
    "        .apply(lambda x: np.array(x), axis=1)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def coord_rf(df):\n",
    "    \"\"\"\n",
    "    Function that combines coordinate data for the rf data \n",
    "    (readout location of deep learning model)\n",
    "    Inputs:\n",
    "        - df: the data frame\n",
    "    Outputs:\n",
    "        - df: the updated data frame\n",
    "    \"\"\"\n",
    "    df = coord_column(df, \"pre_rf_coords_xy\", \"pre_rf_[xy]\")\n",
    "    df = coord_column(df, \"post_rf_coords_xy\", \"post_rf_[xy]\")\n",
    "    return df\n",
    "\n",
    "def rfdistance(row):\n",
    "    \"\"\"\n",
    "    Function that calculates the distance between the rf locations\n",
    "    Inputs:\n",
    "        - row: a row describing one ADP\n",
    "    Outputs:\n",
    "        - the distance between the pre- and post- neuron readout locations\n",
    "    \"\"\"\n",
    "    pre = row[\"pre_rf_coords_xy\"]\n",
    "    post = row[\"post_rf_coords_xy\"]\n",
    "    return math.dist(pre, post)\n",
    "\n",
    "def one_hot(column, df, suffix=''):\n",
    "    \"\"\"\n",
    "    Function for one-hot encoding\n",
    "\n",
    "    Inputs:\n",
    "        - column: the column to be one-hot encoded\n",
    "        - df: the dataframe\n",
    "        - suffix: an optional suffix to be added to the column when it is one-hot encoded\n",
    "    \n",
    "    Outputs:\n",
    "        - df: the updated dataframe\n",
    "    \"\"\"\n",
    "    cats = pd.unique(df[column])\n",
    "\n",
    "    for cat in cats:\n",
    "        new_col = cat+suffix\n",
    "        df[new_col] = df[column]==cat\n",
    "        df[new_col] = df[new_col].astype('int')\n",
    "    \n",
    "    df = df.drop(columns=column)\n",
    "    return df\n",
    "\n",
    "def area_cols(df):\n",
    "    \"\"\"\n",
    "    Function that groups and encodes the compartment data\n",
    "    Inputs:\n",
    "        - df: the data frame\n",
    "    Outputs:\n",
    "        - df: the updated data frame with grouped compartments\n",
    "    \"\"\"\n",
    "    # Encode brain areas\n",
    "    area1 = [\"basal\", \"soma\"] # the cell body\n",
    "    area2 = [\"axon\", \"apical\", \"oblique\", \"apical_shaft\"] # axonal areas\n",
    "    area3 = [\"apical_tuft\"] # terminal areas\n",
    "    df[\"area1\"] = df[\"compartment\"].isin(area1).astype('int')\n",
    "    df[\"area2\"] = df[\"compartment\"].isin(area2).astype('int')\n",
    "    df[\"area3\"] = df[\"compartment\"].isin(area3).astype('int')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cleaning\n",
    "train_path = \"Data/train_data.csv\"\n",
    "feature_path = \"Data/feature_weights.csv\"\n",
    "morph_path = \"Data/imputed_morph_embed.csv\"\n",
    "clean_data = cleaner(train_path, feature_path, morph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(data):\n",
    "    \"\"\"\n",
    "    Data splitting function\n",
    "\n",
    "    Inputs:\n",
    "        - data: the data to be split\n",
    "    \n",
    "    Outputs:\n",
    "        - X_train, y_train: the training data\n",
    "        - X_query, y_query: the query data\n",
    "    \"\"\"\n",
    "    # perform stratified sampling on pre-synaptic neurons\n",
    "    pre_nucleus_ids = pd.unique(data[\"pre_nucleus_id\"])\n",
    "\n",
    "    # Use 80% of the pre-nucleus ids\n",
    "    train_nucleus_idx = random.sample(range(0, len(pre_nucleus_ids)), int(np.floor(0.8*len(pre_nucleus_ids))))\n",
    "    train_nucleus_ids = pre_nucleus_ids[train_nucleus_idx]\n",
    "    training = data[data[\"pre_nucleus_id\"].isin(train_nucleus_ids)]\n",
    "    X_train = training.drop(columns='connected')\n",
    "    y_train = training['connected']\n",
    "    pre_nucleus_ids = np.delete(pre_nucleus_ids, train_nucleus_idx)\n",
    "\n",
    "    # Use 20% for query\n",
    "    query = data[data[\"pre_nucleus_id\"].isin(pre_nucleus_ids)]\n",
    "    X_query = query.drop(columns='connected')\n",
    "    y_query = query['connected']\n",
    "\n",
    "    return X_train, X_query, y_train, y_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "X_train, X_query, y_train, y_query = splitter(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of X_train with y_train to use for cross-validation\n",
    "X_cv = X_train.copy()\n",
    "X_cv['connected']=y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Tune maximum tree depth, learning rate, and L2 regularization strength.\n",
    "For our submission to the leaderboard, we tested tree depths in [3, 14], learning rates from [0.1, 1.0] with a step size of 0.1 and lambda values of [0, 2.0] with a step size of 0.1. Observing our results from this, we decided to keep the ranges for learning rates and lambda values the same, but change the range for maximum tree depth to [1, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree depth: 1, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.5254045009613038\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7688512048259084\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7693484257234846\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7645136535837562\n",
      "avgfold accuracy: 0.7673675896534355\n",
      "standard deviation: 0.001700631513938885\n",
      "avg train time: 0.5218906879425049\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646356810003809\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669692232007879\n",
      "standard deviation: 0.003852172505699784\n",
      "avg train time: 0.5473039627075196\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7768627792405078\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7599502762940618\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7709795250703522\n",
      "avgfold accuracy: 0.7647504173073633\n",
      "standard deviation: 0.009328521515885663\n",
      "avg train time: 0.5733063220977783\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.7565965959459728\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.7551953286638805\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7599964551032471\n",
      "standard deviation: 0.0051800005972935746\n",
      "avg train time: 0.6187151908874512\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.7515809695072301\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.7480262328459337\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.7588949160047602\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7536797144269476\n",
      "standard deviation: 0.008080304522592476\n",
      "avg train time: 0.6716970920562744\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511771326303749\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.754577896423274\n",
      "avgfold accuracy: 0.7537106580481278\n",
      "standard deviation: 0.004717929787047932\n",
      "avg train time: 0.6135959148406982\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7560480907940326\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7424202476180611\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7511268554137953\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7636810117257045\n",
      "avgfold accuracy: 0.7556127036856111\n",
      "standard deviation: 0.008290046347045764\n",
      "avg train time: 0.5978452205657959\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.756848291207708\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.7402782968535881\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.7408241773448094\n",
      "tree depth: 1, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543221006792059\n",
      "standard deviation: 0.0119408079460786\n",
      "avg train time: 0.6268561840057373\n",
      "tree depth: 1, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.735430407906548\n",
      "tree depth: 1, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.7511305019779045\n",
      "tree depth: 1, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.7603413523836129\n",
      "tree depth: 1, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.728561610346959\n",
      "tree depth: 1, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.7579491602288502\n",
      "avgfold accuracy: 0.7466826065687748\n",
      "standard deviation: 0.012555977229873854\n",
      "avg train time: 0.6304618358612061\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6427209854125977\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7688512048259084\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7645136535837562\n",
      "avgfold accuracy: 0.7669752428605685\n",
      "standard deviation: 0.0013976917439607656\n",
      "avg train time: 0.6840180397033692\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646356810003809\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669692232007879\n",
      "standard deviation: 0.003852172505699784\n",
      "avg train time: 0.7097127914428711\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7768627792405078\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7599502762940618\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7647433276228541\n",
      "standard deviation: 0.00932379696846853\n",
      "avg train time: 0.7233389854431153\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.7565965959459728\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.7551736187945739\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.759992113129386\n",
      "standard deviation: 0.005184030703026275\n",
      "avg train time: 0.6464250564575196\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.7515809695072301\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.7588949160047602\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7536840564008089\n",
      "standard deviation: 0.008077270705521531\n",
      "avg train time: 0.8401539325714111\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511771326303749\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.754577896423274\n",
      "avgfold accuracy: 0.7537106580481278\n",
      "standard deviation: 0.004717929787047932\n",
      "avg train time: 0.6719789028167724\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.7560652428642874\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.7424202476180611\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.7511268554137953\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.7636810117257045\n",
      "avgfold accuracy: 0.755616134099662\n",
      "standard deviation: 0.008290229346841843\n",
      "avg train time: 0.6503868579864502\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.756848291207708\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.7408241773448094\n",
      "tree depth: 1, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543255310932568\n",
      "standard deviation: 0.011936774662870662\n",
      "avg train time: 0.7135741233825683\n",
      "tree depth: 1, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.735430407906548\n",
      "tree depth: 1, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.7511305019779045\n",
      "tree depth: 1, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.7603413523836129\n",
      "tree depth: 1, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.7285403924390446\n",
      "tree depth: 1, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7475383337967033\n",
      "standard deviation: 0.01342198735318467\n",
      "avg train time: 0.7128448009490966\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.8722969055175781\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7688512048259084\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7645136535837562\n",
      "avgfold accuracy: 0.7669752428605685\n",
      "standard deviation: 0.0013976917439607656\n",
      "avg train time: 0.7008310317993164\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646013768598712\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669623623726859\n",
      "standard deviation: 0.003856350782972111\n",
      "avg train time: 0.8016774654388428\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7768627792405078\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7599502762940618\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7647433276228541\n",
      "standard deviation: 0.00932379696846853\n",
      "avg train time: 0.6775393962860108\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7565965959459728\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7551736187945739\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.759992113129386\n",
      "standard deviation: 0.005184030703026275\n",
      "avg train time: 0.6257335662841796\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.7515809695072301\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.7588949160047602\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7536840564008089\n",
      "standard deviation: 0.008077270705521531\n",
      "avg train time: 0.622099781036377\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511771326303749\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7545601722120014\n",
      "avgfold accuracy: 0.7537071132058732\n",
      "standard deviation: 0.004717283465371428\n",
      "avg train time: 0.6053380012512207\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.7560652428642874\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.7424202476180611\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.7511268554137953\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.7636810117257045\n",
      "avgfold accuracy: 0.755616134099662\n",
      "standard deviation: 0.008290229346841843\n",
      "avg train time: 0.6505514621734619\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.756848291207708\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.7408241773448094\n",
      "tree depth: 1, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543255310932568\n",
      "standard deviation: 0.011936774662870662\n",
      "avg train time: 0.6725082874298096\n",
      "tree depth: 1, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.7354469199107752\n",
      "tree depth: 1, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.7511305019779045\n",
      "tree depth: 1, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.7603413523836129\n",
      "tree depth: 1, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.7285403924390446\n",
      "tree depth: 1, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7475416361975487\n",
      "standard deviation: 0.013419009564161677\n",
      "avg train time: 0.7466745853424073\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6809167861938477\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7688677168301356\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7645136535837562\n",
      "avgfold accuracy: 0.766978545261414\n",
      "standard deviation: 0.0014021327295595527\n",
      "avg train time: 0.6331098079681396\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646013768598712\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669623623726859\n",
      "standard deviation: 0.003856350782972111\n",
      "avg train time: 0.6248342037200928\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642139587738319\n",
      "standard deviation: 0.008447729808469064\n",
      "avg train time: 0.6549359798431397\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7551736187945739\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7599955435434369\n",
      "standard deviation: 0.005181787851810307\n",
      "avg train time: 0.5960931777954102\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.7588949160047602\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7536874868148599\n",
      "standard deviation: 0.00807638038997534\n",
      "avg train time: 0.5880502223968506\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7526635377181357\n",
      "standard deviation: 0.004981275646045564\n",
      "avg train time: 0.6244015693664551\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.7560652428642874\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.7424202476180611\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.7511268554137953\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.7636810117257045\n",
      "avgfold accuracy: 0.755616134099662\n",
      "standard deviation: 0.008290229346841843\n",
      "avg train time: 0.5636551856994629\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.756848291207708\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.7408241773448094\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543255310932568\n",
      "standard deviation: 0.011936774662870662\n",
      "avg train time: 0.545842981338501\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.7354469199107752\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.7511476540481594\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.7603413523836129\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.7285403924390446\n",
      "tree depth: 1, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7475450666115997\n",
      "standard deviation: 0.013419928738455536\n",
      "avg train time: 0.5806264400482177\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.5409284114837647\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7688677168301356\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7645136535837562\n",
      "avgfold accuracy: 0.766978545261414\n",
      "standard deviation: 0.0014021327295595527\n",
      "avg train time: 0.5572111129760742\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646013768598712\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669623623726859\n",
      "standard deviation: 0.003856350782972111\n",
      "avg train time: 0.5676246643066406\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642105283597809\n",
      "standard deviation: 0.008446929467291805\n",
      "avg train time: 0.5645374298095703\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7565965959459728\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7516783298362134\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7592930553377137\n",
      "standard deviation: 0.00596369965139638\n",
      "avg train time: 0.5626145839691162\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.7588736980968459\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.753683243233277\n",
      "standard deviation: 0.008073648241923113\n",
      "avg train time: 0.5480170726776123\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7526635377181357\n",
      "standard deviation: 0.004981275646045564\n",
      "avg train time: 0.578340482711792\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.7560480907940326\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.7423985377487545\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.7618363638040941\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.7636810117257045\n",
      "avgfold accuracy: 0.7577502633898094\n",
      "standard deviation: 0.008245761694361066\n",
      "avg train time: 0.6363082885742187\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.7408241773448094\n",
      "tree depth: 1, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543288334941023\n",
      "standard deviation: 0.011937474410687796\n",
      "avg train time: 0.5972885131835938\n",
      "tree depth: 1, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.7354469199107752\n",
      "tree depth: 1, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.7511476540481594\n",
      "tree depth: 1, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.7603196425143063\n",
      "tree depth: 1, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.7285403924390446\n",
      "tree depth: 1, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7475407246377385\n",
      "standard deviation: 0.013415790713911895\n",
      "avg train time: 0.5502828121185303\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.5313226699829101\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7688677168301356\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7627838101707267\n",
      "avgfold accuracy: 0.766632576578808\n",
      "standard deviation: 0.0020372295309192255\n",
      "avg train time: 0.5494076251983643\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.5180403709411621\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642105283597809\n",
      "standard deviation: 0.008446929467291805\n",
      "avg train time: 0.49776220321655273\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7565965959459728\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7516783298362134\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7592930553377137\n",
      "standard deviation: 0.00596369965139638\n",
      "avg train time: 0.5484241962432861\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.7588736980968459\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.753683243233277\n",
      "standard deviation: 0.008073648241923113\n",
      "avg train time: 0.5672433853149415\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623551071629074\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7526602353172902\n",
      "standard deviation: 0.004974839769532552\n",
      "avg train time: 0.5162397861480713\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.7560309387237777\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.7423985377487545\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.7636810117257045\n",
      "avgfold accuracy: 0.7571233954901727\n",
      "standard deviation: 0.00802869073166865\n",
      "avg train time: 0.5327665328979492\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543330770756851\n",
      "standard deviation: 0.011932675773586095\n",
      "avg train time: 0.5160449028015137\n",
      "tree depth: 1, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.7354469199107752\n",
      "tree depth: 1, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.7511476540481594\n",
      "tree depth: 1, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.7511315288508469\n",
      "tree depth: 1, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.7285403924390446\n",
      "tree depth: 1, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7457031019050465\n",
      "standard deviation: 0.012104759796540411\n",
      "avg train time: 0.5292362689971923\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6872649669647217\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7688677168301356\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7669926808968321\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7627838101707267\n",
      "avgfold accuracy: 0.7666282346049468\n",
      "standard deviation: 0.002036434118294744\n",
      "avg train time: 0.9543436527252197\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.5778483867645263\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642105283597809\n",
      "standard deviation: 0.008446929467291805\n",
      "avg train time: 0.7231080055236816\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7565965959459728\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7516783298362134\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7592930553377137\n",
      "standard deviation: 0.00596369965139638\n",
      "avg train time: 0.7018997192382812\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7608990857849243\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7540883207708926\n",
      "standard deviation: 0.008369302272633652\n",
      "avg train time: 0.6949324131011962\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623551071629074\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7486887762068202\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7526602353172902\n",
      "standard deviation: 0.004974839769532552\n",
      "avg train time: 0.8331199645996094\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7647873128764612\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7560480907940326\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7423985377487545\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7589983543218359\n",
      "avgfold accuracy: 0.75619029442345\n",
      "standard deviation: 0.007462197325873074\n",
      "avg train time: 0.7131829261779785\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7403126009940979\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543365074897361\n",
      "standard deviation: 0.011928641516735332\n",
      "avg train time: 0.7432374000549317\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7354469199107752\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7511476540481594\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7511315288508469\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7285403924390446\n",
      "tree depth: 1, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7457031019050465\n",
      "standard deviation: 0.012104759796540411\n",
      "avg train time: 0.7564091682434082\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6769109725952148\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7688677168301356\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7669926808968321\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7627838101707267\n",
      "avgfold accuracy: 0.7666282346049468\n",
      "standard deviation: 0.002036434118294744\n",
      "avg train time: 0.7387473583221436\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.6827154636383057\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642105283597809\n",
      "standard deviation: 0.008446929467291805\n",
      "avg train time: 0.7354438781738282\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7592921437779034\n",
      "standard deviation: 0.005967704059126816\n",
      "avg train time: 0.7006062030792236\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7538337058759212\n",
      "standard deviation: 0.008175346541415317\n",
      "avg train time: 0.7510601043701172\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.752504089563935\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7534266003895586\n",
      "standard deviation: 0.004590880236923657\n",
      "avg train time: 0.7025868415832519\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.764754288868007\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7560480907940326\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7423985377487545\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7589983543218359\n",
      "avgfold accuracy: 0.756187933203342\n",
      "standard deviation: 0.007456043947133578\n",
      "avg train time: 0.7646080017089844\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7403126009940979\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543365074897361\n",
      "standard deviation: 0.011928641516735332\n",
      "avg train time: 0.649189805984497\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7354469199107752\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7511476540481594\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7511315288508469\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7448015253222368\n",
      "standard deviation: 0.013443346494636526\n",
      "avg train time: 0.619778823852539\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6639781951904297\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7688677168301356\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7669926808968321\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.7669990171833343\n",
      "standard deviation: 0.0013587095317342131\n",
      "avg train time: 0.5908938407897949\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.7443014144897461\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.760704434542754\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642190155229468\n",
      "standard deviation: 0.008443380345168176\n",
      "avg train time: 0.5908587455749512\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7592921437779034\n",
      "standard deviation: 0.005967704059126816\n",
      "avg train time: 0.6299916744232178\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7538337058759212\n",
      "standard deviation: 0.008175346541415317\n",
      "avg train time: 0.6060100555419922\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517378658130351\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7478855110424765\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7525028846852668\n",
      "standard deviation: 0.005117954633758673\n",
      "avg train time: 0.6234827995300293\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.764754288868007\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.7560309387237777\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.7423985377487545\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7561809579470365\n",
      "standard deviation: 0.007454776910899923\n",
      "avg train time: 0.6054257869720459\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.7403126009940979\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543365074897361\n",
      "standard deviation: 0.011928641516735332\n",
      "avg train time: 0.5766435623168945\n",
      "tree depth: 1, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.7511648061184143\n",
      "tree depth: 1, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.7511315288508469\n",
      "tree depth: 1, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7447983509345969\n",
      "standard deviation: 0.01344957033437975\n",
      "avg train time: 0.6307770729064941\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.5793160438537598\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7687025967878648\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7669926808968321\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.7669659931748801\n",
      "standard deviation: 0.0013141651442244438\n",
      "avg train time: 0.5684413909912109\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.6131990432739258\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.760704434542754\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642157131221013\n",
      "standard deviation: 0.008439750493437058\n",
      "avg train time: 0.5774746894836426\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.7583672139718274\n",
      "avgfold accuracy: 0.7592921437779034\n",
      "standard deviation: 0.005967704059126816\n",
      "avg train time: 0.6374334812164306\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.751598121577485\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.7480479427152402\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7538337058759212\n",
      "standard deviation: 0.008175346541415317\n",
      "avg train time: 0.7698710918426513\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7517550178832901\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7478855110424765\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7493635126812276\n",
      "avgfold accuracy: 0.7525063150993178\n",
      "standard deviation: 0.005117446437550369\n",
      "avg train time: 0.6121292114257812\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.764754288868007\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.7560309387237777\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7555221026482852\n",
      "standard deviation: 0.008687422009121495\n",
      "avg train time: 0.5838654041290283\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.7403126009940979\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543365074897361\n",
      "standard deviation: 0.011928641516735332\n",
      "avg train time: 0.5560352325439453\n",
      "tree depth: 1, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.7511648061184143\n",
      "tree depth: 1, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7466695847300103\n",
      "standard deviation: 0.014785084340183038\n",
      "avg train time: 0.5969038009643555\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6652988910675048\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7687025967878648\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7669926808968321\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.7669659931748801\n",
      "standard deviation: 0.0013141651442244438\n",
      "avg train time: 0.7056849479675293\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.702319860458374\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7606832166348398\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642114695405186\n",
      "standard deviation: 0.008441520077428752\n",
      "avg train time: 0.548485803604126\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7600478883036966\n",
      "standard deviation: 0.006041528748174121\n",
      "avg train time: 0.5266121864318848\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.7549868290155253\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.7479828131073205\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.743553635047598\n",
      "avgfold accuracy: 0.7544984214419455\n",
      "standard deviation: 0.00811248381324029\n",
      "avg train time: 0.6066247940063476\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.762766466438815\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7478855110424765\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551750981296823\n",
      "standard deviation: 0.006177620431811508\n",
      "avg train time: 0.7244776725769043\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.764754288868007\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.7560309387237777\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7555221026482852\n",
      "standard deviation: 0.008687422009121495\n",
      "avg train time: 0.6366293430328369\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.7403126009940979\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.7694965088960767\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7543365074897361\n",
      "standard deviation: 0.011928641516735332\n",
      "avg train time: 0.5654017448425293\n",
      "tree depth: 1, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7469930186105851\n",
      "standard deviation: 0.01489714639258563\n",
      "avg train time: 0.6947930335998536\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.7056355476379395\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7671102733678903\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7669926808968321\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.7667909659300732\n",
      "standard deviation: 0.0011142990086095788\n",
      "avg train time: 0.7221155643463135\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.6297755718231202\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7606832166348398\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642114695405186\n",
      "standard deviation: 0.008441520077428752\n",
      "avg train time: 0.6302896976470947\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7600478883036966\n",
      "standard deviation: 0.006041528748174121\n",
      "avg train time: 0.7172386169433593\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.7663428187292161\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.7549868290155253\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.7479828131073205\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.7435713592588705\n",
      "avgfold accuracy: 0.7545019662841999\n",
      "standard deviation: 0.008107703053341162\n",
      "avg train time: 0.7746078491210937\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.762766466438815\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7478855110424765\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551750981296823\n",
      "standard deviation: 0.006177620431811508\n",
      "avg train time: 0.7426368236541748\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.7574079791430826\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7522543473523691\n",
      "standard deviation: 0.007924666877031935\n",
      "avg train time: 0.7862070083618165\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.7403297530643528\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7531134087573721\n",
      "standard deviation: 0.01053726068380577\n",
      "avg train time: 0.7885615348815918\n",
      "tree depth: 1, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7469930186105851\n",
      "standard deviation: 0.01489714639258563\n",
      "avg train time: 0.6435717582702637\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.692055606842041\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.7665164694060225\n",
      "standard deviation: 0.0011742418880735747\n",
      "avg train time: 0.6334605693817139\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652704360741336\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.7669657927867368\n",
      "standard deviation: 0.0038542561041707045\n",
      "avg train time: 0.7048542022705078\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7606832166348398\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642114695405186\n",
      "standard deviation: 0.008441520077428752\n",
      "avg train time: 0.5962750434875488\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7516349100976003\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7600435463298353\n",
      "standard deviation: 0.006047562679420974\n",
      "avg train time: 0.6428194046020508\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7656162905432249\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7549868290155253\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7479828131073205\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7596472292179817\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7435713592588705\n",
      "avgfold accuracy: 0.7543609042285845\n",
      "standard deviation: 0.007900823524530245\n",
      "avg train time: 0.6428746223449707\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.762766466438815\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7478855110424765\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551750981296823\n",
      "standard deviation: 0.006177620431811508\n",
      "avg train time: 0.6326224327087402\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7522577777664201\n",
      "standard deviation: 0.007926900426062268\n",
      "avg train time: 0.8280488967895507\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7403297530643528\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7531134087573721\n",
      "standard deviation: 0.01053726068380577\n",
      "avg train time: 1.5088001251220704\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7605094076972211\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7469938157421919\n",
      "standard deviation: 0.014897454614552151\n",
      "avg train time: 0.6758300304412842\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.6102603912353516\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7670143907661386\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.7665164694060225\n",
      "standard deviation: 0.0011742418880735747\n",
      "avg train time: 0.731918478012085\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.768480199291369\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652492181662192\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627343012006915\n",
      "avgfold accuracy: 0.766961549205154\n",
      "standard deviation: 0.0038561316001298226\n",
      "avg train time: 0.7090471744537353\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7497678613372338\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7606832166348398\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7709440766478071\n",
      "avgfold accuracy: 0.7642114695405186\n",
      "standard deviation: 0.008441520077428752\n",
      "avg train time: 0.6572095870971679\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7516349100976003\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7599830256736686\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7600435463298353\n",
      "standard deviation: 0.006047562679420974\n",
      "avg train time: 0.6999962806701661\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.7656162905432249\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.7549868290155253\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.7596472292179817\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.7436068076814157\n",
      "avgfold accuracy: 0.7535919053200868\n",
      "standard deviation: 0.008636574000880764\n",
      "avg train time: 0.6085973262786866\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.762766466438815\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7478855110424765\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551750981296823\n",
      "standard deviation: 0.006177620431811508\n",
      "avg train time: 0.6087005138397217\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.75874039428408\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7522577777664201\n",
      "standard deviation: 0.007926900426062268\n",
      "avg train time: 0.5846077919006347\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.7568648032119352\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.7403297530643528\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7531134087573721\n",
      "standard deviation: 0.01053726068380577\n",
      "avg train time: 0.58548002243042\n",
      "tree depth: 1, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.7605094076972211\n",
      "tree depth: 1, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7469938157421919\n",
      "standard deviation: 0.014897454614552151\n",
      "avg train time: 0.6350261688232421\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7539871486570737\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7506885690475854\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.75560081455482\n",
      "standard deviation: 0.0036066285916499814\n",
      "avg train time: 0.5585346221923828\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7663630946869411\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7646377230626644\n",
      "avgfold accuracy: 0.766386210190183\n",
      "standard deviation: 0.0011476045369002998\n",
      "avg train time: 0.737597131729126\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652492181662192\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627520254119642\n",
      "avgfold accuracy: 0.7669694360212699\n",
      "standard deviation: 0.0038539672847773174\n",
      "avg train time: 0.8389567375183106\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7661744925244056\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7709263524365344\n",
      "avgfold accuracy: 0.7642080230905424\n",
      "standard deviation: 0.008433047001646552\n",
      "avg train time: 0.8452775001525878\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7599618077657544\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7621991092346112\n",
      "avgfold accuracy: 0.7600542792488774\n",
      "standard deviation: 0.006045317053539263\n",
      "avg train time: 1.048306369781494\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7656162905432249\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7549868290155253\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7436068076814157\n",
      "avgfold accuracy: 0.7535876617385039\n",
      "standard deviation: 0.008633602375757507\n",
      "avg train time: 0.6354876041412354\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.762766466438815\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7479072209117832\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551794401035437\n",
      "standard deviation: 0.006172500888399919\n",
      "avg train time: 0.6678912162780761\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7522535341848372\n",
      "standard deviation: 0.007923433811960496\n",
      "avg train time: 0.5983999252319336\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7537258336810861\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7403126009940979\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7524821844371512\n",
      "standard deviation: 0.010391515994582297\n",
      "avg train time: 0.5897656917572022\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7605094076972211\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7469938157421919\n",
      "standard deviation: 0.014897454614552151\n",
      "avg train time: 0.5579970359802247\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7540697086782091\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7536571975017434\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7554423230400884\n",
      "avgfold accuracy: 0.7562110522498787\n",
      "standard deviation: 0.00291769627170787\n",
      "avg train time: 0.6314420700073242\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7663630946869411\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7626774649030912\n",
      "avgfold accuracy: 0.7659941585582684\n",
      "standard deviation: 0.001817363080100598\n",
      "avg train time: 0.8602292537689209\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652492181662192\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627520254119642\n",
      "avgfold accuracy: 0.7669694360212699\n",
      "standard deviation: 0.0038539672847773174\n",
      "avg train time: 0.8594482421875\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7662087966649154\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7709263524365344\n",
      "avgfold accuracy: 0.7642148839186442\n",
      "standard deviation: 0.008434657861071535\n",
      "avg train time: 0.7283333301544189\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7698401112608866\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7597284107786972\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7621636608120659\n",
      "avgfold accuracy: 0.7600005101669568\n",
      "standard deviation: 0.006044198375538207\n",
      "avg train time: 0.8136463642120362\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7655997785389979\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7549868290155253\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7532451149499266\n",
      "avgfold accuracy: 0.7555120207913607\n",
      "standard deviation: 0.007131375614736431\n",
      "avg train time: 0.564037561416626\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.762766466438815\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7479072209117832\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551794401035437\n",
      "standard deviation: 0.006172500888399919\n",
      "avg train time: 0.7024165153503418\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.7589806301105634\n",
      "avgfold accuracy: 0.7522535341848372\n",
      "standard deviation: 0.007923433811960496\n",
      "avg train time: 0.7448493480682373\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.7537258336810861\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7524787540231003\n",
      "standard deviation: 0.010395534866311455\n",
      "avg train time: 0.704497241973877\n",
      "tree depth: 1, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.735413895902321\n",
      "tree depth: 1, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7469894737683306\n",
      "standard deviation: 0.01489351740555639\n",
      "avg train time: 0.7681713104248047\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7540697086782091\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7536571975017434\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7582356191294739\n",
      "avgfold accuracy: 0.7567697114677558\n",
      "standard deviation: 0.0029836951423717595\n",
      "avg train time: 0.7636951923370361\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7663196749483279\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7626774649030912\n",
      "avgfold accuracy: 0.7659854746105457\n",
      "standard deviation: 0.0018156823963998096\n",
      "avg train time: 0.6123767852783203\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7652492181662192\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627520254119642\n",
      "avgfold accuracy: 0.7669694360212699\n",
      "standard deviation: 0.0038539672847773174\n",
      "avg train time: 0.7375055313110351\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7709263524365344\n",
      "avgfold accuracy: 0.7642114535045933\n",
      "standard deviation: 0.00843384967922097\n",
      "avg train time: 0.7489513397216797\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7697790043361523\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7597284107786972\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7599847439397555\n",
      "standard deviation: 0.006023043409167626\n",
      "avg train time: 1.0469829559326171\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.7655997785389979\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.7549696769452705\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.7532451149499266\n",
      "avgfold accuracy: 0.7555085903773098\n",
      "standard deviation: 0.007131631544037745\n",
      "avg train time: 0.9447352409362793\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623716191671346\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7627321622983052\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7479072209117832\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7551725792754418\n",
      "standard deviation: 0.006164077297873886\n",
      "avg train time: 1.1280004024505614\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.7589629058992908\n",
      "avgfold accuracy: 0.7522499893425827\n",
      "standard deviation: 0.00792042679710948\n",
      "avg train time: 1.2841572284698486\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.7537258336810861\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.7641809533051198\n",
      "avgfold accuracy: 0.7524822988653549\n",
      "standard deviation: 0.010399520886186496\n",
      "avg train time: 1.0827848434448242\n",
      "tree depth: 1, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.71984295082525\n",
      "tree depth: 1, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.7527648234510327\n",
      "tree depth: 1, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7438753991811199\n",
      "standard deviation: 0.01824214299764828\n",
      "avg train time: 0.6964746475219726\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7540697086782091\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7536571975017434\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7582356191294739\n",
      "avgfold accuracy: 0.7567697114677558\n",
      "standard deviation: 0.0029836951423717595\n",
      "avg train time: 0.7626778602600097\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7663196749483279\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7626774649030912\n",
      "avgfold accuracy: 0.7659854746105457\n",
      "standard deviation: 0.0018156823963998096\n",
      "avg train time: 0.8910638332366944\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.765312871889962\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627520254119642\n",
      "avgfold accuracy: 0.7669821667660184\n",
      "standard deviation: 0.003848364952902999\n",
      "avg train time: 0.7652058124542236\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7734877005583058\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7709263524365344\n",
      "avgfold accuracy: 0.7642114535045933\n",
      "standard deviation: 0.00843384967922097\n",
      "avg train time: 0.7589179515838623\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7697790043361523\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7597284107786972\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7599847439397555\n",
      "standard deviation: 0.006023043409167626\n",
      "avg train time: 1.9223687171936035\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7655997785389979\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7549696769452705\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7532451149499266\n",
      "avgfold accuracy: 0.7555085903773098\n",
      "standard deviation: 0.007131631544037745\n",
      "avg train time: 0.8206008911132813\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623551071629074\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7627321622983052\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7472342149632789\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516959792775255\n",
      "avgfold accuracy: 0.7550346756848955\n",
      "standard deviation: 0.006322640033430153\n",
      "avg train time: 0.8918720245361328\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7470384719691215\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.7589451816880182\n",
      "avgfold accuracy: 0.7522464445003282\n",
      "standard deviation: 0.007917424988686262\n",
      "avg train time: 0.8912856101989746\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7537258336810861\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.740295448923843\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7524787540231003\n",
      "standard deviation: 0.010395534866311455\n",
      "avg train time: 0.9449616432189941\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.71984295082525\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7527648234510327\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.7622490142764067\n",
      "avgfold accuracy: 0.7438753991811199\n",
      "standard deviation: 0.01824214299764828\n",
      "avg train time: 0.8856756210327148\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7540697086782091\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7536571975017434\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7582356191294739\n",
      "avgfold accuracy: 0.7567697114677558\n",
      "standard deviation: 0.0029836951423717595\n",
      "avg train time: 0.7350610733032227\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7663196749483279\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7626774649030912\n",
      "avgfold accuracy: 0.7659854746105457\n",
      "standard deviation: 0.0018156823963998096\n",
      "avg train time: 0.6778138160705567\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.765312871889962\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7627520254119642\n",
      "avgfold accuracy: 0.7669821667660184\n",
      "standard deviation: 0.003848364952902999\n",
      "avg train time: 0.6738317966461181\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7709263524365344\n",
      "avgfold accuracy: 0.7642147559054387\n",
      "standard deviation: 0.008437483736714744\n",
      "avg train time: 0.6358918190002442\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7697790043361523\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7597284107786972\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7599847439397555\n",
      "standard deviation: 0.006023043409167626\n",
      "avg train time: 0.7605790138244629\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.7655997785389979\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.7549696769452705\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.7596260113100675\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.7607742410769984\n",
      "avgfold accuracy: 0.7570144156027241\n",
      "standard deviation: 0.007287894766431051\n",
      "avg train time: 0.7884140968322754\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623551071629074\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7627321622983052\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7472342149632789\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7516782550662529\n",
      "avgfold accuracy: 0.7550311308426408\n",
      "standard deviation: 0.006324515598854973\n",
      "avg train time: 0.862367582321167\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.7470549839733487\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.7589451816880182\n",
      "avgfold accuracy: 0.7522497469011735\n",
      "standard deviation: 0.007915255172709152\n",
      "avg train time: 0.8148648262023925\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.7516337502364219\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.734620641342989\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7509253758179966\n",
      "standard deviation: 0.011814473290190584\n",
      "avg train time: 0.6456923484802246\n",
      "tree depth: 1, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.71984295082525\n",
      "tree depth: 1, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.7527648234510327\n",
      "tree depth: 1, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.7605094076972211\n",
      "tree depth: 1, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7438761963127267\n",
      "standard deviation: 0.01824253093194945\n",
      "avg train time: 0.6358832359313965\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7540697086782091\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7536571975017434\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7582356191294739\n",
      "avgfold accuracy: 0.7567697114677558\n",
      "standard deviation: 0.0029836951423717595\n",
      "avg train time: 0.5832911968231201\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7663196749483279\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7626774649030912\n",
      "avgfold accuracy: 0.7659854746105457\n",
      "standard deviation: 0.0018156823963998096\n",
      "avg train time: 0.6166568756103515\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646185289301262\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.765312871889962\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7660061509944371\n",
      "avgfold accuracy: 0.7676329918825131\n",
      "standard deviation: 0.003316329546455098\n",
      "avg train time: 0.6206618785858155\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7706604892674455\n",
      "avgfold accuracy: 0.764161583271621\n",
      "standard deviation: 0.008395754557746007\n",
      "avg train time: 0.6665160655975342\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7697790043361523\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7597284107786972\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7599847439397555\n",
      "standard deviation: 0.006023043409167626\n",
      "avg train time: 0.642170763015747\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7655997785389979\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7549696769452705\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7596472292179817\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7607742410769984\n",
      "avgfold accuracy: 0.7570186591843069\n",
      "standard deviation: 0.00728942022368053\n",
      "avg train time: 0.6258441925048828\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623551071629074\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7627321622983052\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7472342149632789\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7509515624040763\n",
      "avgfold accuracy: 0.7548857923102056\n",
      "standard deviation: 0.006407698089466912\n",
      "avg train time: 0.849365520477295\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7470549839733487\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7574251312133374\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7589451816880182\n",
      "avgfold accuracy: 0.7522497469011735\n",
      "standard deviation: 0.007915255172709152\n",
      "avg train time: 0.6895860195159912\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7515181662068324\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.734620641342989\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7633638631640016\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7641632290938472\n",
      "avgfold accuracy: 0.7509022590120787\n",
      "standard deviation: 0.011813177640397788\n",
      "avg train time: 0.6545802116394043\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.719859462829477\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7527648234510327\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7605094076972211\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7438794987135722\n",
      "standard deviation: 0.018238180929196487\n",
      "avg train time: 0.6110245227813721\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7540697086782091\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7617660493726857\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7536571975017434\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7561199826566666\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7582356191294739\n",
      "avgfold accuracy: 0.7567697114677558\n",
      "standard deviation: 0.0029836951423717595\n",
      "avg train time: 0.5643362045288086\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.76782746056383\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7657160808783304\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7663196749483279\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7673866917591491\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7627129133256363\n",
      "avgfold accuracy: 0.7659925642950547\n",
      "standard deviation: 0.001802775118739063\n",
      "avg train time: 0.5575579643249512\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.773725498437364\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7646356810003809\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7685019091606755\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.765312871889962\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7659884267831645\n",
      "avgfold accuracy: 0.7676328774543094\n",
      "standard deviation: 0.0033149683848175877\n",
      "avg train time: 0.5524954319000244\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7735042125625329\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7661916445946605\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7497895712065403\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7606619987269255\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7706604892674455\n",
      "avgfold accuracy: 0.764161583271621\n",
      "standard deviation: 0.008395754557746007\n",
      "avg train time: 0.5491983413696289\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7697790043361523\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7566137480162276\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7516566199669068\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7597284107786972\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7621459366007933\n",
      "avgfold accuracy: 0.7599847439397555\n",
      "standard deviation: 0.006023043409167626\n",
      "avg train time: 0.5427473068237305\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.7655997785389979\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.7549696769452705\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.744102370142286\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.7596472292179817\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.7607742410769984\n",
      "avgfold accuracy: 0.7570186591843069\n",
      "standard deviation: 0.00728942022368053\n",
      "avg train time: 0.5438653945922851\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7623551071629074\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7627321622983052\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7472125050939723\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7511559147224605\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7487963379205045\n",
      "avgfold accuracy: 0.7544504054396299\n",
      "standard deviation: 0.006727275169193207\n",
      "avg train time: 0.5544512748718262\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.7470549839733487\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.7574079791430826\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.7391042612549978\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.7587191763761658\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.7589451816880182\n",
      "avgfold accuracy: 0.7522463164871226\n",
      "standard deviation: 0.00791301485515691\n",
      "avg train time: 0.5538949489593505\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.7515181662068324\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.7346034892727342\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.7678738923389272\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.7408453952527237\n",
      "tree depth: 1, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.7648013006996608\n",
      "avgfold accuracy: 0.7519284487541757\n",
      "standard deviation: 0.012985562105994199\n",
      "avg train time: 0.5672266006469726\n",
      "tree depth: 1, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.719875974833704\n",
      "tree depth: 1, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.7527819755212874\n",
      "tree depth: 1, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.7604876978279145\n",
      "tree depth: 1, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.7240325095249955\n",
      "tree depth: 1, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.762231290065134\n",
      "avgfold accuracy: 0.7438818895546071\n",
      "standard deviation: 0.01823154807945328\n",
      "avg train time: 0.6090150356292725\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7699772734778184\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7723295580723961\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7663035887198658\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7603188221293555\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7652937169156308\n",
      "avgfold accuracy: 0.7668445918630133\n",
      "standard deviation: 0.004127520237087817\n",
      "avg train time: 0.6875511646270752\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.763616649304044\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7699605863480353\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7515327690859831\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7551970036623954\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7610366386181848\n",
      "avgfold accuracy: 0.7602687294037285\n",
      "standard deviation: 0.00644623530315187\n",
      "avg train time: 0.686931848526001\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7499263839811543\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7470582394031802\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7470508581152799\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7474367844721814\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7349967027025957\n",
      "avgfold accuracy: 0.7452937937348784\n",
      "standard deviation: 0.005258971936993764\n",
      "avg train time: 0.6415073871612549\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7427980141562915\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7260646470555312\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7463135072529272\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7052601961272706\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.7122783235866674\n",
      "avgfold accuracy: 0.7265429376357376\n",
      "standard deviation: 0.016197320751540806\n",
      "avg train time: 0.6738513946533203\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.700693716832137\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.725890598679471\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.7307547239890917\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6845141560346497\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6934166932364806\n",
      "avgfold accuracy: 0.7070539777543661\n",
      "standard deviation: 0.018171492350030877\n",
      "avg train time: 0.6512091159820557\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.7190502495314093\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.6940888549458446\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.7483325250984398\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.6593838504045241\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.6859193518681714\n",
      "avgfold accuracy: 0.7013549663696779\n",
      "standard deviation: 0.030241174632977468\n",
      "avg train time: 0.681044340133667\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6816865861481296\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6815582746100612\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7003359275319332\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6551305823854464\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6791063234836887\n",
      "avgfold accuracy: 0.6795635388318517\n",
      "standard deviation: 0.014402477839773477\n",
      "avg train time: 0.6747911453247071\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7044187999675764\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.6188270150522958\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.6676657130018053\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.7088303397632819\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.6771460653241155\n",
      "avgfold accuracy: 0.675377586621815\n",
      "standard deviation: 0.03232041590644762\n",
      "avg train time: 0.6417416572570801\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.6708232510034795\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.6119392852822598\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.6846742185624091\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.6740804804472366\n",
      "tree depth: 2, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.6574903110945641\n",
      "avgfold accuracy: 0.6598015092779898\n",
      "standard deviation: 0.025456447506764637\n",
      "avg train time: 0.6505221366882324\n",
      "tree depth: 2, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.6605080643627917\n",
      "tree depth: 2, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.6606479149401754\n",
      "tree depth: 2, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.6475631587179773\n",
      "tree depth: 2, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.6429413555475604\n",
      "tree depth: 2, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.6624777457179691\n",
      "avgfold accuracy: 0.6548276478572947\n",
      "standard deviation: 0.007984026367661781\n",
      "avg train time: 0.7092181205749511\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7699607614735913\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7723295580723961\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7671719834921293\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7603188221293555\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7652299493622254\n",
      "avgfold accuracy: 0.7670022149059396\n",
      "standard deviation: 0.004122403874987113\n",
      "avg train time: 0.7051053047180176\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.763616649304044\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7699434342777804\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7551092738807255\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7551970036623954\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7610366386181848\n",
      "avgfold accuracy: 0.7609805999486261\n",
      "standard deviation: 0.005571476492748832\n",
      "avg train time: 0.7065197467803955\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7499098719769273\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7367426232556795\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7470508581152799\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7474155665642672\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7360601553789516\n",
      "avgfold accuracy: 0.7434358150582211\n",
      "standard deviation: 0.005831287054346132\n",
      "avg train time: 0.6260866165161133\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.759328344206238\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7260303429150214\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7358670058637573\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7099456637053847\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7146710921084682\n",
      "avgfold accuracy: 0.7291684897597739\n",
      "standard deviation: 0.017572856819674142\n",
      "avg train time: 0.6535479068756104\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.7002809167264601\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.725890598679471\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.730689594381172\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6845141560346497\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.693398969025208\n",
      "avgfold accuracy: 0.7069548469693923\n",
      "standard deviation: 0.01818674632187141\n",
      "avg train time: 0.6810548782348633\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6995031638000826\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6940888549458446\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.7403440778876873\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6593838504045241\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6859193518681714\n",
      "avgfold accuracy: 0.6958478597812621\n",
      "standard deviation: 0.02617085409558783\n",
      "avg train time: 0.6583384037017822\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6816865861481296\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6815411225398064\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.70029250779332\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6551518002933606\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6707406937989024\n",
      "avgfold accuracy: 0.6778825421147039\n",
      "standard deviation: 0.014817254432359699\n",
      "avg train time: 0.6598684787750244\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.6717462345124906\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.6205642489469532\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.7056821790221979\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.6426733641454258\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.677075168479025\n",
      "avgfold accuracy: 0.6635482390212184\n",
      "standard deviation: 0.029355877280102076\n",
      "avg train time: 0.6729111194610595\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.6708232510034795\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.6341337030958584\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.6845439593465695\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.6710463196154945\n",
      "tree depth: 2, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.689272693426595\n",
      "avgfold accuracy: 0.6699639852975994\n",
      "standard deviation: 0.019344874053168603\n",
      "avg train time: 0.6401781558990478\n",
      "tree depth: 2, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.6605080643627917\n",
      "tree depth: 2, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.6504743901536284\n",
      "tree depth: 2, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.6478670968882695\n",
      "tree depth: 2, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.6429413555475604\n",
      "tree depth: 2, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.6739876900896904\n",
      "avgfold accuracy: 0.655155719408388\n",
      "standard deviation: 0.011022887667116116\n",
      "avg train time: 0.6580897331237793\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7699607614735913\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7714082210775473\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.766238459111946\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7603188221293555\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7652299493622254\n",
      "avgfold accuracy: 0.7666312426309332\n",
      "standard deviation: 0.0038953760565440163\n",
      "avg train time: 0.6486790657043457\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.763616649304044\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7699262822075256\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7551092738807255\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7551970036623954\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7610366386181848\n",
      "avgfold accuracy: 0.7609771695345751\n",
      "standard deviation: 0.005565959478344814\n",
      "avg train time: 0.7604171752929687\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7498933599727002\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7367426232556795\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7587243594673079\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7279813466913901\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7250039112086328\n",
      "avgfold accuracy: 0.7396691201191421\n",
      "standard deviation: 0.012867587273286109\n",
      "avg train time: 0.8056916236877442\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7527201650599986\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7260303429150214\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7358670058637573\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.7099456637053847\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.714635643685923\n",
      "avgfold accuracy: 0.727839764246017\n",
      "standard deviation: 0.015370703250865097\n",
      "avg train time: 0.6841531753540039\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7232228455086849\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7258734466092163\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.7321224457554067\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6888504506499138\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6860116365883171\n",
      "avgfold accuracy: 0.7112161650223078\n",
      "standard deviation: 0.019654832926679258\n",
      "avg train time: 0.6781097888946533\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6995196758043097\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6613217204790303\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.7279814843586931\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6530106366294892\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6859193518681714\n",
      "avgfold accuracy: 0.6855505738279388\n",
      "standard deviation: 0.026987177782124783\n",
      "avg train time: 0.7198511600494385\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6816865861481296\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6815068183992966\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7142350828349218\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6630527034382236\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6575363544366968\n",
      "avgfold accuracy: 0.6796035090514537\n",
      "standard deviation: 0.01984034363927495\n",
      "avg train time: 0.7059675216674804\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6813595984280572\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6389657121088199\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6732612356420602\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6665421267723872\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.7009032416493219\n",
      "avgfold accuracy: 0.6722063829201292\n",
      "standard deviation: 0.020225873345869866\n",
      "avg train time: 0.7349704742431641\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.6956921431880978\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.6340822468850937\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.6845439593465695\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.6710251017075803\n",
      "tree depth: 2, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.6892549692153225\n",
      "avgfold accuracy: 0.6749196840685328\n",
      "standard deviation: 0.021965819721643763\n",
      "avg train time: 0.721289587020874\n",
      "tree depth: 2, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6642794311464435\n",
      "tree depth: 2, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6504915422238833\n",
      "tree depth: 2, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6478670968882695\n",
      "tree depth: 2, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6429625734554747\n",
      "tree depth: 2, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6771106169015702\n",
      "avgfold accuracy: 0.6565422521231283\n",
      "standard deviation: 0.012483578415248749\n",
      "avg train time: 0.6711726188659668\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7727793731042468\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7714082210775473\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.766238459111946\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7600217714185555\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7652937169156308\n",
      "avgfold accuracy: 0.7671483083255852\n",
      "standard deviation: 0.004580624638027112\n",
      "avg train time: 0.6982217311859131\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7659844957283946\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7698748259967609\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7483904616773724\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.757540659969188\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7610366386181848\n",
      "avgfold accuracy: 0.76056541639798\n",
      "standard deviation: 0.007397732789853641\n",
      "avg train time: 0.7833023548126221\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7498933599727002\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7284289245290854\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.758659229859388\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7279813466913901\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7250039112086328\n",
      "avgfold accuracy: 0.7379933544522392\n",
      "standard deviation: 0.013631821541290041\n",
      "avg train time: 0.7325382232666016\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7571421298283852\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7260131908447666\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7358670058637573\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7143147077002555\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.7125441867557565\n",
      "avgfold accuracy: 0.7291762441985843\n",
      "standard deviation: 0.016336171883957615\n",
      "avg train time: 0.7117709636688232\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7235316324968252\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.712514692102284\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.7321224457554067\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.6888292327419994\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.696114238977818\n",
      "avgfold accuracy: 0.7106224484148667\n",
      "standard deviation: 0.01623520801167062\n",
      "avg train time: 0.6232790946960449\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.6942292046317673\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.6613217204790303\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.728062700195075\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.6719914390354154\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.6841469307409116\n",
      "avgfold accuracy: 0.68795039901644\n",
      "standard deviation: 0.022920128276465612\n",
      "avg train time: 0.6643673419952393\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6816865861481296\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6525718175236384\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7142350828349218\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6630314855303093\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6575363544366968\n",
      "avgfold accuracy: 0.6738122652947391\n",
      "standard deviation: 0.022486868234122673\n",
      "avg train time: 0.6461412906646729\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6813430864238301\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6583814945411279\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6732395257727536\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.653722359062353\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6862846290490912\n",
      "avgfold accuracy: 0.6705942189698312\n",
      "standard deviation: 0.012669136219881635\n",
      "avg train time: 0.6823574066162109\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.6707769673552673\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.634047942744584\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.6845439593465695\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.6794024852627791\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.6784468640028201\n",
      "avgfold accuracy: 0.6694436437424041\n",
      "standard deviation: 0.018236718080919135\n",
      "avg train time: 0.8150265693664551\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.6835177298896298\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.6410701808550398\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.675778795787553\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.6327180140038192\n",
      "tree depth: 2, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.6710420053905366\n",
      "avgfold accuracy: 0.6608253451853157\n",
      "standard deviation: 0.020115758688618302\n",
      "avg train time: 0.7022645473480225\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7727793731042468\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7714082210775473\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.766238459111946\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7603824758530984\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7652937169156308\n",
      "avgfold accuracy: 0.7672204492124937\n",
      "standard deviation: 0.004469307543528114\n",
      "avg train time: 0.7001309394836426\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7659844957283946\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7698576739265061\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7483904616773724\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7574770062454451\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7610366386181848\n",
      "avgfold accuracy: 0.7605492552391806\n",
      "standard deviation: 0.007398662243276772\n",
      "avg train time: 0.6431863307952881\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7498768479684731\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7284289245290854\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7586375199900814\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7279813466913901\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7444648971500656\n",
      "avgfold accuracy: 0.7418779072658191\n",
      "standard deviation: 0.012046158891071959\n",
      "avg train time: 0.7376790523529053\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7492906092729414\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7259617346340019\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7358670058637573\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.704797092224098\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7199812261985626\n",
      "avgfold accuracy: 0.7271795336386722\n",
      "standard deviation: 0.014957567900303562\n",
      "avg train time: 0.6872941493988037\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.730953840942365\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7335844757514863\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.7321224457554067\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.688786796926171\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.6961319631890905\n",
      "avgfold accuracy: 0.7163159045129038\n",
      "standard deviation: 0.01963447342543692\n",
      "avg train time: 0.6711794376373291\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.6942457166359943\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.6613217204790303\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.728062700195075\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.6651841806658733\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.6933172792246499\n",
      "avgfold accuracy: 0.6884263194401246\n",
      "standard deviation: 0.0240984889581561\n",
      "avg train time: 0.7157022476196289\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6888941385387176\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6377625395174671\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.713648916363644\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6585775699037814\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6844164575737832\n",
      "avgfold accuracy: 0.6766599243794786\n",
      "standard deviation: 0.026145758446758965\n",
      "avg train time: 0.6691456317901612\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.6866781525168797\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.6369343653463184\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.6732395257727536\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.65971180545946\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.7067239122384195\n",
      "avgfold accuracy: 0.6726575522667663\n",
      "standard deviation: 0.023669268051232782\n",
      "avg train time: 0.6933928012847901\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6707769673552673\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6451917330632334\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6845005396079564\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6743466268139005\n",
      "tree depth: 2, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6784291397915474\n",
      "avgfold accuracy: 0.6706490013263811\n",
      "standard deviation: 0.013521516070881071\n",
      "avg train time: 0.6524333953857422\n",
      "tree depth: 2, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.672974752645173\n",
      "tree depth: 2, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.6429520337841619\n",
      "tree depth: 2, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.6757353760489397\n",
      "tree depth: 2, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.6244351885164992\n",
      "tree depth: 2, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.6710420053905366\n",
      "avgfold accuracy: 0.657427871277062\n",
      "standard deviation: 0.020299171834171518\n",
      "avg train time: 0.7440732002258301\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7727793731042468\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7714082210775473\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.766238459111946\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7603824758530984\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7652937169156308\n",
      "avgfold accuracy: 0.7672204492124937\n",
      "standard deviation: 0.004469307543528114\n",
      "avg train time: 0.7250672340393066\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7659844957283946\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7670692889473865\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7484121715466792\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7596393878172307\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.754482344111106\n",
      "avgfold accuracy: 0.7591175376301594\n",
      "standard deviation: 0.007024922378106243\n",
      "avg train time: 0.6666468143463135\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7498768479684731\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7339885424170698\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7586375199900814\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7279813466913901\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7526320948829509\n",
      "avgfold accuracy: 0.7446232703899931\n",
      "standard deviation: 0.011646386584978923\n",
      "avg train time: 0.8490618705749512\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7492906092729414\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7335673236812315\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7358670058637573\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.718336423767747\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7199812261985626\n",
      "avgfold accuracy: 0.7314085177568479\n",
      "standard deviation: 0.011364597571638836\n",
      "avg train time: 0.9356416225433349\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7214378603244559\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7335844757514863\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.7418476825106862\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.6812848827018699\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.698712568743205\n",
      "avgfold accuracy: 0.7153734940063406\n",
      "standard deviation: 0.022400530811643837\n",
      "avg train time: 1.075305461883545\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.6941961806233131\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.6697971903103441\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.7280409903257683\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.6651841806658733\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.6933172792246499\n",
      "avgfold accuracy: 0.6901071642299899\n",
      "standard deviation: 0.02235911539378285\n",
      "avg train time: 0.9719524383544922\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6888611145302634\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6394997734121246\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.723352443249617\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6525456876908459\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6844164575737832\n",
      "avgfold accuracy: 0.6777350952913268\n",
      "standard deviation: 0.02948157877479328\n",
      "avg train time: 0.9361169815063477\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.6684916809520521\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.6368657570652989\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.6732612356420602\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.6467126080498897\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.7064048764355126\n",
      "avgfold accuracy: 0.6663472316289628\n",
      "standard deviation: 0.024131692055800763\n",
      "avg train time: 0.8769006729125977\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6422572335087608\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6392767964988111\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6986538050077082\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6810593271155638\n",
      "tree depth: 2, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6939907992530087\n",
      "avgfold accuracy: 0.6710475922767706\n",
      "standard deviation: 0.025404711313211627\n",
      "avg train time: 0.838655948638916\n",
      "tree depth: 2, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.6707390022546391\n",
      "tree depth: 2, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.6429348817139071\n",
      "tree depth: 2, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.6757570859182465\n",
      "tree depth: 2, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.6408274061568834\n",
      "tree depth: 2, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.6705989001087217\n",
      "avgfold accuracy: 0.6601714552304796\n",
      "standard deviation: 0.015063918758981453\n",
      "avg train time: 1.8077214717864991\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7727298370915656\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7714082210775473\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7629876023568025\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7613160638013265\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7652937169156308\n",
      "avgfold accuracy: 0.7667470882485745\n",
      "standard deviation: 0.004544479990408128\n",
      "avg train time: 1.7709601879119874\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7659844957283946\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7670692889473865\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7484121715466792\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7596393878172307\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.755634417843825\n",
      "avgfold accuracy: 0.7593479523767032\n",
      "standard deviation: 0.006886643911250893\n",
      "avg train time: 0.8912340641021729\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.749860335964246\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7416407128761494\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7586375199900814\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7231551951586269\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7526320948829509\n",
      "avgfold accuracy: 0.7451851717744109\n",
      "standard deviation: 0.012297662827346761\n",
      "avg train time: 0.8621838569641114\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7490362993896562\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7335501716109766\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7358452959944506\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.718336423767747\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7236004309260752\n",
      "avgfold accuracy: 0.7320737243377811\n",
      "standard deviation: 0.010626793777002862\n",
      "avg train time: 0.8846772193908692\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6910522700002901\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7335844757514863\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7418476825106862\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6812424468860414\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6986948445319323\n",
      "avgfold accuracy: 0.7092843439360872\n",
      "standard deviation: 0.024007338183005947\n",
      "avg train time: 0.9136248588562011\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7066991452786175\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6697971903103441\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7525828205357576\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6620554617662524\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6932995550133774\n",
      "avgfold accuracy: 0.6968868345808698\n",
      "standard deviation: 0.032119359353094325\n",
      "avg train time: 0.9145598411560059\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6426552103379156\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6394997734121246\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7233741531189235\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6525456876908459\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6843987333625106\n",
      "avgfold accuracy: 0.668494711584464\n",
      "standard deviation: 0.03170945801901413\n",
      "avg train time: 1.047736644744873\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6684916809520521\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6368486049950439\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7057529322709621\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.644519322133968\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7051819058577034\n",
      "avgfold accuracy: 0.6721588892419458\n",
      "standard deviation: 0.029131733946316622\n",
      "avg train time: 0.8095876216888428\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6815742544830091\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6457920555221541\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6793496769414509\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6773480382660356\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6939553508304634\n",
      "avgfold accuracy: 0.6756038752086226\n",
      "standard deviation: 0.015988995056538457\n",
      "avg train time: 0.8852969646453858\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6725388107153901\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5879686351826876\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6653371333451561\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6293752709895848\n",
      "tree depth: 2, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6641863992918237\n",
      "avgfold accuracy: 0.6438812499049285\n",
      "standard deviation: 0.03171924881456121\n",
      "avg train time: 0.8637542724609375\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7727298370915656\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7714082210775473\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7629876023568025\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7613160638013265\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7652937169156308\n",
      "avgfold accuracy: 0.7667470882485745\n",
      "standard deviation: 0.004544479990408128\n",
      "avg train time: 0.9587557315826416\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7659844957283946\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7670521368771317\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7484121715466792\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7596393878172307\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7577541939048517\n",
      "avgfold accuracy: 0.7597684771748575\n",
      "standard deviation: 0.006704181739647619\n",
      "avg train time: 0.8740200519561767\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7498768479684731\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7390041688558124\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.758615810120775\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7231764130665412\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7526143706716784\n",
      "avgfold accuracy: 0.7446575221366561\n",
      "standard deviation: 0.01248026485612987\n",
      "avg train time: 0.6637608051300049\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.759790680324596\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7335330195407217\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7405225957889131\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7340939492061735\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7236004309260752\n",
      "avgfold accuracy: 0.7383081351572959\n",
      "standard deviation: 0.012027312575047057\n",
      "avg train time: 1.0915369510650634\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7189065825855396\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7262336400845687\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7418259726413796\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6639844463509811\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6932606409629296\n",
      "avgfold accuracy: 0.7088422565250797\n",
      "standard deviation: 0.027374025232498357\n",
      "avg train time: 1.0616442203521728\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7066826332743905\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6672366570434525\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7525828205357576\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6620554617662524\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6932995550133774\n",
      "avgfold accuracy: 0.696371425526646\n",
      "standard deviation: 0.03256347163897532\n",
      "avg train time: 0.8594969272613525\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6873634632377739\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6340945241564341\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7233741531189235\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6676357715477079\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6843632849399655\n",
      "avgfold accuracy: 0.6793662394001609\n",
      "standard deviation: 0.029026727684189065\n",
      "avg train time: 0.6441782474517822\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6684751689478251\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6367971487842793\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6996532436898826\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6485682524746539\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.7051641816464307\n",
      "avgfold accuracy: 0.6717315991086144\n",
      "standard deviation: 0.02707366467773104\n",
      "avg train time: 0.6639960289001465\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6815577424787821\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6475292894168115\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7027569703066532\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6772843845422928\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6871883657881135\n",
      "avgfold accuracy: 0.6792633505065306\n",
      "standard deviation: 0.0180609369224412\n",
      "avg train time: 0.7494898319244385\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6725388107153901\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5879857872529425\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6653371333451561\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6419653317835036\n",
      "tree depth: 2, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6636512073257432\n",
      "avgfold accuracy: 0.6462956540845471\n",
      "standard deviation: 0.0308910624350848\n",
      "avg train time: 0.7427470684051514\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7727298370915656\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7677916619272789\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7629876023568025\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7659742509181833\n",
      "standard deviation: 0.004052014182289035\n",
      "avg train time: 0.7002842426300049\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7634416470774252\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7670521368771317\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7484121715466792\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7649092703807231\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7577541939048517\n",
      "avgfold accuracy: 0.7603138839573622\n",
      "standard deviation: 0.006700855205084591\n",
      "avg train time: 0.8206661701202392\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7512308323150931\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7419985592260986\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.758615810120775\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7438855524497459\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7526143706716784\n",
      "avgfold accuracy: 0.7496690249566782\n",
      "standard deviation: 0.006057125838887764\n",
      "avg train time: 0.8048227310180665\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7458923887666333\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7335158674704668\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7405660155275263\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.7340939492061735\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.72356498250353\n",
      "avgfold accuracy: 0.735526640694866\n",
      "standard deviation: 0.007510274460224305\n",
      "avg train time: 0.6387598037719726\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6856131907897041\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.7262164880143138\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.741804262772073\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6643875866013524\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6899427478270512\n",
      "avgfold accuracy: 0.701592855200899\n",
      "standard deviation: 0.028279276086441627\n",
      "avg train time: 0.6330569744110107\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.720559471735952\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.6970635655723646\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.7563981338928725\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.6620342438583382\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.6984962145441511\n",
      "avgfold accuracy: 0.7069103259207357\n",
      "standard deviation: 0.031038559954543573\n",
      "avg train time: 0.6189202785491943\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6495539757403632\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6340945241564341\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7033535993655486\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6676357715477079\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6866780273249907\n",
      "avgfold accuracy: 0.6682631796270089\n",
      "standard deviation: 0.02485304464569144\n",
      "avg train time: 0.6878006458282471\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.6684751689478251\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.647450931537963\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.6996532436898826\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.653932693106025\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.6710951780243545\n",
      "avgfold accuracy: 0.6681214430612101\n",
      "standard deviation: 0.01806551047960559\n",
      "avg train time: 0.6576790809631348\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.6814751824576467\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.6475121373465567\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.7135733241681066\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.6635849961715514\n",
      "tree depth: 2, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.6871706415768409\n",
      "avgfold accuracy: 0.6786632563441406\n",
      "standard deviation: 0.02234610262248527\n",
      "avg train time: 0.6456647872924804\n",
      "tree depth: 2, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.6693123400712317\n",
      "tree depth: 2, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5880029393231974\n",
      "tree depth: 2, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.6653371333451561\n",
      "tree depth: 2, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.6419441138755893\n",
      "tree depth: 2, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.6441513073338627\n",
      "avgfold accuracy: 0.6417495667898074\n",
      "standard deviation: 0.029019255459816026\n",
      "avg train time: 0.8359878063201904\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7727298370915656\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7677916619272789\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7629876023568025\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7659742509181833\n",
      "standard deviation: 0.004052014182289035\n",
      "avg train time: 0.6876992225646973\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7634251350731982\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7670521368771317\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7581647418121097\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7608045277170453\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7577541939048517\n",
      "avgfold accuracy: 0.7614401470768672\n",
      "standard deviation: 0.0034685198361584712\n",
      "avg train time: 0.7029737949371337\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7504217441079666\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7419814071558437\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7566836317524887\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7438643345418315\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7523839559251345\n",
      "avgfold accuracy: 0.749067014696653\n",
      "standard deviation: 0.005442866180278026\n",
      "avg train time: 0.6794134616851807\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7535193710827772\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7466590475365114\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.7406528550047526\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.727444902628253\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.719800320422054\n",
      "avgfold accuracy: 0.7376152993348696\n",
      "standard deviation: 0.012367425495865765\n",
      "avg train time: 0.6479321956634522\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.685596678785477\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.7262164880143138\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.741804262772073\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.6643875866013524\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.6899250236157788\n",
      "avgfold accuracy: 0.701586007957799\n",
      "standard deviation: 0.02828260356734399\n",
      "avg train time: 0.6195158004760742\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.720559471735952\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6970464135021097\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.7563981338928725\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6605277723964242\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6984784903328785\n",
      "avgfold accuracy: 0.7066020563720474\n",
      "standard deviation: 0.031478885176630625\n",
      "avg train time: 0.628893518447876\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6495539757403632\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6340773720861792\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7161833474316701\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6676357715477079\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6895953928932843\n",
      "avgfold accuracy: 0.671409171939841\n",
      "standard deviation: 0.029055178042102763\n",
      "avg train time: 0.6134267330169678\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.6690794832843477\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.6473823232569434\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.6996532436898826\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.6603386562606666\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.6710597296018093\n",
      "avgfold accuracy: 0.66950268721873\n",
      "standard deviation: 0.017233394925406847\n",
      "avg train time: 0.6375589370727539\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.675661268242012\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.6439862133464772\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.7135733241681066\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.6417416212326682\n",
      "tree depth: 2, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.6871351931542957\n",
      "avgfold accuracy: 0.6724195240287119\n",
      "standard deviation: 0.027093475102272706\n",
      "avg train time: 0.7180875778198242\n",
      "tree depth: 2, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.6692793160627777\n",
      "tree depth: 2, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5692658372286588\n",
      "tree depth: 2, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.6653588432144626\n",
      "tree depth: 2, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.6074797737986513\n",
      "tree depth: 2, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.6707017797484549\n",
      "avgfold accuracy: 0.6364171100106011\n",
      "standard deviation: 0.04108442515058759\n",
      "avg train time: 0.6397768497467041\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7702249535412244\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7677916619272789\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7629876023568025\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7654732742081152\n",
      "standard deviation: 0.0032640384369899134\n",
      "avg train time: 0.7669895648956299\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7624179028153468\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7620854389756422\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7628414468768161\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7522562227824436\n",
      "avgfold accuracy: 0.7595488086786104\n",
      "standard deviation: 0.004016607474410851\n",
      "avg train time: 0.8392202854156494\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7504217441079666\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7412265355165211\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7434092237126178\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7438643345418315\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.752366231713862\n",
      "avgfold accuracy: 0.7462576139185598\n",
      "standard deviation: 0.004331467521089157\n",
      "avg train time: 0.9430643558502197\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7535028590785502\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7202526229126383\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.740631145135446\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7141795588520189\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.7192757233755611\n",
      "avgfold accuracy: 0.7295683818708429\n",
      "standard deviation: 0.014998546516429293\n",
      "avg train time: 0.7405831336975097\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7153548129490139\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7262164880143138\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.7417825529027664\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.6643875866013524\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.6927183197051643\n",
      "avgfold accuracy: 0.7080919520345221\n",
      "standard deviation: 0.027065567309493707\n",
      "avg train time: 0.7504478454589844\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.720526447727498\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.6772973392625332\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.756376424023566\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.6627980885432523\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.6984607661216059\n",
      "avgfold accuracy: 0.7030918131356911\n",
      "standard deviation: 0.033026056262269836\n",
      "avg train time: 0.6546214580535888\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6495374637361362\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6953777878884718\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7021161368150732\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6504451148073321\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6895953928932843\n",
      "avgfold accuracy: 0.6774143792280596\n",
      "standard deviation: 0.02274071948395611\n",
      "avg train time: 0.6286482334136962\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6690299472716665\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6413081432613127\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6996532436898826\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6603386562606666\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6842108963301972\n",
      "avgfold accuracy: 0.6709081773627451\n",
      "standard deviation: 0.01997321062566967\n",
      "avg train time: 2.096678352355957\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.6692793160627777\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.6944467012957938\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.7196464639331064\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.6748673880755358\n",
      "tree depth: 2, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.6523786089563707\n",
      "avgfold accuracy: 0.6821236956647169\n",
      "standard deviation: 0.02307473456740668\n",
      "avg train time: 0.8700033664703369\n",
      "tree depth: 2, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.6692793160627777\n",
      "tree depth: 2, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.6532653028062592\n",
      "tree depth: 2, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.657137211082182\n",
      "tree depth: 2, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.6075009917065656\n",
      "tree depth: 2, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.6580291667244271\n",
      "avgfold accuracy: 0.6490423976764423\n",
      "standard deviation: 0.021444543480616717\n",
      "avg train time: 0.7066866397857666\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7702249535412244\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7677916619272789\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7659996505495736\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7660756838466692\n",
      "standard deviation: 0.0030184020682494424\n",
      "avg train time: 0.711016035079956\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7624179028153468\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7620682869053874\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7628414468768161\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7522384985711711\n",
      "avgfold accuracy: 0.759541833422305\n",
      "standard deviation: 0.004020883903616364\n",
      "avg train time: 0.8242436408996582\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7504382561121936\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.741243687586776\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433875138433113\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7237917323960553\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7412993926239556\n",
      "avgfold accuracy: 0.7400321165124584\n",
      "standard deviation: 0.008790567695846312\n",
      "avg train time: 0.8596364498138428\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7458379116617782\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7208874300601766\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7406094352661394\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.7141795588520189\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.719240274953016\n",
      "avgfold accuracy: 0.7281509221586259\n",
      "standard deviation: 0.012612700664544348\n",
      "avg train time: 0.7959465980529785\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7050149208474561\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7262336400845687\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.7417825529027664\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.6646422014963238\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.6927183197051643\n",
      "avgfold accuracy: 0.7060783270072559\n",
      "standard deviation: 0.026749211267521507\n",
      "avg train time: 0.7586024761199951\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.7208071517993582\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.6538484731949252\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.756376424023566\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.6742423823097999\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.6984430419103334\n",
      "avgfold accuracy: 0.7007434946475966\n",
      "standard deviation: 0.035789152670155346\n",
      "avg train time: 0.9252044677734375\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6894819408710132\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6953949399587267\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7213125002288692\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6504238968994179\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6557993817319823\n",
      "avgfold accuracy: 0.6824825319380018\n",
      "standard deviation: 0.026318187738630802\n",
      "avg train time: 0.9184202671051025\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6626331718159102\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.613007588436977\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6739937475576396\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6662566075332799\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6865965355603127\n",
      "avgfold accuracy: 0.6604975301808239\n",
      "standard deviation: 0.02512500137018865\n",
      "avg train time: 0.9971167564392089\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6692628040585507\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6709732806855051\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6780092886852869\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6748673880755358\n",
      "tree depth: 2, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6646009874068983\n",
      "avgfold accuracy: 0.6715427497823555\n",
      "standard deviation: 0.004616922299469252\n",
      "avg train time: 0.8105276107788086\n",
      "tree depth: 2, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.6692793160627777\n",
      "tree depth: 2, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.6411683990257624\n",
      "tree depth: 2, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.6458705736061349\n",
      "tree depth: 2, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.607458555890737\n",
      "tree depth: 2, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.6579759940906094\n",
      "avgfold accuracy: 0.6443505677352043\n",
      "standard deviation: 0.020883255924406933\n",
      "avg train time: 1.0809958457946778\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7702249535412244\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7677916619272789\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7679044954077088\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7664566528182963\n",
      "standard deviation: 0.0031037667149730933\n",
      "avg train time: 0.8196963310241699\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.76474115431919\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7620682869053874\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7628202289689019\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7675909331252636\n",
      "avgfold accuracy: 0.7630727270523092\n",
      "standard deviation: 0.003116805570648081\n",
      "avg train time: 0.8507214069366456\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7504052321037394\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7370611101180965\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433875138433113\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7345436766021827\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7412993926239556\n",
      "avgfold accuracy: 0.7413393850582571\n",
      "standard deviation: 0.005493110292914522\n",
      "avg train time: 0.9844354629516602\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7446275942610278\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7208531259196669\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7447399340752354\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7205933634074115\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7218208805071302\n",
      "avgfold accuracy: 0.7305269796340944\n",
      "standard deviation: 0.011566259082684634\n",
      "avg train time: 0.9246110439300537\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6979510354027378\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7410577230355915\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7417825529027664\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6658188728678309\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6927183197051643\n",
      "avgfold accuracy: 0.7078657007828182\n",
      "standard deviation: 0.029488108382119783\n",
      "avg train time: 0.8905453681945801\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.720774127790904\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6538484731949252\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7334145753261843\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6739628594359727\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7065570670094008\n",
      "avgfold accuracy: 0.6977114205514774\n",
      "standard deviation: 0.029572965672036603\n",
      "avg train time: 1.0681102752685547\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6894654288667861\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6953777878884718\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7213342100981757\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6575937047389736\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6568273859857929\n",
      "avgfold accuracy: 0.68411970351564\n",
      "standard deviation: 0.024448885253552894\n",
      "avg train time: 0.8543067932128906\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.662600147807456\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6756361161213211\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6739937475576396\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6543030839767895\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6873266938503918\n",
      "avgfold accuracy: 0.6707719578627197\n",
      "standard deviation: 0.011367511026381467\n",
      "avg train time: 0.7213667869567871\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6762177352935584\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.624705119802697\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.681021336878058\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.686859657376913\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6642748223123065\n",
      "avgfold accuracy: 0.6666157343327066\n",
      "standard deviation: 0.02223292921203541\n",
      "avg train time: 0.7123811721801758\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6740315209157057\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6411683990257624\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6628734555258942\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.621753890718549\n",
      "tree depth: 2, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6848844163585559\n",
      "avgfold accuracy: 0.6569423365088934\n",
      "standard deviation: 0.022781668260557702\n",
      "avg train time: 0.7760050296783447\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7702084415369974\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7711117610842997\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7679044954077088\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7671173702488551\n",
      "standard deviation: 0.0036262298158873585\n",
      "avg train time: 1.3020416736602782\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.76474115431919\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7620682869053874\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7628202289689019\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7675909331252636\n",
      "avgfold accuracy: 0.7630727270523092\n",
      "standard deviation: 0.003116805570648081\n",
      "avg train time: 1.074135684967041\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7503722080952853\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7370439580478416\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.743344094104698\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.739337078755339\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7332562643699786\n",
      "avgfold accuracy: 0.7406707206746286\n",
      "standard deviation: 0.005851016217432149\n",
      "avg train time: 0.9341936588287354\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7446275942610278\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7208531259196669\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7447616439445419\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.7205933634074115\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.72719478215063\n",
      "avgfold accuracy: 0.7316061019366557\n",
      "standard deviation: 0.010945265581656358\n",
      "avg train time: 1.027886724472046\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7098925919143217\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.7410577230355915\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.741804262772073\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.6636237419164384\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.6927005954938916\n",
      "avgfold accuracy: 0.7098157830264633\n",
      "standard deviation: 0.029752182635093622\n",
      "avg train time: 0.9012752056121827\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.660797086982236\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6538313211246702\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.7333928654568777\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.7101882858697959\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6541936077978608\n",
      "avgfold accuracy: 0.6824806334462881\n",
      "standard deviation: 0.03301782692703173\n",
      "avg train time: 0.8401799201965332\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.689432404858332\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6953434837479621\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7213342100981757\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6527597118054594\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6713184634792132\n",
      "avgfold accuracy: 0.6860376547978284\n",
      "standard deviation: 0.023099981295967593\n",
      "avg train time: 0.8695103645324707\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.662583635803229\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6772704375944493\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6722247855561886\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6573681491526675\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6996022449347372\n",
      "avgfold accuracy: 0.6738098506082544\n",
      "standard deviation: 0.014670331585920645\n",
      "avg train time: 0.9245153427124023\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.6761847112851043\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.6264423536973545\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.6809779171394449\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.6868384394689988\n",
      "tree depth: 2, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.664257098101034\n",
      "avgfold accuracy: 0.6669401039383873\n",
      "standard deviation: 0.021567257666471665\n",
      "avg train time: 0.8883024215698242\n",
      "tree depth: 2, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.6762160465658533\n",
      "tree depth: 2, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.6343100985973218\n",
      "tree depth: 2, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.6795941091446678\n",
      "tree depth: 2, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.6217326728106348\n",
      "tree depth: 2, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.658376521658194\n",
      "avgfold accuracy: 0.6540458897553344\n",
      "standard deviation: 0.022788712914925596\n",
      "avg train time: 0.989376163482666\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7702084415369974\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7711117610842997\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7679479151463219\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7671260541965778\n",
      "standard deviation: 0.0036281558716356093\n",
      "avg train time: 0.9356736183166504\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.76474115431919\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.765760856809101\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7628202289689019\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7545283874532387\n",
      "avgfold accuracy: 0.7611987318986471\n",
      "standard deviation: 0.004238438162661565\n",
      "avg train time: 0.9234821319580078\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7503722080952853\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7352895720829293\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433006743660848\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.739337078755339\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7332562643699786\n",
      "avgfold accuracy: 0.7403111595339233\n",
      "standard deviation: 0.006100796943862912\n",
      "avg train time: 0.9449770450592041\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7446441062652548\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7208531259196669\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7444086623947921\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7191948265205399\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7082160135773399\n",
      "avgfold accuracy: 0.7274633469355186\n",
      "standard deviation: 0.014593460866379825\n",
      "avg train time: 0.7142313480377197\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6933787738686025\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6856967983404019\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.741804262772073\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.663602524008524\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.692682871282619\n",
      "avgfold accuracy: 0.6954330460544441\n",
      "standard deviation: 0.025573618784315248\n",
      "avg train time: 0.703725528717041\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7183831395424474\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6538313211246702\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7333494457182644\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7101882858697959\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.704054487592062\n",
      "avgfold accuracy: 0.703961335969448\n",
      "standard deviation: 0.026920072004103308\n",
      "avg train time: 0.7356627941131592\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7253413231419241\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6878285298509215\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7346898339744283\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6527597118054594\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6713007392679406\n",
      "avgfold accuracy: 0.6943840276081348\n",
      "standard deviation: 0.031277175675197215\n",
      "avg train time: 0.6835661888122558\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6575788097947207\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6873630768286364\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6744769883232291\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6573893670605817\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6996022449347372\n",
      "avgfold accuracy: 0.6752820973883811\n",
      "standard deviation: 0.016562753585840695\n",
      "avg train time: 0.7972520351409912\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6771655493543807\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6264423536973545\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.7226521037909617\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6869233111006559\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6703081834366751\n",
      "avgfold accuracy: 0.6766983002760055\n",
      "standard deviation: 0.03092899928964136\n",
      "avg train time: 0.8482877254486084\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6761830225573993\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6442189398576198\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6614566903801216\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6217114549027205\n",
      "tree depth: 2, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6661821058739422\n",
      "avgfold accuracy: 0.6539504427143606\n",
      "standard deviation: 0.019153586750873215\n",
      "avg train time: 0.6756982326507568\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7702084415369974\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7711117610842997\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7684038224017602\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7672172356476654\n",
      "standard deviation: 0.003653306475640072\n",
      "avg train time: 0.7452204704284668\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7647081303107359\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7579837470593227\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7627990110609877\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7545106632419661\n",
      "avgfold accuracy: 0.7596289167231631\n",
      "standard deviation: 0.0036594771287713486\n",
      "avg train time: 0.8401115417480469\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7503722080952853\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7370096539073319\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433006743660848\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7310754711759333\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7428519939242593\n",
      "avgfold accuracy: 0.740922000293779\n",
      "standard deviation: 0.006495382716418386\n",
      "avg train time: 0.9455595493316651\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7427171428628613\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7208188217791571\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7444086623947921\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7192160444284541\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.7300695700046935\n",
      "avgfold accuracy: 0.7314460482939916\n",
      "standard deviation: 0.010578020301892477\n",
      "avg train time: 0.8887036323547364\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7381249918690889\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6856453421296371\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.7289037614571874\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6676108635688521\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6926651470713464\n",
      "avgfold accuracy: 0.7025900212192224\n",
      "standard deviation: 0.026699489922572046\n",
      "avg train time: 1.4180830478668214\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7183831395424474\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.6538313211246702\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7333277358489578\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.7144724582330095\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.6871883657881135\n",
      "avgfold accuracy: 0.7014406041074397\n",
      "standard deviation: 0.02808358247397168\n",
      "avg train time: 0.9050594329833984\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6889337923670509\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6877942257104117\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7204272345340722\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6527384938975451\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6406313185823007\n",
      "avgfold accuracy: 0.6781050130182761\n",
      "standard deviation: 0.028460680967936344\n",
      "avg train time: 0.8970359802246094\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.6575788097947207\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.643501080580426\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.6855755704856681\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.651810902314597\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.6811729269108977\n",
      "avgfold accuracy: 0.6639278580172618\n",
      "standard deviation: 0.016555362403448366\n",
      "avg train time: 1.015659236907959\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.6766371652191143\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.6264423536973545\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.7255773125065064\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.6868808752848274\n",
      "tree depth: 2, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.6597907948962193\n",
      "avgfold accuracy: 0.6750657003208044\n",
      "standard deviation: 0.03253011958032136\n",
      "avg train time: 0.7916932106018066\n",
      "tree depth: 2, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.6761665105531722\n",
      "tree depth: 2, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.6758320108184426\n",
      "tree depth: 2, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.6614566903801216\n",
      "tree depth: 2, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.6449206173488685\n",
      "tree depth: 2, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.6112875500783231\n",
      "avgfold accuracy: 0.6539326758357856\n",
      "standard deviation: 0.024208919938020663\n",
      "avg train time: 0.8929625988006592\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7702084415369974\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7711117610842997\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7683821125324537\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7672128936738041\n",
      "standard deviation: 0.003651906263660233\n",
      "avg train time: 0.9198215484619141\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7647081303107359\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7579665949890678\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7627777931530734\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7545283874532387\n",
      "avgfold accuracy: 0.7596247875697838\n",
      "standard deviation: 0.0036524050747691545\n",
      "avg train time: 0.9583605766296387\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7504052321037394\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7426624346190165\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433006743660848\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7310754711759333\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7428342697129866\n",
      "avgfold accuracy: 0.7420556163955521\n",
      "standard deviation: 0.006209740255653108\n",
      "avg train time: 0.8642398834228515\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7411798377420384\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7234748649951522\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7444086623947921\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7192372623363684\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.7300518457934209\n",
      "avgfold accuracy: 0.7316704946523543\n",
      "standard deviation: 0.009767928735014586\n",
      "avg train time: 0.7945178031921387\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7381415038733159\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.6855595817783628\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.7475622170850917\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.6676108635688521\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.6926651470713464\n",
      "avgfold accuracy: 0.7063078626753938\n",
      "standard deviation: 0.031078529755333376\n",
      "avg train time: 0.6988921165466309\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.7183666275382203\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6537970169841605\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.7211758326781137\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6724370151016152\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6969436132438476\n",
      "avgfold accuracy: 0.6925440211091916\n",
      "standard deviation: 0.02612112580541656\n",
      "avg train time: 0.8573579788208008\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7130926683699049\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6621867264642\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7204272345340722\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6823324015904206\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6405958701597556\n",
      "avgfold accuracy: 0.6837269802236706\n",
      "standard deviation: 0.03011800904336903\n",
      "avg train time: 0.8287969112396241\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6575953217989479\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6434667764399162\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6820424854295388\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6532749379606823\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6558277008628424\n",
      "avgfold accuracy: 0.6584414444983855\n",
      "standard deviation: 0.012770984188497696\n",
      "avg train time: 0.8688549995422363\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6542219318444509\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6268957099964071\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6984962122817178\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6869020931927416\n",
      "tree depth: 2, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6591349990791331\n",
      "avgfold accuracy: 0.6651301892788901\n",
      "standard deviation: 0.02531448234844668\n",
      "avg train time: 0.9795741558074951\n",
      "tree depth: 2, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.6761499985489451\n",
      "tree depth: 2, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.6483672131857894\n",
      "tree depth: 2, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.6613915607722017\n",
      "tree depth: 2, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.6174157510678142\n",
      "tree depth: 2, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.6112875500783231\n",
      "avgfold accuracy: 0.6429224147306147\n",
      "standard deviation: 0.025004727812568354\n",
      "avg train time: 0.8710773944854736\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7702084415369974\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7711289131545547\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7683821125324537\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7611038847221838\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.767216324087855\n",
      "standard deviation: 0.003655573264020155\n",
      "avg train time: 0.9295145034790039\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7650152285711712\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7579665949890678\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7626929215214162\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7545106632419661\n",
      "avgfold accuracy: 0.7596656880532848\n",
      "standard deviation: 0.003729917047336359\n",
      "avg train time: 0.9534111976623535\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7503887200995123\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7426452825487615\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433006743660848\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7376627090655818\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7428165455017139\n",
      "avgfold accuracy: 0.7433627863163308\n",
      "standard deviation: 0.004066243346592764\n",
      "avg train time: 0.9547053813934326\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7499725425384254\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7234748649951522\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7465796493254508\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7201477873412117\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.7300163973708756\n",
      "avgfold accuracy: 0.7340382483142232\n",
      "standard deviation: 0.012098708443026091\n",
      "avg train time: 1.0167768478393555\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6995724641814599\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6855424297081079\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7475622170850917\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.7250085332890525\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6942531967941952\n",
      "avgfold accuracy: 0.7103877682115816\n",
      "standard deviation: 0.022760898321194246\n",
      "avg train time: 1.0854381561279296\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7192797288628834\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.649139056347259\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.7077060358144837\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.674568492329265\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6969258890325749\n",
      "avgfold accuracy: 0.6895238404772932\n",
      "standard deviation: 0.02500155463633569\n",
      "avg train time: 0.907330846786499\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7476806263153313\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6710418889665246\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7203838147954591\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6770122418103488\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6405958701597556\n",
      "avgfold accuracy: 0.6913428884094839\n",
      "standard deviation: 0.03797386993519142\n",
      "avg train time: 0.8038386821746826\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6987451752424012\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6458240125372606\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6820424854295388\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6655642579728596\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6907509322539058\n",
      "avgfold accuracy: 0.6765853726871931\n",
      "standard deviation: 0.018916798113988102\n",
      "avg train time: 0.7847747325897216\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6879231826537893\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6643968158535682\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6984962122817178\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6684420520484506\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6590818264453153\n",
      "avgfold accuracy: 0.6756680178565683\n",
      "standard deviation: 0.01500447831084246\n",
      "avg train time: 0.8678309917449951\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6761665105531722\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6384583719254915\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6759524747420056\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.661536545540088\n",
      "tree depth: 2, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.656745894221115\n",
      "avgfold accuracy: 0.6617719593963745\n",
      "standard deviation: 0.0139794866147906\n",
      "avg train time: 0.8168588161468506\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7702084415369974\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7631165492201224\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7683821125324537\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7603612579451839\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7654653259455687\n",
      "standard deviation: 0.0035388112284852127\n",
      "avg train time: 0.7010966777801514\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7650152285711712\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7579665949890678\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7626929215214162\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7545106632419661\n",
      "avgfold accuracy: 0.7596656880532848\n",
      "standard deviation: 0.003729917047336359\n",
      "avg train time: 0.8529923915863037\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7503556960910582\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.742576674267742\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433006743660848\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7376414911576674\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7428165455017139\n",
      "avgfold accuracy: 0.7433382162768533\n",
      "standard deviation: 0.00406327936648902\n",
      "avg train time: 0.9256111145019531\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7499725425384254\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7234748649951522\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7335923085333388\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7201477873412117\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.7088789386861112\n",
      "avgfold accuracy: 0.7272132884188478\n",
      "standard deviation: 0.013847173292739585\n",
      "avg train time: 0.798130989074707\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.735228761309472\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.6855424297081079\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7475622170850917\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.7250297511969668\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.6942886452167404\n",
      "avgfold accuracy: 0.7175303609032757\n",
      "standard deviation: 0.023810887037827775\n",
      "avg train time: 0.8879541873931884\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.7141940315609447\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.649139056347259\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.7295124224918431\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.6742829730901576\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.6876740487841587\n",
      "avgfold accuracy: 0.6909605064548726\n",
      "standard deviation: 0.02850972301244717\n",
      "avg train time: 0.9104909896850586\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7476641143111041\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6710247368962696\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7203621049261525\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6839892434432051\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6983085755477177\n",
      "avgfold accuracy: 0.7042697550248899\n",
      "standard deviation: 0.027181244211227393\n",
      "avg train time: 1.1682979583740234\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6987616872466282\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6551105044694684\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6639701962729124\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6494054373195325\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6833884533199726\n",
      "avgfold accuracy: 0.6701272557257029\n",
      "standard deviation: 0.018377137903470963\n",
      "avg train time: 1.0333438396453858\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.6879231826537893\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.6644139679238231\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.6984527925431046\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.6903084899307189\n",
      "tree depth: 2, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.6590641022340428\n",
      "avgfold accuracy: 0.6800325070570958\n",
      "standard deviation: 0.015432292559813125\n",
      "avg train time: 0.979006576538086\n",
      "tree depth: 2, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.6486754620609183\n",
      "tree depth: 2, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.629622889166933\n",
      "tree depth: 2, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.6789428130654702\n",
      "tree depth: 2, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.6455995904021254\n",
      "tree depth: 2, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.656745894221115\n",
      "avgfold accuracy: 0.6519173297833124\n",
      "standard deviation: 0.01613169179459844\n",
      "avg train time: 0.9088935375213623\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7714633538582549\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7631165492201224\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.768360402663147\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7658548510595116\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7652582684930855\n",
      "avgfold accuracy: 0.7668106850588243\n",
      "standard deviation: 0.002864234872804498\n",
      "avg train time: 0.8128043174743652\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7649987165669442\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.757932290848558\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7626929215214162\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7544929390306936\n",
      "avgfold accuracy: 0.759651979982083\n",
      "standard deviation: 0.0037332258434994007\n",
      "avg train time: 0.6620140075683594\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7503391840868312\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7529780507665169\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7433006743660848\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7502066439727304\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7428165455017139\n",
      "avgfold accuracy: 0.7479282197387754\n",
      "standard deviation: 0.004099954857516319\n",
      "avg train time: 0.6772356510162354\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7499560305341983\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7234234087843876\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7335705986640322\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7201477873412117\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7088789386861112\n",
      "avgfold accuracy: 0.7271953528019882\n",
      "standard deviation: 0.013842535730791366\n",
      "avg train time: 0.6547693729400634\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.735228761309472\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7041961185767753\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.7475187973464785\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6926544525318499\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6942886452167404\n",
      "avgfold accuracy: 0.7147773549962632\n",
      "standard deviation: 0.022411758848448225\n",
      "avg train time: 0.7081360816955566\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7141610075524907\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6491219042770041\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.7033696855940108\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6742617551822434\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6876740487841587\n",
      "avgfold accuracy: 0.6857176802779816\n",
      "standard deviation: 0.022781275542720615\n",
      "avg train time: 0.6746964454650879\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7476641143111041\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6710075848260149\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7203838147954591\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6839892434432051\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6983085755477177\n",
      "avgfold accuracy: 0.7042706665847003\n",
      "standard deviation: 0.02718801248016728\n",
      "avg train time: 0.6681686878204346\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.686291746599778\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6550075920479391\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6937811163894866\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6493842194116182\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6833884533199726\n",
      "avgfold accuracy: 0.6735706255537589\n",
      "standard deviation: 0.017867563110543692\n",
      "avg train time: 0.6684497356414795\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6682176067000709\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6284148417766656\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6984527925431046\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6902872720228046\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6590641022340428\n",
      "avgfold accuracy: 0.6688873230553377\n",
      "standard deviation: 0.024767077865566443\n",
      "avg train time: 0.6909929752349854\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6639706441583031\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.629622889166933\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6789428130654702\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6655467301358868\n",
      "tree depth: 2, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6878052475547519\n",
      "avgfold accuracy: 0.6651776648162689\n",
      "standard deviation: 0.019835432873839157\n",
      "avg train time: 0.6809403419494628\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7714633538582549\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7657162614264383\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.768360402663147\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7614433712488122\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7665344117047126\n",
      "avgfold accuracy: 0.766703560180273\n",
      "standard deviation: 0.0032885778233389296\n",
      "avg train time: 0.6756878852844238\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7649987165669442\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.757932290848558\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7581430319428031\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7626929215214162\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7544929390306936\n",
      "avgfold accuracy: 0.759651979982083\n",
      "standard deviation: 0.0037332258434994007\n",
      "avg train time: 0.7390422344207763\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7453723231789511\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7529780507665169\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7462910126895494\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7501854260648161\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7428342697129866\n",
      "avgfold accuracy: 0.747532216482564\n",
      "standard deviation: 0.0036048040702429188\n",
      "avg train time: 0.6321292400360108\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7499560305341983\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7234062567141327\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7335705986640322\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7201477873412117\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7088789386861112\n",
      "avgfold accuracy: 0.7271919223879373\n",
      "standard deviation: 0.013843472150754406\n",
      "avg train time: 0.6571518898010253\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7352122493052449\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7196719801974836\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7459122670177911\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.7172792184429746\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.6942531967941952\n",
      "avgfold accuracy: 0.7224657823515377\n",
      "standard deviation: 0.017566250723992212\n",
      "avg train time: 0.6693498134613037\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.7141279835440365\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6491219042770041\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.7033479757247042\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6742617551822434\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6848807526947732\n",
      "avgfold accuracy: 0.6851480742845524\n",
      "standard deviation: 0.022748801651786422\n",
      "avg train time: 0.728261661529541\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7020196182624767\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6709904327557599\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.7231570544258576\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6839892434432051\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6674971631360169\n",
      "avgfold accuracy: 0.6895307024046632\n",
      "standard deviation: 0.02072018720154517\n",
      "avg train time: 0.7006922245025635\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.6863082586040051\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.6660410674726334\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.6937811163894866\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.6494054373195325\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.6833707291086999\n",
      "avgfold accuracy: 0.6757813217788714\n",
      "standard deviation: 0.016018538906687634\n",
      "avg train time: 0.7056305408477783\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.6785030216967736\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.6295248515443183\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.7221423142092926\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.6924399671583686\n",
      "tree depth: 2, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.6695778271107159\n",
      "avgfold accuracy: 0.6784375963438938\n",
      "standard deviation: 0.030255991629065938\n",
      "avg train time: 0.7034208297729492\n",
      "tree depth: 2, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.6568241986924495\n",
      "tree depth: 2, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.6313772751318453\n",
      "tree depth: 2, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.7245738195716303\n",
      "tree depth: 2, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.6457268978496111\n",
      "tree depth: 2, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.6877875233434794\n",
      "avgfold accuracy: 0.6692579429178032\n",
      "standard deviation: 0.033305989341576164\n",
      "avg train time: 0.6974251747131348\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7667985250276952\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7613993561654471\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7578390937725109\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7559626933827803\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7579456946009477\n",
      "avgfold accuracy: 0.7599890725898762\n",
      "standard deviation: 0.0038305893621356675\n",
      "avg train time: 0.77179856300354\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7356069737699307\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.6967033720970122\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7426019042955723\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7243719960516242\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.7306404084291992\n",
      "avgfold accuracy: 0.7259849309286677\n",
      "standard deviation: 0.015813868784094284\n",
      "avg train time: 0.7731596469879151\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.697107234459452\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6790616553733825\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6979381640146622\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.671107667044899\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.687996748250848\n",
      "avgfold accuracy: 0.6866422938286487\n",
      "standard deviation: 0.01037050549536692\n",
      "avg train time: 1.1850532054901124\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.6570652489359765\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.6364788424699703\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.708483536856819\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.6220237271561546\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.6599044674914201\n",
      "avgfold accuracy: 0.6567911645820681\n",
      "standard deviation: 0.02933678327076878\n",
      "avg train time: 0.9600793361663819\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6310536510042801\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6222453323800393\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6273326993005237\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6297221376580965\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6305537677316376\n",
      "avgfold accuracy: 0.6281815176149155\n",
      "standard deviation: 0.003231377991311173\n",
      "avg train time: 1.1289939403533935\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.6161217329998909\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.6023664440508496\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.6416330948386486\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5957591859703503\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5971442235904301\n",
      "avgfold accuracy: 0.610604936290034\n",
      "standard deviation: 0.01710345878567621\n",
      "avg train time: 1.0573719978332519\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5988087089313932\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5489975066306292\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6333302468699862\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5773125213332226\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5884560924748345\n",
      "avgfold accuracy: 0.5893810152480132\n",
      "standard deviation: 0.02755992802642083\n",
      "avg train time: 1.0551319599151612\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5942183717562668\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5826150226858697\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.6323750126204963\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5968892701962195\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.6106889866305978\n",
      "avgfold accuracy: 0.60335733277789\n",
      "standard deviation: 0.017037480809865092\n",
      "avg train time: 1.2473050117492677\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5936057388721601\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5602393345718933\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.6287655506747585\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5786395630956005\n",
      "tree depth: 3, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.663963113836965\n",
      "avgfold accuracy: 0.6050426602102755\n",
      "standard deviation: 0.037066588952221764\n",
      "avg train time: 1.1702391147613525\n",
      "tree depth: 3, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5944412212678618\n",
      "tree depth: 3, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5537338251463794\n",
      "tree depth: 3, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.6381064181174352\n",
      "tree depth: 3, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.580039945017943\n",
      "tree depth: 3, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5905724029079589\n",
      "avgfold accuracy: 0.5913787624915157\n",
      "standard deviation: 0.027346710073599626\n",
      "avg train time: 0.8282296180725097\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7657747807656166\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7614336603059568\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7612202097434939\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7559626933827803\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7638193397879828\n",
      "avgfold accuracy: 0.7616421367971661\n",
      "standard deviation: 0.003295345851846461\n",
      "avg train time: 0.8096197128295899\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7367248489651976\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6967033720970122\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7363663328578087\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7258360316977094\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.7263654078846005\n",
      "avgfold accuracy: 0.7243991987004657\n",
      "standard deviation: 0.014615903894420223\n",
      "avg train time: 0.8158301830291748\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.699128328795034\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6845429153825182\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6979381640146622\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6710864491369847\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6791880132842469\n",
      "avgfold accuracy: 0.6863767741226893\n",
      "standard deviation: 0.01081760824984614\n",
      "avg train time: 0.8080600738525391\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.6654946896393679\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.6152277885203902\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.7084618269875124\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.6335492024834177\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.6738426288074874\n",
      "avgfold accuracy: 0.659315227287635\n",
      "standard deviation: 0.03247217178817142\n",
      "avg train time: 0.9060342311859131\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6365819200558807\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6173912964979084\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6514234612018479\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6275809739942251\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6177464903091142\n",
      "avgfold accuracy: 0.6301448284117952\n",
      "standard deviation: 0.012784060980061665\n",
      "avg train time: 0.8423405647277832\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6257119550913514\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6023835961211045\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6473259195437475\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5842318656076163\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.6117770947740312\n",
      "avgfold accuracy: 0.6142860862275702\n",
      "standard deviation: 0.021323234045399455\n",
      "avg train time: 0.8743381023406982\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6074577467819104\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5497865018623538\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6211895909808309\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6010539765126984\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6063642790800837\n",
      "avgfold accuracy: 0.5971704190435754\n",
      "standard deviation: 0.02460729331304488\n",
      "avg train time: 1.0351920127868652\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5961221933345542\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5766535047095973\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.6385832505481088\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5858596481517357\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.604598987244509\n",
      "avgfold accuracy: 0.6003635167977011\n",
      "standard deviation: 0.021303534737194783\n",
      "avg train time: 0.8307306289672851\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5881055527368397\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5730002942934159\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.6060754676907447\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5492001771234052\n",
      "tree depth: 3, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.6957279699936034\n",
      "avgfold accuracy: 0.6024218923676019\n",
      "standard deviation: 0.0502444260028671\n",
      "avg train time: 0.9419526576995849\n",
      "tree depth: 3, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5744598823344571\n",
      "tree depth: 3, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5512321505626782\n",
      "tree depth: 3, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.6173477288076364\n",
      "tree depth: 3, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5737825072187013\n",
      "tree depth: 3, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5724124136811107\n",
      "avgfold accuracy: 0.5778469365209167\n",
      "standard deviation: 0.021569354107123944\n",
      "avg train time: 1.1517154216766357\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7657747807656166\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.759336232935947\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7716072051655887\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7661957213627433\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.766665808511186\n",
      "avgfold accuracy: 0.7659159497482164\n",
      "standard deviation: 0.003907519687929304\n",
      "avg train time: 1.0202312469482422\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.735000970705703\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7160039648364505\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.728583737058433\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7256547569627025\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6959547210763646\n",
      "avgfold accuracy: 0.7202396301279308\n",
      "standard deviation: 0.013598106948113033\n",
      "avg train time: 0.9440331935882569\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6948285778761158\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6683026130727663\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7131447504228193\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6803240804804472\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6791702890729743\n",
      "avgfold accuracy: 0.6871540621850245\n",
      "standard deviation: 0.01549309835371009\n",
      "avg train time: 0.9434335708618165\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.6748653145837024\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.602763288792115\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.6980692079245732\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.6580023800957573\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.668745185252664\n",
      "avgfold accuracy: 0.6604890753297624\n",
      "standard deviation: 0.031703488945433025\n",
      "avg train time: 1.0751428604125977\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6486523202368122\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6202679695018136\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6530734112691485\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6057800348711704\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.6115395507358024\n",
      "avgfold accuracy: 0.6278626573229494\n",
      "standard deviation: 0.01938843730016178\n",
      "avg train time: 0.9986284255981446\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6486753369699771\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5937611601299224\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6379158682404491\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6051029068533843\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.6106959178864027\n",
      "avgfold accuracy: 0.6192302380160271\n",
      "standard deviation: 0.020674896195696367\n",
      "avg train time: 1.111182737350464\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5989622580616107\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5470372958226584\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6299435072581586\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6009497320085979\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5855067441118982\n",
      "avgfold accuracy: 0.5924799074525847\n",
      "standard deviation: 0.026947861015466118\n",
      "avg train time: 1.0280407905578612\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6017594165958652\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5960546627451617\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6235833002454\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.6105250048432181\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5939186151746974\n",
      "avgfold accuracy: 0.6051681999208685\n",
      "standard deviation: 0.010853946743095488\n",
      "avg train time: 1.1066879272460937\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.6017825584199713\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5623734132078163\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5907329984259037\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5763401876401074\n",
      "tree depth: 3, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5906290411696791\n",
      "avgfold accuracy: 0.5843716397726956\n",
      "standard deviation: 0.01364689151167829\n",
      "avg train time: 1.236847496032715\n",
      "tree depth: 3, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5814527786700932\n",
      "tree depth: 3, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5692317136362569\n",
      "tree depth: 3, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6239306581543054\n",
      "tree depth: 3, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5516942038210685\n",
      "tree depth: 3, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.6080446135230781\n",
      "avgfold accuracy: 0.5868707935609604\n",
      "standard deviation: 0.026075462439395573\n",
      "avg train time: 1.3054794788360595\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7657747807656166\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7636903311071752\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7703753662559578\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7652621334145149\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7668784990464572\n",
      "avgfold accuracy: 0.7663962221179443\n",
      "standard deviation: 0.002238766258931615\n",
      "avg train time: 0.8729442119598388\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7209128536445997\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.719275135456218\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7353893887390124\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7256335390547883\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7269503068565963\n",
      "avgfold accuracy: 0.725632244750243\n",
      "standard deviation: 0.005650214848892237\n",
      "avg train time: 0.8259785652160645\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7013128544451817\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6720858181266689\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7216108147583169\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.665654664710929\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6791348406504291\n",
      "avgfold accuracy: 0.6879597985383051\n",
      "standard deviation: 0.020678864212430138\n",
      "avg train time: 0.9155640602111816\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6823420001340975\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6083646132930349\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6841049230136647\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6441429348979234\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6687097368301189\n",
      "avgfold accuracy: 0.6575328416337679\n",
      "standard deviation: 0.02843693767319406\n",
      "avg train time: 0.8572192668914795\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.6263625530760863\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5668181465292335\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.6530082816612288\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.6152741261450752\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.6217024550508061\n",
      "avgfold accuracy: 0.616633112492486\n",
      "standard deviation: 0.028024599400921044\n",
      "avg train time: 0.7778648376464844\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.6175846715562213\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5961893516336896\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.6248963549914128\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.628229042703346\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5986153331260557\n",
      "avgfold accuracy: 0.613102950802145\n",
      "standard deviation: 0.013296087862617685\n",
      "avg train time: 1.0822643756866455\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5993718058028186\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5369692111311519\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6424476072846812\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6052223729001189\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.597654760089433\n",
      "avgfold accuracy: 0.5963331514416408\n",
      "standard deviation: 0.0338839748989681\n",
      "avg train time: 1.063099193572998\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6143284292680778\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6067353471669293\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.6056686038147641\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5778545005027722\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5979632009727522\n",
      "avgfold accuracy: 0.6005100163450592\n",
      "standard deviation: 0.012458514926395579\n",
      "avg train time: 1.1245124340057373\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5848790820926815\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.6218066004777303\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.6015003089078994\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5647704314615448\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5843689289625494\n",
      "avgfold accuracy: 0.5914650703804811\n",
      "standard deviation: 0.01911883193454645\n",
      "avg train time: 0.9952751636505127\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5756223524502313\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5428792728966597\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.6237843127100038\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5671237742045591\n",
      "tree depth: 3, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5560674232957528\n",
      "avgfold accuracy: 0.5730954271114413\n",
      "standard deviation: 0.027620202761061752\n",
      "avg train time: 0.8363255500793457\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.767222896045425\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.759336232935947\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7703102366480381\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7663327152464505\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7620363237411354\n",
      "avgfold accuracy: 0.7650476809233993\n",
      "standard deviation: 0.003892718252994515\n",
      "avg train time: 0.7855639934539795\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7329980771020532\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7207379362267974\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7344936604565978\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7256335390547883\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.7196799146069285\n",
      "avgfold accuracy: 0.7267086254894332\n",
      "standard deviation: 0.006105271309454094\n",
      "avg train time: 0.7757926464080811\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7012633184325003\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6719829057051396\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7141812005086909\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6562472901041523\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6727400640448036\n",
      "avgfold accuracy: 0.6832829557590573\n",
      "standard deviation: 0.021202669099904106\n",
      "avg train time: 0.7822498798370361\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.6720235611289407\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.639568562241252\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.7297254669322071\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.637165933265067\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.6668558239381811\n",
      "avgfold accuracy: 0.6690678695011296\n",
      "standard deviation: 0.033409328185716035\n",
      "avg train time: 0.8266431808471679\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.6643470428001157\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.6188345980728295\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.6511412329008623\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.6185052445133258\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.6334994524307914\n",
      "avgfold accuracy: 0.637265514143585\n",
      "standard deviation: 0.018065294739014814\n",
      "avg train time: 1.5713191986083985\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.607343851480026\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.6102415914232426\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.6249180648607194\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5888785874408435\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5957511401915799\n",
      "avgfold accuracy: 0.6054266470792824\n",
      "standard deviation: 0.012443700737413226\n",
      "avg train time: 0.7634016513824463\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6077384508537707\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5478214162554683\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6485151234088365\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6027417227096191\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5800370921203504\n",
      "avgfold accuracy: 0.5973707610696091\n",
      "standard deviation: 0.033189382851262446\n",
      "avg train time: 0.8796290397644043\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5939524909609286\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5847173248547942\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5936043248152176\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5936756796649416\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5928551624983415\n",
      "avgfold accuracy: 0.5917609965588447\n",
      "standard deviation: 0.003540537483193115\n",
      "avg train time: 0.7742005825042725\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5885183528425165\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6215810958909056\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6215369488897363\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5674863236745726\n",
      "tree depth: 3, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.6139820262635184\n",
      "avgfold accuracy: 0.6026209495122499\n",
      "standard deviation: 0.021352758199727196\n",
      "avg train time: 0.7459056377410889\n",
      "tree depth: 3, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5781107239963453\n",
      "tree depth: 3, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5576739265060872\n",
      "tree depth: 3, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.6160668465185478\n",
      "tree depth: 3, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5668806907812802\n",
      "tree depth: 3, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5575491277509659\n",
      "avgfold accuracy: 0.5752562631106454\n",
      "standard deviation: 0.02175538131133435\n",
      "avg train time: 0.7781574726104736\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7671898720369709\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7667654264817132\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7766109376937214\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7663114973385363\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7667012569337313\n",
      "avgfold accuracy: 0.7687157980969348\n",
      "standard deviation: 0.003957389727225844\n",
      "avg train time: 0.8183390617370605\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7353856878950997\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6867504770983752\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.734428530848678\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7256335390547883\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.7235543875839425\n",
      "avgfold accuracy: 0.7211505244961769\n",
      "standard deviation: 0.017822371155037355\n",
      "avg train time: 0.7877403736114502\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6921965393841323\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6594106187564208\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6797026584912007\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6562685080120665\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.696100178430328\n",
      "avgfold accuracy: 0.6767357006148297\n",
      "standard deviation: 0.016382233120039707\n",
      "avg train time: 0.7537140369415283\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.6789437796277894\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.6172100261975304\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.7076858953333198\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.6377697211228885\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.6362890848563942\n",
      "avgfold accuracy: 0.6555797014275845\n",
      "standard deviation: 0.032931719836344846\n",
      "avg train time: 0.798125171661377\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.6208029487437366\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.606348071475385\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.6835460900525379\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.6139549257834482\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.6348642166987815\n",
      "avgfold accuracy: 0.6319032505507779\n",
      "standard deviation: 0.02747426589399797\n",
      "avg train time: 0.8021359920501709\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.624709663925674\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5795839810496706\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.624939774730026\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5964612219669924\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5947514550686294\n",
      "avgfold accuracy: 0.6040892193481985\n",
      "standard deviation: 0.017921134362308206\n",
      "avg train time: 0.805241060256958\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5996574509668529\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5428841476955742\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5942443736127263\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6108414284264615\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5933088626997439\n",
      "avgfold accuracy: 0.5881872526802718\n",
      "standard deviation: 0.023494969895316745\n",
      "avg train time: 0.7784856796264649\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5839312680318591\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5866015249093197\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5837214953339475\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5763055932250297\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5757480310282617\n",
      "avgfold accuracy: 0.5812615825056835\n",
      "standard deviation: 0.004396690947738242\n",
      "avg train time: 0.7147607803344727\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6117806395249347\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6264963375816304\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5955140086201259\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5668170370575374\n",
      "tree depth: 3, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.6092675841008874\n",
      "avgfold accuracy: 0.6019751213770231\n",
      "standard deviation: 0.02014177307961975\n",
      "avg train time: 0.8271489143371582\n",
      "tree depth: 3, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5684874778964307\n",
      "tree depth: 3, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5631403815703713\n",
      "tree depth: 3, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.6059347458872876\n",
      "tree depth: 3, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5829352669305067\n",
      "tree depth: 3, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5437421652054919\n",
      "avgfold accuracy: 0.5728480074980176\n",
      "standard deviation: 0.02076962847627822\n",
      "avg train time: 0.8862815856933594\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7641664865357114\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7665301722970594\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7736045131417948\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7711897711233499\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7767223675585542\n",
      "avgfold accuracy: 0.770442662131294\n",
      "standard deviation: 0.0045757650884054755\n",
      "avg train time: 0.7831443786621094\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7357538930802694\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7135488716645995\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7411529666928754\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6996757350160057\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7314379979364661\n",
      "avgfold accuracy: 0.7243138928780432\n",
      "standard deviation: 0.015414962789525815\n",
      "avg train time: 0.8350309371948242\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7079540575998751\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6519545235425706\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6796809486218941\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6735325049124069\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6771391340683105\n",
      "avgfold accuracy: 0.6780522337490115\n",
      "standard deviation: 0.01788364593447279\n",
      "avg train time: 0.854133415222168\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6574780490416533\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6386154487794045\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7076858953333198\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6371022795413241\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6465051618052158\n",
      "avgfold accuracy: 0.6574773669001834\n",
      "standard deviation: 0.02612355930040367\n",
      "avg train time: 0.841301965713501\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6272724020362803\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6181044615242954\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6571436194170979\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6183779370658401\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6205078036038569\n",
      "avgfold accuracy: 0.6282812447294741\n",
      "standard deviation: 0.01480623680891016\n",
      "avg train time: 0.85067720413208\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.61951332368632\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.566293834823758\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.624939774730026\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6025064806870912\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5825857148798219\n",
      "avgfold accuracy: 0.5991678257614035\n",
      "standard deviation: 0.02211309538542423\n",
      "avg train time: 0.8471380710601807\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.57374167269605\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5831221823210904\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6605801871024544\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.610798992610633\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6096538530851019\n",
      "avgfold accuracy: 0.6075793775630659\n",
      "standard deviation: 0.030222176795684185\n",
      "avg train time: 0.9735416889190673\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.584570295104541\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5828257223278428\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6209893632102985\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5912683696344062\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6230142447208585\n",
      "avgfold accuracy: 0.6005335989995894\n",
      "standard deviation: 0.01776546179611582\n",
      "avg train time: 0.9703938961029053\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5914145834021334\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6251684062476868\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6400651453018011\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5657464552256017\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5763966975536627\n",
      "avgfold accuracy: 0.5997582575461772\n",
      "standard deviation: 0.02843414474949804\n",
      "avg train time: 1.0637633800506592\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5718295325701785\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5658773103387262\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5842964145235363\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5590858771759887\n",
      "tree depth: 3, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5627032095675095\n",
      "avgfold accuracy: 0.5687584688351879\n",
      "standard deviation: 0.008824799795628094\n",
      "avg train time: 0.8357263088226319\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7641499745314844\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7651359798074997\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7735610934031816\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7558662902794306\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7767223675585542\n",
      "avgfold accuracy: 0.7670871411160302\n",
      "standard deviation: 0.007390414309218689\n",
      "avg train time: 0.9692605972290039\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7492938616374103\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7135145675240897\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7442566933096461\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6996545171080915\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7330260476593149\n",
      "avgfold accuracy: 0.7279491374477105\n",
      "standard deviation: 0.018741670124427436\n",
      "avg train time: 0.8982574939727783\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7029492315913669\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6816638952532097\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7238525549377397\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6840298342235629\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6771923067021283\n",
      "avgfold accuracy: 0.6939375645416015\n",
      "standard deviation: 0.017351233566424616\n",
      "avg train time: 1.0769965648651123\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6287419704124899\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6583325660038746\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7039196253556625\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6347161874187031\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6609536615844058\n",
      "avgfold accuracy: 0.6573328021550273\n",
      "standard deviation: 0.026508071105040557\n",
      "avg train time: 1.0408041954040528\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6242061103422187\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6160756424353051\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6890603968564109\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6280920488196385\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6226807522987015\n",
      "avgfold accuracy: 0.6360229901504549\n",
      "standard deviation: 0.026800809617061893\n",
      "avg train time: 1.0864219188690185\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6255963710617619\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5798632889726631\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6459750686999659\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6060480262733051\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6087817030689621\n",
      "avgfold accuracy: 0.6132528916153317\n",
      "standard deviation: 0.021953904136858327\n",
      "avg train time: 1.0681084156036378\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6230567747752616\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5833280071641489\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6064389118281123\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6255514349763374\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.60070332442832\n",
      "avgfold accuracy: 0.6078156906344361\n",
      "standard deviation: 0.015485316837710658\n",
      "avg train time: 0.9571544647216796\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5957209641409305\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5831638889340259\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6225741836696793\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5832807498224153\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6195863426535619\n",
      "avgfold accuracy: 0.6008652258441226\n",
      "standard deviation: 0.01715093943956512\n",
      "avg train time: 1.0092314243316651\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6044525620125831\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5890983246941064\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6290534026165885\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5600194651242171\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5912174057695774\n",
      "avgfold accuracy: 0.5947682320434144\n",
      "standard deviation: 0.022456590066903714\n",
      "avg train time: 1.0440073966979981\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5716363296116277\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5651177444485971\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6242941022916729\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5800302585817212\n",
      "tree depth: 3, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5851842426810889\n",
      "avgfold accuracy: 0.5852525355229415\n",
      "standard deviation: 0.020699428258515548\n",
      "avg train time: 1.142087984085083\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7641499745314844\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7651531318777545\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7692303668235532\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7639793725034364\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7645105840276141\n",
      "avgfold accuracy: 0.7654046859527686\n",
      "standard deviation: 0.001954704632923174\n",
      "avg train time: 1.1185166358947753\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7297154406253346\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7105423845710809\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7453856065135885\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7050226478104041\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.7314663170673263\n",
      "avgfold accuracy: 0.7244264793175469\n",
      "standard deviation: 0.01473834593273464\n",
      "avg train time: 1.0107399463653564\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7029822555998211\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.687588581415461\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7141594906393844\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6776603105194697\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6768200982654038\n",
      "avgfold accuracy: 0.691842147287908\n",
      "standard deviation: 0.014608525600218783\n",
      "avg train time: 1.0771501541137696\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.6288311602535044\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.6417958037008751\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.6824010906201332\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.6506725154291091\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.6448107668147315\n",
      "avgfold accuracy: 0.6497022673636706\n",
      "standard deviation: 0.01784486750268141\n",
      "avg train time: 0.9937877178192138\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6021112348677489\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6127383912080293\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6745485262660647\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6247723687487892\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.6041556839268138\n",
      "avgfold accuracy: 0.6236652410034892\n",
      "standard deviation: 0.026664315086786565\n",
      "avg train time: 0.9430116176605224\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.6338606917228826\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5807208924854071\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.6374543373441663\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.6204167012610817\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5897780809927142\n",
      "avgfold accuracy: 0.6124461407612504\n",
      "standard deviation: 0.023099155983426784\n",
      "avg train time: 1.0760301113128663\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6148419901268222\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5716965564059371\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6314358645994687\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6101024917204033\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5889452410987823\n",
      "avgfold accuracy: 0.6034044287902827\n",
      "standard deviation: 0.02086097087725044\n",
      "avg train time: 2.4329038143157957\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5806354970063237\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.6012100334194548\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.6281198782364054\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5762244116643143\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.6117027323010383\n",
      "avgfold accuracy: 0.5995785105255073\n",
      "standard deviation: 0.019331425121965437\n",
      "avg train time: 1.0034753799438476\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5808484643335705\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5875667350943996\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.6015381050056681\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5600194651242171\n",
      "tree depth: 3, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5864709808123035\n",
      "avgfold accuracy: 0.5832887500740318\n",
      "standard deviation: 0.013482442807575358\n",
      "avg train time: 0.9134491920471192\n",
      "tree depth: 3, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5585818389969508\n",
      "tree depth: 3, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5499016915552233\n",
      "tree depth: 3, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.6230244672842733\n",
      "tree depth: 3, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5470936078746114\n",
      "tree depth: 3, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5990407141965981\n",
      "avgfold accuracy: 0.5755284639815313\n",
      "standard deviation: 0.030203168195038807\n",
      "avg train time: 0.7613626003265381\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7710768228502121\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7624456324509947\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7735828032724881\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7599595014714158\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.764305022784028\n",
      "avgfold accuracy: 0.7662739565658278\n",
      "standard deviation: 0.005194071112948005\n",
      "avg train time: 0.7940571308135986\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7304370277191519\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7196205239867188\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7312267174729921\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.721713761197059\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.7335506447058078\n",
      "avgfold accuracy: 0.7273097350163459\n",
      "standard deviation: 0.005558966706340297\n",
      "avg train time: 0.8223018169403076\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7067536223834728\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6770451135557325\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7147890768492755\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6804052620411627\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6828675199372622\n",
      "avgfold accuracy: 0.6923721189533811\n",
      "standard deviation: 0.015348014878089672\n",
      "avg train time: 0.7959015846252442\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.6447274043229427\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.6417443474901104\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.6608608460152974\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.6521153331672801\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.6496600714117384\n",
      "avgfold accuracy: 0.6498216004814739\n",
      "standard deviation: 0.00660825383317606\n",
      "avg train time: 0.8471418380737304\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.6127153190369399\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.6129785201915976\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.674526816396758\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.623923652432218\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5941948752274937\n",
      "avgfold accuracy: 0.6236678366570014\n",
      "standard deviation: 0.027164594037166784\n",
      "avg train time: 0.8946047306060791\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6135986487176177\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5881574885035993\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6382632261493545\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6075835570438841\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.6100295271497289\n",
      "avgfold accuracy: 0.611526489512837\n",
      "standard deviation: 0.016019018388662227\n",
      "avg train time: 0.8165642261505127\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6076294966440603\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5992741966060566\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6269587925755387\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6299167889002666\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5836173827974153\n",
      "avgfold accuracy: 0.6094793315046675\n",
      "standard deviation: 0.017318361829988062\n",
      "avg train time: 1.0271373748779298\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5876976937233369\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.603647974521051\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.640402040623089\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5867641767912988\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5988989205064174\n",
      "avgfold accuracy: 0.6034821612330387\n",
      "standard deviation: 0.019558893128904787\n",
      "avg train time: 1.4352807521820068\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.608933944977999\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5767490146587009\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.6237835280159324\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5485133626693973\n",
      "tree depth: 3, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5949926627706408\n",
      "avgfold accuracy: 0.5905945026185341\n",
      "standard deviation: 0.02615228571988914\n",
      "avg train time: 1.1031532764434815\n",
      "tree depth: 3, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5777260068069486\n",
      "tree depth: 3, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.584761378593133\n",
      "tree depth: 3, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.6402066517993297\n",
      "tree depth: 3, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5833923744683992\n",
      "tree depth: 3, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5607217615687611\n",
      "avgfold accuracy: 0.5893616346473143\n",
      "standard deviation: 0.026824020419137326\n",
      "avg train time: 0.8651499271392822\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.771043798841758\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7624456324509947\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7698326195232932\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7621758503307225\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7661412114646932\n",
      "avgfold accuracy: 0.7663278225222923\n",
      "standard deviation: 0.003657024232352153\n",
      "avg train time: 0.8447649478912354\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7304370277191519\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7095598417676382\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7312050076036856\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7154853827064825\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.7392541770718019\n",
      "avgfold accuracy: 0.7251882873737521\n",
      "standard deviation: 0.010954397487601061\n",
      "avg train time: 0.9292425632476806\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6951950943335805\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6829820769893242\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7124717444743152\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6668313360824362\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6828852441485349\n",
      "avgfold accuracy: 0.6880730992056383\n",
      "standard deviation: 0.015166156753982911\n",
      "avg train time: 0.8522959232330323\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.640496953785402\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.6206158857058257\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.6847618427337068\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.6451074271902878\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.6344777496786869\n",
      "avgfold accuracy: 0.6450919718187818\n",
      "standard deviation: 0.021478145129645288\n",
      "avg train time: 0.7812039375305175\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.6426501441548005\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5904336585004757\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.6744833966581449\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.6376982259983948\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.6157613786465832\n",
      "avgfold accuracy: 0.6322053607916798\n",
      "standard deviation: 0.028078379744924806\n",
      "avg train time: 0.7912822246551514\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.6136151607218447\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5903970072345627\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.6382198064107413\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5941812193839426\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5974703886850219\n",
      "avgfold accuracy: 0.6067767164872226\n",
      "standard deviation: 0.017601030555799507\n",
      "avg train time: 0.7989653587341309\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5945271587444072\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5794933458994816\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6235559667352489\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6152316903292466\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5830608029562796\n",
      "avgfold accuracy: 0.5991737929329328\n",
      "standard deviation: 0.017440677273678745\n",
      "avg train time: 0.8436081409454346\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6314317383737977\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.599551157403646\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.6119419713502963\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5661611269476655\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.606311106446266\n",
      "avgfold accuracy: 0.6030794201043344\n",
      "standard deviation: 0.021299068382160218\n",
      "avg train time: 0.843513011932373\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.597855953778397\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5702608378515497\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.6147200499274681\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5661265325325879\n",
      "tree depth: 3, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.6032519471877915\n",
      "avgfold accuracy: 0.5904430642555589\n",
      "standard deviation: 0.01901055719341475\n",
      "avg train time: 0.7526297569274902\n",
      "tree depth: 3, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5929765939838263\n",
      "tree depth: 3, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5699495729134506\n",
      "tree depth: 3, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5980540633289966\n",
      "tree depth: 3, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5743862950765228\n",
      "tree depth: 3, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5761202394649863\n",
      "avgfold accuracy: 0.5822973529535564\n",
      "standard deviation: 0.011095308472876176\n",
      "avg train time: 0.8970265865325928\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7644471906075717\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7657828836782705\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7722150815061732\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7621758503307225\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7661412114646932\n",
      "avgfold accuracy: 0.7661524435174861\n",
      "standard deviation: 0.003335070040506772\n",
      "avg train time: 0.7257021903991699\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.732969994185773\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7112970756622956\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7315033221331333\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7246902646703384\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7140401497943397\n",
      "avgfold accuracy: 0.7229001612891761\n",
      "standard deviation: 0.008851573656034116\n",
      "avg train time: 0.7077908039093017\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7177969008469157\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6713874580451333\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7098504739290625\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6668101181745218\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.679932430157696\n",
      "avgfold accuracy: 0.689155476230666\n",
      "standard deviation: 0.02073015296396911\n",
      "avg train time: 0.7270539283752442\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.641190457962939\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.6417614995603653\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.6631838020311022\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.6410990876299597\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.6372390629734295\n",
      "avgfold accuracy: 0.6448947820315591\n",
      "standard deviation: 0.009284880814320412\n",
      "avg train time: 0.7484342575073242\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.6603131726765359\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5884734476925049\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.6597761372440394\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.6147630513196616\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.6183348549090124\n",
      "avgfold accuracy: 0.6283321327683509\n",
      "standard deviation: 0.027872181984945107\n",
      "avg train time: 0.7779476642608643\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.6273796675182858\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5885323063756953\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.6291619519631215\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.6033939427485493\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5958220370366702\n",
      "avgfold accuracy: 0.6088579811284645\n",
      "standard deviation: 0.016542241119717874\n",
      "avg train time: 1.0573949813842773\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6150963000101074\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5830681984368145\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6296234828594043\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.613918025074032\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5773289514594254\n",
      "avgfold accuracy: 0.6038069915679567\n",
      "standard deviation: 0.020136319697450604\n",
      "avg train time: 1.1535723686218262\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6167524415249886\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5842370668876575\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6263509162349541\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5752659157372301\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.6078282593240243\n",
      "avgfold accuracy: 0.6020869199417709\n",
      "standard deviation: 0.0193637059362049\n",
      "avg train time: 1.0099690914154054\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5652725781643254\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5722284511319463\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6246575464290404\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5824357235767857\n",
      "tree depth: 3, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.6237372737192525\n",
      "avgfold accuracy: 0.5936663146042701\n",
      "standard deviation: 0.02552109506356357\n",
      "avg train time: 1.0672113418579101\n",
      "tree depth: 3, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5829124648619546\n",
      "tree depth: 3, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5654804655974608\n",
      "tree depth: 3, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.6094131639231146\n",
      "tree depth: 3, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5620042620319375\n",
      "tree depth: 3, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5902285136020944\n",
      "avgfold accuracy: 0.5820077740033124\n",
      "standard deviation: 0.017278581169113166\n",
      "avg train time: 1.2493671894073486\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7644471906075717\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7658000357485254\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7722150815061732\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7620909786990655\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7695654498682072\n",
      "avgfold accuracy: 0.7668237472859085\n",
      "standard deviation: 0.003624321265509094\n",
      "avg train time: 1.3330710411071778\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7331284844081647\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7107677086097977\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7315901616103597\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7258669360418454\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7272196356535877\n",
      "avgfold accuracy: 0.7257145852647511\n",
      "standard deviation: 0.007939594537158623\n",
      "avg train time: 1.0786078453063965\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.717395671653292\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6442631741440665\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7190329639516775\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6756736685762783\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6799147059464234\n",
      "avgfold accuracy: 0.6872560368543474\n",
      "standard deviation: 0.02812331175383263\n",
      "avg train time: 1.197470474243164\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6627140431093405\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.627871030875532\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6886149521885903\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6480239670107658\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6496883905425985\n",
      "avgfold accuracy: 0.6553824767453654\n",
      "standard deviation: 0.02000799453459254\n",
      "avg train time: 0.9083311557769775\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6703442777899533\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6064509838969143\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6519605842936681\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6147206155038331\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6005258842797139\n",
      "avgfold accuracy: 0.6288004691528165\n",
      "standard deviation: 0.027420696158717093\n",
      "avg train time: 0.9051618576049805\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6377856076367519\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5890248416141723\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6335143884120564\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.608326183820884\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.595485277022491\n",
      "avgfold accuracy: 0.6128272597012712\n",
      "standard deviation: 0.019689771862925545\n",
      "avg train time: 2.5349803447723387\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.608471608859641\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5890174391417465\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6296669025980175\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6272239596306239\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6002459605631348\n",
      "avgfold accuracy: 0.6109251741586328\n",
      "standard deviation: 0.015600751469223656\n",
      "avg train time: 1.4366605758666993\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6127829306906121\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.590791504850425\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6236267199840131\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5813014880211071\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6076332930000257\n",
      "avgfold accuracy: 0.6032271873092366\n",
      "standard deviation: 0.015246307223509921\n",
      "avg train time: 1.0359302043914795\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5946361129541176\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5666908601131315\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6269475452938497\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.560970580909418\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6296783501234754\n",
      "avgfold accuracy: 0.5957846898787984\n",
      "standard deviation: 0.028912613528072468\n",
      "avg train time: 1.0299275875091554\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5971227957725267\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5615943481220289\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.6308287729529555\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5511153239421028\n",
      "tree depth: 3, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5535222661641837\n",
      "avgfold accuracy: 0.5788367013907595\n",
      "standard deviation: 0.030803087175809193\n",
      "avg train time: 1.3321720123291017\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7643596894942624\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7723716257815476\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7725463531866165\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7621334145148941\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.776864161248735\n",
      "avgfold accuracy: 0.7696550488452111\n",
      "standard deviation: 0.00551953886383619\n",
      "avg train time: 1.0766327381134033\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7218639200698909\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7099494645845859\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7414022378428655\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.7258245002260169\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.743695824809539\n",
      "avgfold accuracy: 0.7285471895065797\n",
      "standard deviation: 0.012591170865541707\n",
      "avg train time: 1.1208383083343505\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7173791596490648\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6408573146357712\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7312821691873657\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6517218793531305\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6724033040306243\n",
      "avgfold accuracy: 0.6827287653711913\n",
      "standard deviation: 0.03571930392622413\n",
      "avg train time: 0.9868263721466064\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.6514643020475885\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.631904114510832\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.6856029039958191\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.6265606693788688\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.6496706663313259\n",
      "avgfold accuracy: 0.6490405312528867\n",
      "standard deviation: 0.020698211537752514\n",
      "avg train time: 0.9874208450317383\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.6517301828429268\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.600736997376636\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.664893258065478\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.6121108128303766\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.668369511188037\n",
      "avgfold accuracy: 0.6395681524606909\n",
      "standard deviation: 0.02785878075648566\n",
      "avg train time: 0.9859665393829345\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6368939594084899\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.592023926235265\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6365047267355209\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6082837480050554\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6096112753708717\n",
      "avgfold accuracy: 0.6166635271510406\n",
      "standard deviation: 0.017492904145792965\n",
      "avg train time: 1.0439263343811036\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5972565805340482\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5768739539493996\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6295800631207911\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6372775579110508\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5943297376618696\n",
      "avgfold accuracy: 0.6070635786354318\n",
      "standard deviation: 0.02275793476408156\n",
      "avg train time: 1.2230700969696044\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6427739216410331\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6491513336185993\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6366462332330494\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5754914713235362\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.6111178333290426\n",
      "avgfold accuracy: 0.623036158629052\n",
      "standard deviation: 0.027053131253569535\n",
      "avg train time: 1.235727071762085\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5975207726016813\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5764722344092196\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.6080398185159552\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5852982961097427\n",
      "tree depth: 3, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5942553751888767\n",
      "avgfold accuracy: 0.5923172993650951\n",
      "standard deviation: 0.010755113855523073\n",
      "avg train time: 1.4640275478363036\n",
      "tree depth: 3, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5949794875874761\n",
      "tree depth: 3, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5580046906398445\n",
      "tree depth: 3, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.6275940024262741\n",
      "tree depth: 3, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5839712543473649\n",
      "tree depth: 3, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5823626274608433\n",
      "avgfold accuracy: 0.5893824124923606\n",
      "standard deviation: 0.022603416989077797\n",
      "avg train time: 1.0613581180572509\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7643431774900353\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7723716257815476\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.765588732420891\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7614332235537229\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7652301473981056\n",
      "avgfold accuracy: 0.7657933813288604\n",
      "standard deviation: 0.0035985130460742167\n",
      "avg train time: 1.1937436580657959\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.724170659569507\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7075432999499882\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7753163232583846\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7209886622570318\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7436781005982664\n",
      "avgfold accuracy: 0.7343394091266356\n",
      "standard deviation: 0.023520187187612635\n",
      "avg train time: 0.986308240890503\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7042255970090255\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6635834466272711\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7312821691873657\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6539479146486591\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6723678556080791\n",
      "avgfold accuracy: 0.68508139661608\n",
      "standard deviation: 0.028615036162717284\n",
      "avg train time: 1.0311625957489015\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6514147660349074\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.631904114510832\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6720623538834248\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6400843181210158\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6549133692042324\n",
      "avgfold accuracy: 0.6500757843508824\n",
      "standard deviation: 0.01370141103077491\n",
      "avg train time: 0.8045296669006348\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.649978221667152\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6023321399103398\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.677006580444482\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6112524100775838\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6470370861792739\n",
      "avgfold accuracy: 0.6375212876557664\n",
      "standard deviation: 0.027324509222972717\n",
      "avg train time: 1.0608548641204834\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6330185795073018\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.611339323919555\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.641638718479493\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6140204245426618\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6107739440231781\n",
      "avgfold accuracy: 0.6221581980944381\n",
      "standard deviation: 0.012730366816838846\n",
      "avg train time: 1.3188217639923097\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6026907186524604\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5881795153727687\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6359949371538518\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.60304661482117\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5703244223788466\n",
      "avgfold accuracy: 0.6000472416758195\n",
      "standard deviation: 0.02159865706259096\n",
      "avg train time: 0.7552965164184571\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5966291243734194\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5771949684853277\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6395819045362119\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5727967970184227\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5998631572068227\n",
      "avgfold accuracy: 0.5972131903240409\n",
      "standard deviation: 0.02366161815745368\n",
      "avg train time: 0.7505517482757569\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5961882413514625\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5707067916781766\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6261555274111948\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6120102583972177\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5887822575694264\n",
      "avgfold accuracy: 0.5987686152814956\n",
      "standard deviation: 0.019074777494482113\n",
      "avg train time: 0.7731672763824463\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5748677413479599\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5560787839723833\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6388550162614768\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6020433767839186\n",
      "tree depth: 3, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5892005093482837\n",
      "avgfold accuracy: 0.5922090855428044\n",
      "standard deviation: 0.027884506728999583\n",
      "avg train time: 0.8781067848205566\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7684893792787356\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7723030175005281\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.773539383533875\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7656016199411433\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7683602035016704\n",
      "avgfold accuracy: 0.7696587207511905\n",
      "standard deviation: 0.0028832420099901292\n",
      "avg train time: 0.8268125534057618\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.72415414756528\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7101135828147089\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.762839556742013\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.7196634655301248\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.710828601926097\n",
      "avgfold accuracy: 0.7255198709156447\n",
      "standard deviation: 0.019401591337106307\n",
      "avg train time: 0.7717172622680664\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6973598556150321\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6652006160301444\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7312170395794458\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.670903329366507\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6723501313968065\n",
      "avgfold accuracy: 0.6874061943975872\n",
      "standard deviation: 0.02453806359568996\n",
      "avg train time: 0.750557279586792\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.6785210973377646\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.6397572350140557\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.6678345530094849\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.6287594904012029\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.6436126497398799\n",
      "avgfold accuracy: 0.6516970051004776\n",
      "standard deviation: 0.01851484198562044\n",
      "avg train time: 0.7265998363494873\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6141848874131494\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6105772303559145\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6743418901606164\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6162233968948053\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.6521025469394545\n",
      "avgfold accuracy: 0.633485990352788\n",
      "standard deviation: 0.02534214560935334\n",
      "avg train time: 0.8179220676422119\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.6254972990363995\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5859546210385488\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.6458174759739754\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.6104133801972342\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.6186538907119192\n",
      "avgfold accuracy: 0.6172673333916154\n",
      "standard deviation: 0.019556217152028973\n",
      "avg train time: 0.9807891368865966\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6224408895266859\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5881623633025138\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6387898866535571\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6130674637219901\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5723095340413776\n",
      "avgfold accuracy: 0.6069540274492249\n",
      "standard deviation: 0.023849974826966505\n",
      "avg train time: 1.0103994846343993\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.604645764971134\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5662620583567595\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.6292053717017346\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5886604119964206\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5833337954170537\n",
      "avgfold accuracy: 0.5944214804886204\n",
      "standard deviation: 0.021276911459718648\n",
      "avg train time: 0.738342571258545\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.598782314742818\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5703980544135887\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.6097548981911756\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5859039290030351\n",
      "tree depth: 3, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5799663933111401\n",
      "avgfold accuracy: 0.5889611179323515\n",
      "standard deviation: 0.013882910405277142\n",
      "avg train time: 0.8017114639282227\n",
      "tree depth: 3, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5736673061315576\n",
      "tree depth: 3, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5463414634146342\n",
      "tree depth: 3, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.613743890502743\n",
      "tree depth: 3, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5882803346894344\n",
      "tree depth: 3, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5822668771127953\n",
      "avgfold accuracy: 0.580859974370233\n",
      "standard deviation: 0.02182500922239075\n",
      "avg train time: 0.7536021709442139\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7684893792787356\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7723030175005281\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7738433217041671\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7573939796492588\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7683424792903979\n",
      "avgfold accuracy: 0.7680744354846176\n",
      "standard deviation: 0.005752862187894548\n",
      "avg train time: 0.8468033313751221\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7308333158206015\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7012999463772119\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.7600389836014633\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.6943943209808209\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.710828601926097\n",
      "avgfold accuracy: 0.7194790337412389\n",
      "standard deviation: 0.023700553818219828\n",
      "avg train time: 1.0656625747680664\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7084923239194895\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6746217968508799\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7194132790115784\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6677861419385788\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.695479831035787\n",
      "avgfold accuracy: 0.6931586745512626\n",
      "standard deviation: 0.019581593764561953\n",
      "avg train time: 0.8798346042633056\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.6550771786088486\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.6397229308735459\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.6739993711984842\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.6265279199992619\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.6435772013173346\n",
      "avgfold accuracy: 0.6477809203994951\n",
      "standard deviation: 0.01596831250627885\n",
      "avg train time: 0.7871955871582031\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.6281194553440351\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.614897024386633\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.667026448898368\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.6161809610789767\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.6360517388540456\n",
      "avgfold accuracy: 0.6324551257124117\n",
      "standard deviation: 0.01897390527616334\n",
      "avg train time: 0.8951889514923096\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6111169069913828\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5970787316134321\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6409874224002954\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6110517624701335\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.6232656512706972\n",
      "avgfold accuracy: 0.6167000949491881\n",
      "standard deviation: 0.01470411065823496\n",
      "avg train time: 0.7739211559295655\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.586365162473118\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5793904334779524\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6272410208765242\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6156057712709527\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5872897601587456\n",
      "avgfold accuracy: 0.5991784296514585\n",
      "standard deviation: 0.018732066888637235\n",
      "avg train time: 0.7405415534973144\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6130256696618441\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5758767867492133\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.6413782000478141\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5790002675301432\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5839647377311821\n",
      "avgfold accuracy: 0.5986491323440394\n",
      "standard deviation: 0.02511074315744188\n",
      "avg train time: 0.7045088768005371\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6019922733827492\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5558215029185601\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6373843687894734\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.6037062150019834\n",
      "tree depth: 3, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5941667541325137\n",
      "avgfold accuracy: 0.5986142228450559\n",
      "standard deviation: 0.02604404224681491\n",
      "avg train time: 0.7314582347869873\n",
      "tree depth: 3, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5547163412801106\n",
      "tree depth: 3, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5509797443077695\n",
      "tree depth: 3, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.6311214638415588\n",
      "tree depth: 3, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5628626647847305\n",
      "tree depth: 3, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5723946894698382\n",
      "avgfold accuracy: 0.5744149807368015\n",
      "standard deviation: 0.029293035041454106\n",
      "avg train time: 0.6909491539001464\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7647840605119922\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7593607874786277\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7734581676974931\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7481254439616601\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7682538582340348\n",
      "avgfold accuracy: 0.7627964635767617\n",
      "standard deviation: 0.008653881436421183\n",
      "avg train time: 0.8663078308105469\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7220736350326687\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7050513749641161\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7402194423126922\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6943943209808209\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.7108108777148243\n",
      "avgfold accuracy: 0.7145099302010245\n",
      "standard deviation: 0.015658694930073068\n",
      "avg train time: 0.7160590648651123\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6961032545660695\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6759645331296751\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7142302438881487\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6672132584248932\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6692555237157868\n",
      "avgfold accuracy: 0.6845533627449146\n",
      "standard deviation: 0.018014374074166277\n",
      "avg train time: 0.7572828769683838\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6550441546003944\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5918376005878646\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6651047331176994\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6264854841834334\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6355198144799875\n",
      "avgfold accuracy: 0.6347983573938759\n",
      "standard deviation: 0.025470059680834153\n",
      "avg train time: 0.8730855464935303\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6280368953228997\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.614495124298345\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.677853265347439\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6053035544608345\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.6241129477838795\n",
      "avgfold accuracy: 0.6299603574426796\n",
      "standard deviation: 0.025214098846922803\n",
      "avg train time: 0.987572431564331\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6315044162105852\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5955767519034284\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6283803966680843\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5908827572210076\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6212876688998512\n",
      "avgfold accuracy: 0.6135263981805913\n",
      "standard deviation: 0.016964739926757948\n",
      "avg train time: 0.9421038150787353\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6156229954176686\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5706822371354959\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6237079358203951\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6581661269937915\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5834933133185071\n",
      "avgfold accuracy: 0.6103345217371716\n",
      "standard deviation: 0.030943279603307252\n",
      "avg train time: 1.0462692737579347\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6130091576576171\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5719809196759522\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6439238783975292\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5790002675301432\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.6113270582364113\n",
      "avgfold accuracy: 0.6038482562995307\n",
      "standard deviation: 0.02599561412242202\n",
      "avg train time: 1.300205612182617\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.6059452722128987\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5590998954626455\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.613716556992592\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5914593308056347\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5992499391039668\n",
      "avgfold accuracy: 0.5938941989155475\n",
      "standard deviation: 0.01888602835857159\n",
      "avg train time: 1.1256325244903564\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5938748720319673\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5533467300029429\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5966268355956064\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5580789490677958\n",
      "tree depth: 3, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5803526622953546\n",
      "avgfold accuracy: 0.5764560097987335\n",
      "standard deviation: 0.017872985476949873\n",
      "avg train time: 1.054923677444458\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7647840605119922\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7608921965302264\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7734581676974931\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7517942969953597\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7682538582340348\n",
      "avgfold accuracy: 0.7638365159938212\n",
      "standard deviation: 0.00730302663419042\n",
      "avg train time: 0.9755172729492188\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7220571230284416\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7289241679891816\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7416032503074693\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7184886391940886\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7284214163922219\n",
      "avgfold accuracy: 0.7278989193822806\n",
      "standard deviation: 0.00789699666706831\n",
      "avg train time: 1.0693673610687255\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6974308447241144\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6786500056872654\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7142085340188421\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6671920405169789\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6588799288655118\n",
      "avgfold accuracy: 0.6832722707625426\n",
      "standard deviation: 0.020161562852549296\n",
      "avg train time: 1.6251037120819092\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.6992075113607592\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5918376005878646\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.6918497227152717\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.6466757073404736\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.6352610806025836\n",
      "avgfold accuracy: 0.6529663245213905\n",
      "standard deviation: 0.03934446449476684\n",
      "avg train time: 1.5308393955230712\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.6189238326263193\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.6085067046539886\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.6558844469973159\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.6062874196256423\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.646881033905723\n",
      "avgfold accuracy: 0.6272966875617978\n",
      "standard deviation: 0.020324193011883013\n",
      "avg train time: 1.0937986850738526\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.607112683420847\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5944569925379467\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.6283586867987777\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5967370547698779\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.6245772429048695\n",
      "avgfold accuracy: 0.6102485320864638\n",
      "standard deviation: 0.013964640667515125\n",
      "avg train time: 1.0606510162353515\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6128043837870132\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5750412101056387\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6407220650218433\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.612654637035397\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5832451743606907\n",
      "avgfold accuracy: 0.6048934940621165\n",
      "standard deviation: 0.023521533184810366\n",
      "avg train time: 1.1417354106903077\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6070515764961126\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5743919591094645\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.6013105437249846\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5591725938431166\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5962190989763525\n",
      "avgfold accuracy: 0.5876291544300061\n",
      "standard deviation: 0.01801775357614552\n",
      "avg train time: 1.14926438331604\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5806520090105507\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5504285309342101\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.6238599049055411\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5971323536194983\n",
      "tree depth: 3, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5978674506247041\n",
      "avgfold accuracy: 0.5899880498189009\n",
      "standard deviation: 0.024135226509098558\n",
      "avg train time: 1.0557378292083741\n",
      "tree depth: 3, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5957242165053994\n",
      "tree depth: 3, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5666394039023668\n",
      "tree depth: 3, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.6022167345950168\n",
      "tree depth: 3, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5650481092999013\n",
      "tree depth: 3, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5997250271804245\n",
      "avgfold accuracy: 0.5858706982966216\n",
      "standard deviation: 0.01649030047745994\n",
      "avg train time: 1.3707947254180908\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.765406575580447\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7609093486004813\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7785053199642389\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7517730790874455\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7682538582340348\n",
      "avgfold accuracy: 0.7649696362933295\n",
      "standard deviation: 0.008772990600385059\n",
      "avg train time: 1.0782263278961182\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7178729561391131\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.702924518252511\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7444633294150942\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7018823974390908\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7104635227810574\n",
      "avgfold accuracy: 0.7155213448053732\n",
      "standard deviation: 0.01557851749754816\n",
      "avg train time: 1.036224126815796\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6966333274290408\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6509817303369569\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.714165114280229\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6798226920912555\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6663630116504509\n",
      "avgfold accuracy: 0.6815931751575867\n",
      "standard deviation: 0.022177873853489023\n",
      "avg train time: 1.0723987102508545\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6597732176292165\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5918376005878646\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.7066977039328344\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6580623437485585\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6524922815515715\n",
      "avgfold accuracy: 0.6537726294900091\n",
      "standard deviation: 0.03658704499455492\n",
      "avg train time: 1.062274980545044\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6447752516079189\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5968288530320346\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6574531812282345\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6205287871659333\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6257824892714062\n",
      "avgfold accuracy: 0.6290737124611054\n",
      "standard deviation: 0.020859105623532354\n",
      "avg train time: 1.011479663848877\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6288541769866693\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6017783988632691\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6355599550736488\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5868531997527653\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.6038083289930469\n",
      "avgfold accuracy: 0.6113708119338799\n",
      "standard deviation: 0.018116851510218232\n",
      "avg train time: 1.1882352352142334\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6242505801717849\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5703122940623143\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6162405254730006\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6255071541250381\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5966550749664825\n",
      "avgfold accuracy: 0.6065931257597241\n",
      "standard deviation: 0.020869413450168933\n",
      "avg train time: 1.1298302173614503\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6283588168598571\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6009821817072267\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6069221525937016\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.564909270380723\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.6142584843521949\n",
      "avgfold accuracy: 0.6030861811787406\n",
      "standard deviation: 0.02116669308617828\n",
      "avg train time: 1.0625961303710938\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5897749538914792\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5743479053711257\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.605478053937778\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5969298609765773\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.6187143906733021\n",
      "avgfold accuracy: 0.5970490329700524\n",
      "standard deviation: 0.014890961967262147\n",
      "avg train time: 1.272458028793335\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5483525898328084\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5372290198584864\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.6185578578479608\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5735855496821927\n",
      "tree depth: 3, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5877188048930706\n",
      "avgfold accuracy: 0.5730887644229037\n",
      "standard deviation: 0.028912014631381436\n",
      "avg train time: 1.0462524890899658\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7641400923471364\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7608921965302264\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7732193591351206\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7517518611795312\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7748507304553439\n",
      "avgfold accuracy: 0.7649708479294717\n",
      "standard deviation: 0.008458335280703886\n",
      "avg train time: 1.058113193511963\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7245289825703286\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7012387405686182\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7444416195457877\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.699814573935184\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7186767638560755\n",
      "avgfold accuracy: 0.7177401360951988\n",
      "standard deviation: 0.01645330823612448\n",
      "avg train time: 1.066786289215088\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7077443426370972\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6713751807737931\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.7141434044109223\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6691362466443418\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6612621024677251\n",
      "avgfold accuracy: 0.6847322553867758\n",
      "standard deviation: 0.021758064507172714\n",
      "avg train time: 1.043841791152954\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.6744739675744266\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.6418644119818947\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.7150608425626435\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.6747110213193849\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.6424711709267485\n",
      "avgfold accuracy: 0.6697162828730197\n",
      "standard deviation: 0.026913695050706787\n",
      "avg train time: 1.0028385162353515\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.6420606530947999\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.6038561464895129\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.658061057568819\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.6048136975433352\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.6210821076562651\n",
      "avgfold accuracy: 0.6259747324705465\n",
      "standard deviation: 0.021209658758245813\n",
      "avg train time: 0.9024182319641113\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5907706152372775\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6054047076113667\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6199786772464353\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6175195343130472\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.6214400575096196\n",
      "avgfold accuracy: 0.6110227183835492\n",
      "standard deviation: 0.011598693872386082\n",
      "avg train time: 0.8906065464019776\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6261280075614972\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5799221476558536\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6182104999390554\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6202681759056818\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5994306468445953\n",
      "avgfold accuracy: 0.6087918955813366\n",
      "standard deviation: 0.016982645376447877\n",
      "avg train time: 0.8620622634887696\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.6185968449062469\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5686265163784215\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5990744271864061\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.566118691131837\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.610206769262455\n",
      "avgfold accuracy: 0.5925246497730733\n",
      "standard deviation: 0.021464909617207848\n",
      "avg train time: 0.8346030235290527\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5958200361662929\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5511806943519135\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.6345130424001595\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5906106144890635\n",
      "tree depth: 3, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5882328070199758\n",
      "avgfold accuracy: 0.592071438885481\n",
      "standard deviation: 0.02647362378480286\n",
      "avg train time: 0.7936525821685791\n",
      "tree depth: 3, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5920519217471101\n",
      "tree depth: 3, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5701676750278496\n",
      "tree depth: 3, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.6155618958836518\n",
      "tree depth: 3, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5675693502707589\n",
      "tree depth: 3, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5695553500383199\n",
      "avgfold accuracy: 0.582981238593538\n",
      "standard deviation: 0.018578184619339057\n",
      "avg train time: 0.9056202888488769\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7420219499576192\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7274465622737507\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7376247205835194\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7248909122777885\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.7310515309163714\n",
      "avgfold accuracy: 0.7326071352018098\n",
      "standard deviation: 0.006362490295487047\n",
      "avg train time: 1.022945022583008\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.682021642233904\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.6303923852029992\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.6715469406609113\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.647873596619895\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.6632400848385711\n",
      "avgfold accuracy: 0.6590149299112561\n",
      "standard deviation: 0.018152638444984923\n",
      "avg train time: 0.8977191925048829\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6255352641370276\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5880497012831554\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6409769598126779\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6109802673456397\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6253430476533739\n",
      "avgfold accuracy: 0.6181770480463749\n",
      "standard deviation: 0.017803266814835985\n",
      "avg train time: 0.908944034576416\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5945386045655192\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5650516638410887\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.6358107956117816\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5536771556933182\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5723521117556079\n",
      "avgfold accuracy: 0.5842860662934631\n",
      "standard deviation: 0.029014476349178264\n",
      "avg train time: 1.026475429534912\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.558636316101806\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5532463452549249\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.6078781715372628\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.546036402549839\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.584283971569969\n",
      "avgfold accuracy: 0.5700162414027603\n",
      "standard deviation: 0.0229035310190843\n",
      "avg train time: 1.0743930339813232\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5504363547298886\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5241398236767179\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5865550256254927\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5269186062602054\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5718028592420374\n",
      "avgfold accuracy: 0.5519705339068683\n",
      "standard deviation: 0.024470020074004444\n",
      "avg train time: 1.079817819595337\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5393913875387406\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5181048226205114\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5761897400727045\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5626444893403076\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5443166672937803\n",
      "avgfold accuracy: 0.5481294213732089\n",
      "standard deviation: 0.01995212040275297\n",
      "avg train time: 1.0202672004699707\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5341009163661982\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5217410615145459\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5595181298333886\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5407471471139033\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5401408827251322\n",
      "avgfold accuracy: 0.5392496275106337\n",
      "standard deviation: 0.012222178176702077\n",
      "avg train time: 1.000200128555298\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5292215565716276\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5244436861423911\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.551778953772626\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5260292991632763\n",
      "tree depth: 4, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5473581023409821\n",
      "avgfold accuracy: 0.5357663195981806\n",
      "standard deviation: 0.011459663160482242\n",
      "avg train time: 1.0350127220153809\n",
      "tree depth: 4, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5864659232261855\n",
      "tree depth: 4, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5174604464232517\n",
      "tree depth: 4, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5601638022717417\n",
      "tree depth: 4, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5241851862102049\n",
      "tree depth: 4, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5228459142227389\n",
      "avgfold accuracy: 0.5422242544708246\n",
      "standard deviation: 0.026807934918823485\n",
      "avg train time: 1.0934189319610597\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.751689790978041\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7155506085373978\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7342436046125365\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7334920063838227\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.73261126150836\n",
      "avgfold accuracy: 0.7335174544040315\n",
      "standard deviation: 0.011440196626331778\n",
      "avg train time: 1.7616546630859375\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6810474339845067\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6200718942565842\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6786726167402448\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.660259319735422\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6373525375327502\n",
      "avgfold accuracy: 0.6554807604499016\n",
      "standard deviation: 0.023650673960343857\n",
      "avg train time: 0.9639947414398193\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6229742147541263\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.582798820659759\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6480592161533982\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6261782857775441\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6180903796149786\n",
      "avgfold accuracy: 0.6196201833919612\n",
      "standard deviation: 0.021076953625876513\n",
      "avg train time: 0.9967706203460693\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5867383087504615\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.553748630091231\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.6166803464999767\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5886950064114982\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5990443778603808\n",
      "avgfold accuracy: 0.5889813339227097\n",
      "standard deviation: 0.02056675334461327\n",
      "avg train time: 0.921117639541626\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5817384238341274\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5476352711561759\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.6020116678777112\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5513275030212456\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5696439710946829\n",
      "avgfold accuracy: 0.5704713673967886\n",
      "standard deviation: 0.020051915292906626\n",
      "avg train time: 1.2079805850982666\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5343535375217783\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5230372163814909\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5768571223803642\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5314246441387835\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5635753595836493\n",
      "avgfold accuracy: 0.5458495760012132\n",
      "standard deviation: 0.020670329689883943\n",
      "avg train time: 0.939998197555542\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5428985622547592\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.535910838122372\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5678917310508153\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5480654803088589\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5551991349792755\n",
      "avgfold accuracy: 0.5499931493432163\n",
      "standard deviation: 0.010952826820222188\n",
      "avg train time: 0.9118157863616944\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5568876072905002\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5236252615690714\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5420102972787333\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5533565807802655\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5436608714766942\n",
      "avgfold accuracy: 0.5439081236790528\n",
      "standard deviation: 0.011599980262768804\n",
      "avg train time: 1.000025749206543\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.524145741454037\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5230323415825764\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5592085680222518\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5402803531397891\n",
      "tree depth: 4, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5203716539362602\n",
      "avgfold accuracy: 0.5334077316269829\n",
      "standard deviation: 0.014671663758753431\n",
      "avg train time: 0.9593636512756347\n",
      "tree depth: 4, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5524262388756626\n",
      "tree depth: 4, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5218096697955654\n",
      "tree depth: 4, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5547749157369349\n",
      "tree depth: 4, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5232843476415834\n",
      "tree depth: 4, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5219207896086613\n",
      "avgfold accuracy: 0.5348431923316814\n",
      "standard deviation: 0.015342132942172723\n",
      "avg train time: 1.018278217315674\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7409437285906855\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7229649971383125\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7305207543734924\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7261524552809527\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7558367114953888\n",
      "avgfold accuracy: 0.7352837293757665\n",
      "standard deviation: 0.011935475914494481\n",
      "avg train time: 0.9219663619995118\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6768847201915793\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6335238115872165\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.7098456349822895\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6501554442384154\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6467710249743048\n",
      "avgfold accuracy: 0.6634361271947611\n",
      "standard deviation: 0.027142423854495046\n",
      "avg train time: 1.095313549041748\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6485927769488418\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5917568955836128\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6499262649137647\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6127413537025249\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6063039771545808\n",
      "avgfold accuracy: 0.621864253660665\n",
      "standard deviation: 0.02338244192426262\n",
      "avg train time: 0.8204861640930176\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5865285937876836\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.579113653228471\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.6132992305289937\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5810371866899141\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5779492988539663\n",
      "avgfold accuracy: 0.5875855926178057\n",
      "standard deviation: 0.013189556140148023\n",
      "avg train time: 0.8350778102874756\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5761342245812706\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5750585427240015\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5852090137284843\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.557033275214716\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5876515717117627\n",
      "avgfold accuracy: 0.5762173255920471\n",
      "standard deviation: 0.010779331261840164\n",
      "avg train time: 0.8680997371673584\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5403044888634037\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5256026244472971\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.576677819785067\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5654106587699148\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.555386972011589\n",
      "avgfold accuracy: 0.5526765127754543\n",
      "standard deviation: 0.01806449670782194\n",
      "avg train time: 0.8301816940307617\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5431957783308465\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5269453607260922\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5664427934481184\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5384459266229393\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5614521178947202\n",
      "avgfold accuracy: 0.5472963954045433\n",
      "standard deviation: 0.014671601874264957\n",
      "avg train time: 0.82045578956604\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5470827291440877\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5205281393253639\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5476162825066059\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5434321349827951\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5517926207870342\n",
      "avgfold accuracy: 0.5420903813491773\n",
      "standard deviation: 0.011102460575236048\n",
      "avg train time: 0.8881501674652099\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.534721742706948\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5202831355428811\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5445881480853726\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5311082205555402\n",
      "tree depth: 4, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5582831477407076\n",
      "avgfold accuracy: 0.5377968789262899\n",
      "standard deviation: 0.012858727523325921\n",
      "avg train time: 1.0702759742736816\n",
      "tree depth: 4, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5993734945305237\n",
      "tree depth: 4, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5128490671982002\n",
      "tree depth: 4, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5642179241912288\n",
      "tree depth: 4, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5226053745883265\n",
      "tree depth: 4, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.532983965034785\n",
      "avgfold accuracy: 0.5464059651086128\n",
      "standard deviation: 0.03160857052094515\n",
      "avg train time: 1.0413333892822265\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7372152679998358\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7231390455143725\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7398552134812534\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7370104890266516\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7305552530007386\n",
      "avgfold accuracy: 0.7335550538045703\n",
      "standard deviation: 0.006041874643603019\n",
      "avg train time: 0.9336783409118652\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.6815923301240001\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.6592956096116591\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.7136448621109421\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.6542505004658714\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.6498585033636395\n",
      "avgfold accuracy: 0.6717283611352226\n",
      "standard deviation: 0.023633156967414488\n",
      "avg train time: 1.0116257667541504\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6240045888360838\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5675336586810238\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6332651172620664\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6078958292973182\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6193487986153332\n",
      "avgfold accuracy: 0.6104095985383651\n",
      "standard deviation: 0.022942419444287793\n",
      "avg train time: 0.9366682052612305\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5877620530125401\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.569359180600467\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6155731431653408\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5811953984815358\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.6199125077481538\n",
      "avgfold accuracy: 0.5947604566016075\n",
      "standard deviation: 0.01971804694162785\n",
      "avg train time: 0.9104015827178955\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5692618533673981\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5282978466027165\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5866474887435635\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5642224559267152\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5920080640210393\n",
      "avgfold accuracy: 0.5680875417322866\n",
      "standard deviation: 0.022433014751624653\n",
      "avg train time: 0.9447621822357177\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5498798876783422\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.51073936255685\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5646296270139828\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5591320030627589\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5556031281747627\n",
      "avgfold accuracy: 0.5479968016973393\n",
      "standard deviation: 0.01923622922541048\n",
      "avg train time: 0.9399813175201416\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5352072206494121\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5194941403111566\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5839015826233159\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.524628917240934\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5568652108388998\n",
      "avgfold accuracy: 0.5440194143327436\n",
      "standard deviation: 0.023709647978192614\n",
      "avg train time: 1.0268534183502198\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5498963996825692\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5217287842432056\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5273135004522453\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5247889740680265\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5309599393218063\n",
      "avgfold accuracy: 0.5309375195535706\n",
      "standard deviation: 0.009951668909712429\n",
      "avg train time: 0.9182743072509766\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.536752719226878\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5419409643796638\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5367838424166903\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5260389855994981\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5577159729799845\n",
      "avgfold accuracy: 0.539846496920543\n",
      "standard deviation: 0.01032766818294276\n",
      "avg train time: 0.9035861015319824\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5195140617228725\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5273521356132949\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5466505856694983\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5209176283914059\n",
      "tree depth: 4, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5523492006281698\n",
      "avgfold accuracy: 0.5333567224050484\n",
      "standard deviation: 0.01356352761438874\n",
      "avg train time: 0.9774857521057129\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7371822439913818\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7231096161727772\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7437412800871324\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7322092454727442\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7269643674040862\n",
      "avgfold accuracy: 0.7326413506256244\n",
      "standard deviation: 0.007308857115171931\n",
      "avg train time: 1.0268122196197509\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.6623226961000648\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.6391300108870509\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.700804651457203\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.6501476028376645\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.6705919688529167\n",
      "avgfold accuracy: 0.6645993860269801\n",
      "standard deviation: 0.021021221268952735\n",
      "avg train time: 0.9904956340789794\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6007770148898249\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.560454909012781\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6593692733680587\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6022730837000341\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6000615891587238\n",
      "avgfold accuracy: 0.6045871740258845\n",
      "standard deviation: 0.031588252463822075\n",
      "avg train time: 0.9481028556823731\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.590443627517205\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5647134972349057\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.622487344192453\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5698281349458943\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5993562843716025\n",
      "avgfold accuracy: 0.5893657776524122\n",
      "standard deviation: 0.02091633126155012\n",
      "avg train time: 0.9372534275054931\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5463810315099077\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5612538343904425\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.600231458594571\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5475622468841964\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5704415606019498\n",
      "avgfold accuracy: 0.5651740263962135\n",
      "standard deviation: 0.019674805252859664\n",
      "avg train time: 0.8259665966033936\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5464222489750048\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5282489180654631\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5839827984596979\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5391867083644682\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.559910111514004\n",
      "avgfold accuracy: 0.5515501570757276\n",
      "standard deviation: 0.0192034533888577\n",
      "avg train time: 0.9207068920135498\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5360939277855\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5315567399511437\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5519960524656918\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5498247216302733\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.554738305486188\n",
      "avgfold accuracy: 0.5448419494637593\n",
      "standard deviation: 0.00924090629565837\n",
      "avg train time: 0.8694129943847656\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5523798301365093\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5286605677515802\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5590622225779502\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5285832895137409\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.559417497262154\n",
      "avgfold accuracy: 0.5456206814483868\n",
      "standard deviation: 0.014104122986334545\n",
      "avg train time: 0.8183328151702881\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.538129845397604\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5322207958921694\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5353839481934511\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.525361857581712\n",
      "tree depth: 4, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5611650648864561\n",
      "avgfold accuracy: 0.5384523023902785\n",
      "standard deviation: 0.012129136611640463\n",
      "avg train time: 0.8717287063598633\n",
      "tree depth: 4, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5656474131693742\n",
      "tree depth: 4, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5201116148403323\n",
      "tree depth: 4, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5637893504459417\n",
      "tree depth: 4, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5216081329163553\n",
      "tree depth: 4, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5465144694915824\n",
      "avgfold accuracy: 0.5435341961727171\n",
      "standard deviation: 0.019685107242901526\n",
      "avg train time: 0.8314669132232666\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7454184817362222\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7310459693537641\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7348353947246586\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7281663114973385\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7442558702785771\n",
      "avgfold accuracy: 0.736744405518112\n",
      "standard deviation: 0.006947832684272031\n",
      "avg train time: 0.8480527400970459\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6630541653782299\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6576881898066149\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6960823625359848\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6613954003265713\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6731797036987162\n",
      "avgfold accuracy: 0.6702799643492234\n",
      "standard deviation: 0.013884326957090003\n",
      "avg train time: 0.8872691631317139\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5972681514461013\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5874419763518088\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6144981122876291\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.613763503353352\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.601284559736533\n",
      "avgfold accuracy: 0.6028512606350848\n",
      "standard deviation: 0.01025473448318578\n",
      "avg train time: 0.8583533763885498\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5903842093201758\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5638167147827374\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.6150738161712893\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.576320814767664\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5832345794411031\n",
      "avgfold accuracy: 0.585766026896594\n",
      "standard deviation: 0.01707434731823922\n",
      "avg train time: 0.8771378517150878\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.553895682160931\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5498184588774602\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.6116822376126886\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.554849675735016\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5594881960713642\n",
      "avgfold accuracy: 0.565946850091492\n",
      "standard deviation: 0.023073546817978404\n",
      "avg train time: 0.8764091968536377\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5622623272118831\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.539123150058949\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5825555707263076\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5338628585134549\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5602823199507286\n",
      "avgfold accuracy: 0.5556172452922646\n",
      "standard deviation: 0.017532267019217028\n",
      "avg train time: 1.6142035484313966\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5512784669454693\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.527773534897241\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5752184195947526\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5535147925718872\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5583894930083433\n",
      "avgfold accuracy: 0.5532349414035387\n",
      "standard deviation: 0.015244881409119683\n",
      "avg train time: 0.8205294132232666\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5437241624661129\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5366778064849269\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5607065490044063\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.544680301478796\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5268196031757034\n",
      "avgfold accuracy: 0.542521684521989\n",
      "standard deviation: 0.011113065695240244\n",
      "avg train time: 0.8458613395690918\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5389554456089577\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5319757921096866\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5378042062740999\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5215541656288342\n",
      "tree depth: 4, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5475104909507504\n",
      "avgfold accuracy: 0.5355600201144657\n",
      "standard deviation: 0.008583585373019361\n",
      "avg train time: 0.9052957057952881\n",
      "tree depth: 4, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5342643476807639\n",
      "tree depth: 4, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5095461201114343\n",
      "tree depth: 4, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5630729247588243\n",
      "tree depth: 4, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5339283572726685\n",
      "tree depth: 4, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.536947059068162\n",
      "avgfold accuracy: 0.5355517617783707\n",
      "standard deviation: 0.01697025363440711\n",
      "avg train time: 0.9267918586730957\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7350125416177561\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7271378250091629\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7531408688028132\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7334149761529165\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7432349953164514\n",
      "avgfold accuracy: 0.7383882413798201\n",
      "standard deviation: 0.00898700866826974\n",
      "avg train time: 0.8572966575622558\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6565698888091642\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6485977731196366\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.696261665131282\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6597422485447283\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6613578528157731\n",
      "avgfold accuracy: 0.664505885684117\n",
      "standard deviation: 0.0164747077048205\n",
      "avg train time: 0.8913336753845215\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6462034774280903\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6125399688373966\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6052070729184813\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6071744204282328\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.586119962214754\n",
      "avgfold accuracy: 0.6114489803653911\n",
      "standard deviation: 0.019533856524683813\n",
      "avg train time: 0.8487491130828857\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5841838891874392\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5636794982206984\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6140429897262621\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5816197566398214\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5861590743010818\n",
      "avgfold accuracy: 0.5859370416150605\n",
      "standard deviation: 0.016168514765970515\n",
      "avg train time: 1.3358218193054199\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5535654420763896\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5418306494857087\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5941422326011091\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5486983274753456\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5565247871609378\n",
      "avgfold accuracy: 0.5589522877598982\n",
      "standard deviation: 0.018284076581620012\n",
      "avg train time: 1.214090061187744\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5536017184493127\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5232479160234641\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.585887643317833\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5356451627782545\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5674675567719359\n",
      "avgfold accuracy: 0.55316999946816\n",
      "standard deviation: 0.02225983703393489\n",
      "avg train time: 1.0787950038909913\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5583918884028689\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5297680498457217\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5502970590189336\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5204102436369339\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5648197180365139\n",
      "avgfold accuracy: 0.5447373917881944\n",
      "standard deviation: 0.016950199647347546\n",
      "avg train time: 0.8534376621246338\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5513395738702036\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5440309892772478\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.565493182839473\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5302922536185758\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5651316245477356\n",
      "avgfold accuracy: 0.551257524830647\n",
      "standard deviation: 0.013318534056337481\n",
      "avg train time: 0.8587000370025635\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5266720781007793\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5331813118264427\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5166820728269337\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5159332650670209\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5412397838240333\n",
      "avgfold accuracy: 0.5267417023290419\n",
      "standard deviation: 0.009692123057558828\n",
      "avg train time: 0.8818745613098145\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5373587222911056\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5133636293058467\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5394107366027873\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.533293665070711\n",
      "tree depth: 4, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5404387286888639\n",
      "avgfold accuracy: 0.5327730963918629\n",
      "standard deviation: 0.010008461972579553\n",
      "avg train time: 0.8631278038024902\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7319231080995884\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7282184054352203\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7205406228273783\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7319430991060802\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7472441326919612\n",
      "avgfold accuracy: 0.7319738736320457\n",
      "standard deviation: 0.008694907720125161\n",
      "avg train time: 0.8314203262329102\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6446332733897544\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6314998672971406\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6811692517105024\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6671108589562634\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6898791783095262\n",
      "avgfold accuracy: 0.6628584859326374\n",
      "standard deviation: 0.021904364730300375\n",
      "avg train time: 0.8970088481903076\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6277429316112814\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.603329487658634\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6304050381544414\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6000276755320622\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5864921706514786\n",
      "avgfold accuracy: 0.6095994607215796\n",
      "standard deviation: 0.016893407451123516\n",
      "avg train time: 0.8063229560852051\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5855708975425133\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5749653799003013\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6495740680580862\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5784273840164578\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5929437835547045\n",
      "avgfold accuracy: 0.5962963026144126\n",
      "standard deviation: 0.02734629958282784\n",
      "avg train time: 0.8969940185546875\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5578107158904524\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5625622665287279\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.604692444390039\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5618345187686233\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5526680383951965\n",
      "avgfold accuracy: 0.5679135967946078\n",
      "standard deviation: 0.018723091674090007\n",
      "avg train time: 0.8089266300201416\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5576207652963705\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5209275117401407\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5989288664361758\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5297059935977269\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5605481831198177\n",
      "avgfold accuracy: 0.5535462640380463\n",
      "standard deviation: 0.02741368464069783\n",
      "avg train time: 0.9297943592071534\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5575497761872882\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5293686774309448\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5590348890677992\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5102814601610716\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5697079366839685\n",
      "avgfold accuracy: 0.5451885479062144\n",
      "standard deviation: 0.02197496134656839\n",
      "avg train time: 0.8779821872711182\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5548236067621161\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5281508804428484\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5337283744853061\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5246713530567625\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5629835293558487\n",
      "avgfold accuracy: 0.5408715488205763\n",
      "standard deviation: 0.015224232593746256\n",
      "avg train time: 0.860047721862793\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5362854420163459\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5289178488054034\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5451091849487307\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5048206164263508\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5392795256644599\n",
      "avgfold accuracy: 0.5308825235722582\n",
      "standard deviation: 0.014036965603663307\n",
      "avg train time: 0.8548919200897217\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5425121563376575\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.513037739971004\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5497165161885003\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5347037334292751\n",
      "tree depth: 4, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5462663305337662\n",
      "avgfold accuracy: 0.5372472952920406\n",
      "standard deviation: 0.013092864500008353\n",
      "avg train time: 0.8769101619720459\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7419822961292859\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7286226526489116\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7512738200424467\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7316248304873662\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7378610936729517\n",
      "avgfold accuracy: 0.7382729385961924\n",
      "standard deviation: 0.00800346460252812\n",
      "avg train time: 1.398361587524414\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.6591028552757855\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.6207874064083745\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.688940600228189\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.658542514229836\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.6547858340974217\n",
      "avgfold accuracy: 0.6564318420479214\n",
      "standard deviation: 0.02164090451612853\n",
      "avg train time: 1.066038465499878\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6203653180862487\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5916148042226591\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6435604342601616\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6061444293766547\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6047796949851374\n",
      "avgfold accuracy: 0.6132929361861723\n",
      "standard deviation: 0.01766158461848589\n",
      "avg train time: 1.0974853038787842\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5915070256076167\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.57723432797286\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.6166586366306701\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5827309292521149\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5849289744315875\n",
      "avgfold accuracy: 0.5906119787789699\n",
      "standard deviation: 0.013802887489335903\n",
      "avg train time: 0.9625885486602783\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5702872863571816\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5739387833585198\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5943376214248685\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5618787996199227\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.565748308278494\n",
      "avgfold accuracy: 0.5732381598077974\n",
      "standard deviation: 0.011309453990538287\n",
      "avg train time: 0.9750185966491699\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5369030159926266\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5320933289279595\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5924922825338086\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5409459496858827\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5513778346360794\n",
      "avgfold accuracy: 0.5507624823552714\n",
      "standard deviation: 0.021811654190221838\n",
      "avg train time: 1.0008161067962646\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5510423577941164\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5211970700653042\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5625028445160085\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5495179844832517\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5460393814151248\n",
      "avgfold accuracy: 0.5460599276547611\n",
      "standard deviation: 0.013603781766039151\n",
      "avg train time: 0.9440336227416992\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5533424674738534\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5291971567283961\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5531410518980965\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5405464995064531\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5664786646044531\n",
      "avgfold accuracy: 0.5485411680422505\n",
      "standard deviation: 0.012681251431511083\n",
      "avg train time: 0.996269416809082\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5374858772327482\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5249582482500375\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.537218039802822\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5127621103515715\n",
      "tree depth: 4, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5494247057681911\n",
      "avgfold accuracy: 0.5323697962810741\n",
      "standard deviation: 0.012489532248169951\n",
      "avg train time: 1.0103330612182617\n",
      "tree depth: 4, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5257523469562372\n",
      "tree depth: 4, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5180141874703224\n",
      "tree depth: 4, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5456085119427821\n",
      "tree depth: 4, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5272175020064761\n",
      "tree depth: 4, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5545753219568321\n",
      "avgfold accuracy: 0.53423357406653\n",
      "standard deviation: 0.013619110844153386\n",
      "avg train time: 1.0338223934173585\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7556477309003646\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7290637316766239\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7719239600057126\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7280274725781604\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.7425614752880927\n",
      "avgfold accuracy: 0.745444874089791\n",
      "standard deviation: 0.016644890921044087\n",
      "avg train time: 1.1208364009857177\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.6485912133120779\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.6291182572052236\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.6969998006877058\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.6707681805182705\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.6382032996738349\n",
      "avgfold accuracy: 0.6567361502794226\n",
      "standard deviation: 0.02445654916153677\n",
      "avg train time: 1.0298594951629638\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6360237642766291\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5915805000821494\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6558635218220805\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5971092906761133\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5826282925940521\n",
      "avgfold accuracy: 0.6126410738902048\n",
      "standard deviation: 0.02828597290176127\n",
      "avg train time: 1.0542248249053956\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5973704758359327\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.55103860299096\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.6223409987481514\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5930409874629841\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5999163298406405\n",
      "avgfold accuracy: 0.5927414789757337\n",
      "standard deviation: 0.02318731221318454\n",
      "avg train time: 0.9728606224060059\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5653418785457027\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5594871711541899\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5836515267792546\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5644443214420798\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5729761228139314\n",
      "avgfold accuracy: 0.5691802041470316\n",
      "standard deviation: 0.008425489828713863\n",
      "avg train time: 1.081308698654175\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5509861919615561\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5292706398083301\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5900334436613197\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5576273766363158\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5538095172083278\n",
      "avgfold accuracy: 0.5563454338551699\n",
      "standard deviation: 0.019517798039667106\n",
      "avg train time: 1.0991621494293213\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5372283149849941\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5276877745459666\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5663125342322789\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5274605854297549\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5664183626789502\n",
      "avgfold accuracy: 0.5450215143743888\n",
      "standard deviation: 0.017780390111121033\n",
      "avg train time: 1.0610005378723144\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5385988113358411\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5336664445924939\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5315413013261853\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5405677174143673\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5462697961616686\n",
      "avgfold accuracy: 0.5381288141661112\n",
      "standard deviation: 0.005210619960688598\n",
      "avg train time: 1.107370376586914\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5310824719571128\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5156546042475748\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5516108984590178\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5282880838384119\n",
      "tree depth: 4, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5470922391718931\n",
      "avgfold accuracy: 0.5347456595348021\n",
      "standard deviation: 0.013087871727594414\n",
      "avg train time: 1.00246901512146\n",
      "tree depth: 4, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5171082501978939\n",
      "tree depth: 4, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5183523540765055\n",
      "tree depth: 4, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5305538946197712\n",
      "tree depth: 4, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5307705790643825\n",
      "tree depth: 4, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5344196261478655\n",
      "avgfold accuracy: 0.5262409408212838\n",
      "standard deviation: 0.00709430652854695\n",
      "avg train time: 1.022657585144043\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.73408786938104\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7467105037472759\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7445606314799382\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.721455456231146\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7311295570531469\n",
      "avgfold accuracy: 0.7355888035785094\n",
      "standard deviation: 0.009231198197602639\n",
      "avg train time: 0.9338488101959228\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6636271444340035\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6422394104020986\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6891520752804104\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6750311349735698\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6480400388942469\n",
      "avgfold accuracy: 0.6636179607968659\n",
      "standard deviation: 0.017216072138391103\n",
      "avg train time: 0.9168245792388916\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6208325952967808\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5938837522952178\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6696211706275408\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6159106633825036\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5848686725060847\n",
      "avgfold accuracy: 0.6170233708216255\n",
      "standard deviation: 0.02950133360606417\n",
      "avg train time: 0.9973266124725342\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5728697888364841\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5671073845981631\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.612631848221334\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5710490871687008\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5680593869997366\n",
      "avgfold accuracy: 0.5783434991648837\n",
      "standard deviation: 0.01726773908609959\n",
      "avg train time: 0.9544397830963135\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5770588968179867\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5457510711016503\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.574528542723715\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5458339099069179\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5740112563594272\n",
      "avgfold accuracy: 0.5634367353819394\n",
      "standard deviation: 0.014443366310796723\n",
      "avg train time: 0.8701589584350586\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5424428559562802\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5357515946911634\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.6027385561524462\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.564637127648779\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.563047296909254\n",
      "avgfold accuracy: 0.5617234862715846\n",
      "standard deviation: 0.023404100476762002\n",
      "avg train time: 0.8948229312896728\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5421291278759659\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5259285137821399\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5726027726903445\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.538955156412882\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5794703134313874\n",
      "avgfold accuracy: 0.5518171768385439\n",
      "standard deviation: 0.020621589149802805\n",
      "avg train time: 0.8467082023620606\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5520759842405429\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5216258718216763\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5482080726187282\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5410557292963958\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5392440772419147\n",
      "avgfold accuracy: 0.5404419470438515\n",
      "standard deviation: 0.010502569546746544\n",
      "avg train time: 0.875661563873291\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5424791323292033\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5285061991192863\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5368602193062991\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5190735154383342\n",
      "tree depth: 4, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5610870387496807\n",
      "avgfold accuracy: 0.5376012209885607\n",
      "standard deviation: 0.014155719933703795\n",
      "avg train time: 0.8816303730010986\n",
      "tree depth: 4, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5281532173890419\n",
      "tree depth: 4, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5071742596173463\n",
      "tree depth: 4, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5385640516998305\n",
      "tree depth: 4, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5309191044197825\n",
      "tree depth: 4, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5302864192934476\n",
      "avgfold accuracy: 0.5270194104838897\n",
      "standard deviation: 0.010528772304254707\n",
      "avg train time: 0.8529913902282715\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7312197217377032\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7357602610003449\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.767549813687471\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7213918025074032\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7317676286589605\n",
      "avgfold accuracy: 0.7375378455183765\n",
      "standard deviation: 0.015733955864475305\n",
      "avg train time: 0.8342526435852051\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.654476116636795\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.6365008693391397\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.7091292092951721\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.6557126910764859\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.6448568101568641\n",
      "avgfold accuracy: 0.6601351393008914\n",
      "standard deviation: 0.025471582355530917\n",
      "avg train time: 0.8676611900329589\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6088464438646897\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.587963940931881\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6494486477890198\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6153262483971255\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6008131353238579\n",
      "avgfold accuracy: 0.6124796832613149\n",
      "standard deviation: 0.020616195878854486\n",
      "avg train time: 0.8645318031311036\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5870636077428291\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5621775185106948\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.6079320538634936\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5764250592717645\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5916570454234898\n",
      "avgfold accuracy: 0.5850510569624544\n",
      "standard deviation: 0.015289447739144799\n",
      "avg train time: 0.830066442489624\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5771744808475762\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5392015079377976\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5905062218392915\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5495239808485318\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5722565594434399\n",
      "avgfold accuracy: 0.5657325501833274\n",
      "standard deviation: 0.018728743705780097\n",
      "avg train time: 0.8846312046051026\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5479199002674945\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5178426667677736\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5984946690500441\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5323679185232336\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5491623082270045\n",
      "avgfold accuracy: 0.5491574925671101\n",
      "standard deviation: 0.027196311340387424\n",
      "avg train time: 0.8199398040771484\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5418913299969077\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5257055368688264\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5924440238484222\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5326552827978117\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.555564214124315\n",
      "avgfold accuracy: 0.5496520775272566\n",
      "standard deviation: 0.023616354807236235\n",
      "avg train time: 0.8748328685760498\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5364456209664425\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5207976976505275\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.545429209347485\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5301552597348684\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5708068377828696\n",
      "avgfold accuracy: 0.5407269250964386\n",
      "standard deviation: 0.01705389342386276\n",
      "avg train time: 0.8787536144256591\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5302898957542133\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5351807015738379\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5397524708708483\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5236625799130988\n",
      "tree depth: 4, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5278050297152839\n",
      "avgfold accuracy: 0.5313381355654564\n",
      "standard deviation: 0.0056224932260194056\n",
      "avg train time: 0.8414724349975586\n",
      "tree depth: 4, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5219644681683584\n",
      "tree depth: 4, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5068091913430793\n",
      "tree depth: 4, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5636373813607956\n",
      "tree depth: 4, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5213419865496914\n",
      "tree depth: 4, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5600555688679675\n",
      "avgfold accuracy: 0.5347617192579784\n",
      "standard deviation: 0.022798181367006773\n",
      "avg train time: 0.8371727943420411\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7377387110429282\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7308229924404507\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.750497888388254\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7045212594212125\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7317321802364152\n",
      "avgfold accuracy: 0.7310626063058521\n",
      "standard deviation: 0.015014988528711545\n",
      "avg train time: 0.8399922847747803\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6397819965114638\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6270110802373846\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6909322845635506\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6630909879242428\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6365726722367558\n",
      "avgfold accuracy: 0.6514778042946795\n",
      "standard deviation: 0.023017808387257813\n",
      "avg train time: 0.7689470767974853\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6364861003949871\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5719563651332716\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6445478409665757\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6237594442753162\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6095012664394535\n",
      "avgfold accuracy: 0.6172502034419208\n",
      "standard deviation: 0.025570016721315748\n",
      "avg train time: 0.8570908546447754\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5851415854326095\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5405319669452524\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5984673355398931\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5720869196210298\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5687080535251376\n",
      "avgfold accuracy: 0.5729871722127845\n",
      "standard deviation: 0.019336132727740353\n",
      "avg train time: 0.8290421009063721\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5658702626809691\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5337962586821071\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.584367952466372\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5524926429210602\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5654824451094049\n",
      "avgfold accuracy: 0.5604019123719827\n",
      "standard deviation: 0.016736206842152563\n",
      "avg train time: 0.7715624332427978\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5533045023732253\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5165905656391674\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.6204634874001673\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5605305399496305\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5487263322368747\n",
      "avgfold accuracy: 0.5599230855198131\n",
      "standard deviation: 0.0338005605482304\n",
      "avg train time: 0.8234956741333008\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5373273870103565\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5172374695099383\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5789790659315652\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.547184014612681\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5431042916355585\n",
      "avgfold accuracy: 0.5447664457400199\n",
      "standard deviation: 0.019961437337785826\n",
      "avg train time: 0.8374361991882324\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5375189012412024\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5374888285858208\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5436329138358829\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5523381212003801\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5419770714057973\n",
      "avgfold accuracy: 0.5425911672538166\n",
      "standard deviation: 0.005444762681217183\n",
      "avg train time: 0.8356918334960938\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5313136400162919\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5314440779317853\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5502431766927028\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5357434109170749\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5382869698331944\n",
      "avgfold accuracy: 0.5374062550782099\n",
      "standard deviation: 0.006942546799821553\n",
      "avg train time: 0.8622161388397217\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5094829566094551\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5144099055913943\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5623564990717069\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5148317788909492\n",
      "tree depth: 4, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5555607484964126\n",
      "avgfold accuracy: 0.5313283777319836\n",
      "standard deviation: 0.022740039593861193\n",
      "avg train time: 0.8261839389801026\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7429466221943353\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7412169664668\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.737657677734515\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7102155001429902\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7317499044476878\n",
      "avgfold accuracy: 0.7327573341972657\n",
      "standard deviation: 0.011904197008088058\n",
      "avg train time: 0.8478403091430664\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.6779646302862181\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.6339134344041641\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.6929239688989121\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.6564862221976218\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.640298222231904\n",
      "avgfold accuracy: 0.660317295603764\n",
      "standard deviation: 0.022303487947538465\n",
      "avg train time: 0.8136709213256836\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6369319245091181\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5966452356062535\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6425561566312141\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6111421692082031\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5811571830584266\n",
      "avgfold accuracy: 0.613686533802643\n",
      "standard deviation: 0.023361684899033265\n",
      "avg train time: 0.8588299751281738\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.578700215056346\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.550144348212303\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.6227534862649765\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5488759121394109\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.583560744535695\n",
      "avgfold accuracy: 0.5768069412417463\n",
      "standard deviation: 0.027023395782215635\n",
      "avg train time: 0.8033243179321289\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5659809056183845\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5282120862514421\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.6429694288421106\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5425257613077611\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5675916262508441\n",
      "avgfold accuracy: 0.5694559616541086\n",
      "standard deviation: 0.039613317376543535\n",
      "avg train time: 0.8105014324188232\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5236503813272249\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5131455271914478\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.6097339730159402\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5310985341193185\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5582085872318346\n",
      "avgfold accuracy: 0.5471674005771533\n",
      "standard deviation: 0.03465611150084699\n",
      "avg train time: 0.810914945602417\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5525614621830071\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5380033906934673\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5638158992620214\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5401106098764749\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5558372065850891\n",
      "avgfold accuracy: 0.5500657137200119\n",
      "standard deviation: 0.009728414280386916\n",
      "avg train time: 0.8257110118865967\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5388630034034743\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5266489007328448\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5586714449304316\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5371437928394174\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5570070045290806\n",
      "avgfold accuracy: 0.5436668292870497\n",
      "standard deviation: 0.012315318964497743\n",
      "avg train time: 0.9025860786437988\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5333281045319948\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.52803821842349\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5381949839216185\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.533324569414847\n",
      "tree depth: 4, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5563228895811343\n",
      "avgfold accuracy: 0.5378417531746169\n",
      "standard deviation: 0.009783296393772983\n",
      "avg train time: 0.8345585823059082\n",
      "tree depth: 4, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5230707724515723\n",
      "tree depth: 4, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5229417064323875\n",
      "tree depth: 4, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5511332813342729\n",
      "tree depth: 4, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5288300630079613\n",
      "tree depth: 4, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5348875849326381\n",
      "avgfold accuracy: 0.5321726816317665\n",
      "standard deviation: 0.010450873591542781\n",
      "avg train time: 0.881761121749878\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7435526252585629\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.747028990609693\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7410709661624222\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7241058496849602\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7253125501278321\n",
      "avgfold accuracy: 0.7362141963686941\n",
      "standard deviation: 0.009590187443949814\n",
      "avg train time: 0.8294495105743408\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6820167011417301\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6519129974777429\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7184202486643199\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6604193765625144\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6536692087872481\n",
      "avgfold accuracy: 0.673287706526711\n",
      "standard deviation: 0.024981990566569596\n",
      "avg train time: 0.7648833274841309\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6572319326150111\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5920999369887103\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6314197783710065\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6318609950276294\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5909301547254331\n",
      "avgfold accuracy: 0.6207085595455581\n",
      "standard deviation: 0.025605743435839826\n",
      "avg train time: 0.8518426418304443\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5728747299286582\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5668892824837641\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.6155353470675721\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.556892591260067\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5877648482352033\n",
      "avgfold accuracy: 0.5799913597950529\n",
      "standard deviation: 0.020394440968642714\n",
      "avg train time: 0.8159591674804687\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5583440411178927\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5582522220958386\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.6148357923029881\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5406276810671685\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5877862361102585\n",
      "avgfold accuracy: 0.5719691945388292\n",
      "standard deviation: 0.02624429314493582\n",
      "avg train time: 0.8157717704772949\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5327898382123804\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5359622943331366\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5752393447699878\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5443311285159458\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5336892698219063\n",
      "avgfold accuracy: 0.5444023751306714\n",
      "standard deviation: 0.015948544607709583\n",
      "avg train time: 0.8396757125854493\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5451012886368389\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5302777371544535\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.6030376553759652\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5373965626989179\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5510482039135851\n",
      "avgfold accuracy: 0.5533722895559521\n",
      "standard deviation: 0.02580326177656183\n",
      "avg train time: 0.8497979640960693\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5359997968523116\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5194892655122421\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5527719841198845\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5257110305445623\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5474998960311629\n",
      "avgfold accuracy: 0.5362943946120327\n",
      "standard deviation: 0.01258237628533405\n",
      "avg train time: 0.8311398506164551\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5305260049055663\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5284547429085217\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5375927312218786\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5189249900829344\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5706793026760588\n",
      "avgfold accuracy: 0.5372355543589921\n",
      "standard deviation: 0.01775392545050815\n",
      "avg train time: 0.8183739662170411\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5332901394313666\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.514397628320054\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5563106928168582\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5387236044612957\n",
      "tree depth: 4, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5432071712752916\n",
      "avgfold accuracy: 0.5371858472609732\n",
      "standard deviation: 0.01370299397666023\n",
      "avg train time: 0.8217759132385254\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7358001767284816\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.737622434185701\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7410492562931157\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7321534331497523\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7357058691893797\n",
      "avgfold accuracy: 0.7364662339092861\n",
      "standard deviation: 0.0028969960413048283\n",
      "avg train time: 0.7910521984100342\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.6929956203159697\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.65911181163777\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.6812504675468841\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.6610965045803006\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.6521875043320349\n",
      "avgfold accuracy: 0.6693283816825919\n",
      "standard deviation: 0.01528545777096406\n",
      "avg train time: 0.8268415927886963\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6217688384455499\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5717456654912985\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6494213142788687\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6319228037159015\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5832663641998658\n",
      "avgfold accuracy: 0.6116249972262968\n",
      "standard deviation: 0.029455163256947754\n",
      "avg train time: 0.8386362075805665\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5785185830098482\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5829948959049884\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5988959092851803\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5752465428647866\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5864852393956738\n",
      "avgfold accuracy: 0.5844282340920953\n",
      "standard deviation: 0.008183236332621942\n",
      "avg train time: 0.7907282829284668\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.58321956312239\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5486815474417236\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5865606492663372\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.548368527384939\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5949005760863753\n",
      "avgfold accuracy: 0.5723461726603529\n",
      "standard deviation: 0.01981880439843699\n",
      "avg train time: 0.8371043682098389\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5375800081659367\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5193103423372675\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5825499470854631\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5599922508510227\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5592826348277781\n",
      "avgfold accuracy: 0.5517430366534936\n",
      "standard deviation: 0.021570497641078656\n",
      "avg train time: 0.9070310592651367\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5572525601112008\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5379862386232124\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5670667560171649\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5464953551232022\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5362238320338879\n",
      "avgfold accuracy: 0.5490049483817337\n",
      "standard deviation: 0.011707424588374137\n",
      "avg train time: 0.860468578338623\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5616018470428001\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5214420738477872\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5515779413080222\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5311294384634545\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.562398630383853\n",
      "avgfold accuracy: 0.5456299862091833\n",
      "standard deviation: 0.0165347326868725\n",
      "avg train time: 0.7794229984283447\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5267331850255136\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5357467198922489\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5419942110502711\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5316598861613114\n",
      "tree depth: 4, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5314313637344814\n",
      "avgfold accuracy: 0.5335130731727652\n",
      "standard deviation: 0.005111821141996843\n",
      "avg train time: 0.809672737121582\n",
      "tree depth: 4, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5364555031507906\n",
      "tree depth: 4, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5107001836174259\n",
      "tree depth: 4, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5595615495720018\n",
      "tree depth: 4, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5180029336063986\n",
      "tree depth: 4, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5374327420642072\n",
      "avgfold accuracy: 0.5324305824021648\n",
      "standard deviation: 0.017074415064685593\n",
      "avg train time: 0.8395623207092285\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7372036970877829\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7411826623262902\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7452392610692868\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7295127261321599\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7246744785220186\n",
      "avgfold accuracy: 0.7355625650275077\n",
      "standard deviation: 0.007524204048039977\n",
      "avg train time: 0.7905854225158692\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.6774032221424976\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.6473530744634561\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.6973905783352244\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.661702137473593\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.657011955426084\n",
      "avgfold accuracy: 0.6681721935681711\n",
      "standard deviation: 0.017542942503964362\n",
      "avg train time: 0.8132415294647217\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6110442291545954\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5717628175615533\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6283860203089289\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6037583372540337\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6100401220693165\n",
      "avgfold accuracy: 0.6049983052696856\n",
      "standard deviation: 0.018520763973968565\n",
      "avg train time: 0.7779911994934082\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5982786360684218\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5755534250878818\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.623979701533763\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5884731408961337\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5808560695026725\n",
      "avgfold accuracy: 0.5934281946177746\n",
      "standard deviation: 0.017082386426636988\n",
      "avg train time: 0.7783307075500489\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5778184490124321\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5548120584470335\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.589762462642023\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5571278332825949\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5524021752261075\n",
      "avgfold accuracy: 0.5663845957220381\n",
      "standard deviation: 0.014780913473540516\n",
      "avg train time: 0.7959075450897217\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5463182358574683\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5351855763727524\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.596356639270381\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5494737036319524\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5446852120667223\n",
      "avgfold accuracy: 0.5544038734398552\n",
      "standard deviation: 0.02151052761474916\n",
      "avg train time: 0.7939186573028565\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5437357333781659\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5314489527306998\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5674848671748346\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5330390501757396\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5324167902740619\n",
      "avgfold accuracy: 0.5416250787467003\n",
      "standard deviation: 0.013676618912811727\n",
      "avg train time: 0.8096535205841064\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5559745059658372\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5303340681641326\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5532278913753228\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5368236791852323\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5651813315536509\n",
      "avgfold accuracy: 0.5483082952488353\n",
      "standard deviation: 0.012826860975076303\n",
      "avg train time: 0.8040492534637451\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5359221779233503\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.519214832388164\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5189503618224363\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.510059594645707\n",
      "tree depth: 4, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5531573850550242\n",
      "avgfold accuracy: 0.5274608703669363\n",
      "standard deviation: 0.015330589600135749\n",
      "avg train time: 0.8324749946594239\n",
      "tree depth: 4, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.526143693965513\n",
      "tree depth: 4, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5149856735076345\n",
      "tree depth: 4, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5319159927452417\n",
      "tree depth: 4, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.532732313028718\n",
      "tree depth: 4, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5293896138102302\n",
      "avgfold accuracy: 0.5270334574114675\n",
      "standard deviation: 0.006446834193469958\n",
      "avg train time: 0.8057383537292481\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7353940064426838\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7378748404406097\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7492443396093165\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7201187280325465\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.724656754310746\n",
      "avgfold accuracy: 0.7334577337671805\n",
      "standard deviation: 0.01027485494432398\n",
      "avg train time: 0.9416965961456298\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6855470176818548\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6515233746607952\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6921046175061063\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6689143811289772\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6647892205109722\n",
      "avgfold accuracy: 0.6725757222977412\n",
      "standard deviation: 0.014607317496771941\n",
      "avg train time: 1.030129861831665\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6412052812394611\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5675508107512787\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6456823778113627\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6195061762562385\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5929329905992368\n",
      "avgfold accuracy: 0.6133755273315156\n",
      "standard deviation: 0.02957571505290049\n",
      "avg train time: 0.9174519538879394\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5972664627183963\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5565884712811152\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.6196923946927477\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5755975608631075\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5830396131171045\n",
      "avgfold accuracy: 0.5864369005344943\n",
      "standard deviation: 0.021180976766932442\n",
      "avg train time: 0.9741593360900879\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5878281010294484\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5484536957294956\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5862623347368894\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5582099465862231\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5475245514982405\n",
      "avgfold accuracy: 0.5656557259160594\n",
      "standard deviation: 0.017868046918457406\n",
      "avg train time: 1.0395829677581787\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5529577502844568\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5143510469082039\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.600600526372783\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.547955700698346\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5426080137199257\n",
      "avgfold accuracy: 0.551694607596743\n",
      "standard deviation: 0.027872823892651603\n",
      "avg train time: 1.1120193958282472\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5358016528015868\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.51901388234402\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.564369893276375\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5251690513750127\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5482726320354722\n",
      "avgfold accuracy: 0.5385254223664934\n",
      "standard deviation: 0.01630617388412198\n",
      "avg train time: 1.0154384136199952\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5496652316233902\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.530346345435473\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5399856557923762\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5288609673520974\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5513495155052193\n",
      "avgfold accuracy: 0.5400415431417113\n",
      "standard deviation: 0.009375402667751647\n",
      "avg train time: 0.9810922622680665\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.534947969673953\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5237527285332814\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5538140578466006\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5378209208572035\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5402578229123552\n",
      "avgfold accuracy: 0.5381186999646788\n",
      "standard deviation: 0.009668954898725969\n",
      "avg train time: 0.8996852397918701\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5278560013129545\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5131578044627881\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5543841380894164\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5178447218147769\n",
      "tree depth: 4, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5461777094774032\n",
      "avgfold accuracy: 0.5318840750314678\n",
      "standard deviation: 0.01596598164424524\n",
      "avg train time: 0.8374484539031982\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7313468766793458\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7383305438650656\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7497975489295988\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7142123082316257\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7413101855794233\n",
      "avgfold accuracy: 0.7349994926570119\n",
      "standard deviation: 0.011961202753683401\n",
      "avg train time: 0.8083608627319336\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.670256776676644\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.6354056645163386\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.7045596741531712\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.6708548971853984\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.6484193766226565\n",
      "avgfold accuracy: 0.6658992778308417\n",
      "standard deviation: 0.02355240795502816\n",
      "avg train time: 0.8025938510894776\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6331820108218675\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5884759753660161\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6432782059591761\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6056988533104548\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6148611075354633\n",
      "avgfold accuracy: 0.6170992305985956\n",
      "standard deviation: 0.019484676583957014\n",
      "avg train time: 0.7945554733276368\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5824930349363987\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5616923857446435\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.6105420771270571\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.568653308609858\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.6050280319788339\n",
      "avgfold accuracy: 0.5856817676793582\n",
      "standard deviation: 0.019328423673842614\n",
      "avg train time: 0.879333209991455\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5715917346911205\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5456358814087807\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5856705446247672\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5508607090471315\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5563581399677994\n",
      "avgfold accuracy: 0.5620234019479199\n",
      "standard deviation: 0.014669642693570944\n",
      "avg train time: 0.8439272880554199\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5559579939616102\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5389859334969099\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5684071442733288\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5397904962222899\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5472835418321091\n",
      "avgfold accuracy: 0.5500850219572496\n",
      "standard deviation: 0.011020047930740783\n",
      "avg train time: 0.8402342796325684\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.551438645895566\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5282317659952083\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5626765234704613\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5336640559414755\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5714094609661379\n",
      "avgfold accuracy: 0.5494840904537697\n",
      "standard deviation: 0.016495558401228624\n",
      "avg train time: 0.8336331844329834\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5449691926030225\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5133685041047612\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5384932984510662\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5369413001964962\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5635790232474319\n",
      "avgfold accuracy: 0.5394702637205555\n",
      "standard deviation: 0.016125275051692634\n",
      "avg train time: 0.8570637226104736\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5307687438767984\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5267126342149497\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5462324745118288\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5361350196957537\n",
      "tree depth: 4, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5500627773740047\n",
      "avgfold accuracy: 0.5379823299346671\n",
      "standard deviation: 0.008904566428824656\n",
      "avg train time: 0.8833231449127197\n",
      "tree depth: 4, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5298440716400823\n",
      "tree depth: 4, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5222948025616165\n",
      "tree depth: 4, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.561140746390538\n",
      "tree depth: 4, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5196039631361913\n",
      "tree depth: 4, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5570673064545835\n",
      "avgfold accuracy: 0.5379901780366023\n",
      "standard deviation: 0.01761046508684367\n",
      "avg train time: 0.8780642509460449\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7356037214054618\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7381933273030266\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.747263902555644\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7271054161016246\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7229091866864439\n",
      "avgfold accuracy: 0.7342151108104401\n",
      "standard deviation: 0.008560339841458561\n",
      "avg train time: 0.8598761558532715\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.670256776676644\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6393432382025352\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.7050372912779161\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6700061808688271\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6830058479995406\n",
      "avgfold accuracy: 0.6735298670050927\n",
      "standard deviation: 0.02132776159508441\n",
      "avg train time: 0.824113416671753\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6612840034705231\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6001612294603959\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6547563184874446\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5977882637293701\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6184554587600182\n",
      "avgfold accuracy: 0.6264890547815505\n",
      "standard deviation: 0.026799832499051646\n",
      "avg train time: 1.2482681274414062\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5943636023389004\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5413381142472318\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5919977944865302\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5748567791215786\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5795161587376401\n",
      "avgfold accuracy: 0.5764144897863762\n",
      "standard deviation: 0.019012674512348716\n",
      "avg train time: 1.445460844039917\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5873921590996655\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5543832566906615\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.624631782307032\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5339998523971623\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5477655611643718\n",
      "avgfold accuracy: 0.5696345223317787\n",
      "standard deviation: 0.032621203337437836\n",
      "avg train time: 0.930644416809082\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5475896601829531\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5154316273342613\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5851551314022536\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5741796511037924\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5419061745607069\n",
      "avgfold accuracy: 0.5528524489167934\n",
      "standard deviation: 0.02468257929929143\n",
      "avg train time: 0.8039710044860839\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5405373456502878\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5321399103398096\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.548028770023431\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5314264891742544\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5654400654310547\n",
      "avgfold accuracy: 0.5435145161237674\n",
      "standard deviation: 0.012533744827446224\n",
      "avg train time: 0.8573461532592773\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5587320106717584\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5195407217230067\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5534449900683887\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5303019400547976\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5564681488992176\n",
      "avgfold accuracy: 0.5436975622834337\n",
      "standard deviation: 0.01579322962556785\n",
      "avg train time: 0.8110292434692383\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5321062162191913\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5186610913410933\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5400129893025273\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5415091467633465\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5407506352000856\n",
      "avgfold accuracy: 0.5346080157652487\n",
      "standard deviation: 0.008661807214100318\n",
      "avg train time: 0.8897586345672608\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5329598993468251\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5108374001794649\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.557895513276239\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5216081329163553\n",
      "tree depth: 4, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5522605795718069\n",
      "avgfold accuracy: 0.5351123050581383\n",
      "standard deviation: 0.017829214329044947\n",
      "avg train time: 0.865708589553833\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7318752608146122\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7332560587431325\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7503893390417211\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7385593963043939\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7373293673347737\n",
      "avgfold accuracy: 0.7382818844477266\n",
      "standard deviation: 0.006540313582941371\n",
      "avg train time: 0.8031941890716553\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6869505380411559\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6207579770667793\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.7061010748739389\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6560715504755579\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6493233113975589\n",
      "avgfold accuracy: 0.6638408903709981\n",
      "standard deviation: 0.029823612021689087\n",
      "avg train time: 0.8193543434143067\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6170215746847959\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5855209444832623\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.674141662390084\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6353388868901004\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6127342021827514\n",
      "avgfold accuracy: 0.6249514541261989\n",
      "standard deviation: 0.02930694345091462\n",
      "avg train time: 0.7620746612548828\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5959273016482983\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5795375801859285\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.6493135496264072\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5549866696187234\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.6069351175045895\n",
      "avgfold accuracy: 0.5973400437167895\n",
      "standard deviation: 0.031324248124865964\n",
      "avg train time: 0.8038179397583007\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5722241319439232\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5448297341068015\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.6024892850024561\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5605517578575447\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5555499555409449\n",
      "avgfold accuracy: 0.5671289728903341\n",
      "standard deviation: 0.01975298341586945\n",
      "avg train time: 0.8024004459381103\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5491252765760708\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5386380172928978\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5799286765402106\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5652196975986863\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5521505706403886\n",
      "avgfold accuracy: 0.5570124477296508\n",
      "standard deviation: 0.014254665936617004\n",
      "avg train time: 0.836694860458374\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5580500774062744\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5272761248598495\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5501129174768633\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5343951512467827\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5487866341623775\n",
      "avgfold accuracy: 0.5437241810304295\n",
      "standard deviation: 0.011218244430937767\n",
      "avg train time: 0.7924366474151612\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5501325088339223\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5314612300020402\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5535535394149216\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5435266930506739\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5547737539087332\n",
      "avgfold accuracy: 0.5466895450420582\n",
      "standard deviation: 0.00855836506700032\n",
      "avg train time: 0.7963815212249756\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5394177817273157\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5220032173672836\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5542265453634259\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5462716445723669\n",
      "tree depth: 4, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5418955796411193\n",
      "avgfold accuracy: 0.5407629537343023\n",
      "standard deviation: 0.010645506207206867\n",
      "avg train time: 0.8440228939056397\n",
      "tree depth: 4, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5397645338160842\n",
      "tree depth: 4, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5164435994792993\n",
      "tree depth: 4, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5433402229472796\n",
      "tree depth: 4, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5408859860330815\n",
      "tree depth: 4, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5348450072184079\n",
      "avgfold accuracy: 0.5350558698988305\n",
      "standard deviation: 0.009708278412732089\n",
      "avg train time: 0.8206817626953125\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6846289752650176\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6925307247742698\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.69553961580332\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6857291118921761\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6814850314579995\n",
      "avgfold accuracy: 0.6879826918385566\n",
      "standard deviation: 0.005221825949703164\n",
      "avg train time: 0.9015401840209961\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5892019748357056\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5814903885214735\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.6333197842823686\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5970917628391406\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.6288878899078936\n",
      "avgfold accuracy: 0.6059983600773163\n",
      "standard deviation: 0.021130440117456052\n",
      "avg train time: 0.8275750160217286\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5435821842479482\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5317454127239474\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6054144937180008\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5721657948874067\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5549793151523192\n",
      "avgfold accuracy: 0.5615774401459245\n",
      "standard deviation: 0.02565585652588134\n",
      "avg train time: 0.9069356441497802\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5390330645379191\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5303291933652181\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5370009411097562\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5132422808328491\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5399530456928187\n",
      "avgfold accuracy: 0.5319117051077121\n",
      "standard deviation: 0.009922993376943854\n",
      "avg train time: 0.9123923778533936\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5131932164683726\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5301136189243305\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5534779472193843\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5245786400243545\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.529449915735733\n",
      "avgfold accuracy: 0.530162667674435\n",
      "standard deviation: 0.013139682799801043\n",
      "avg train time: 0.8910630702972412\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5131106564472372\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.49794175156941445\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.529495734664593\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5344509635697746\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5350081887836439\n",
      "avgfold accuracy: 0.5220014590069326\n",
      "standard deviation: 0.014410032915382709\n",
      "avg train time: 0.8974903583526611\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5192052747347321\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5109820192139297\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.520546429563506\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5105070157473778\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5232040621119735\n",
      "avgfold accuracy: 0.5168889602743039\n",
      "standard deviation: 0.005181584200544846\n",
      "avg train time: 0.9044527530670166\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5195850508319548\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5044961895321818\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5143108581257425\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5138054779103128\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5106377943555813\n",
      "avgfold accuracy: 0.5125670741511547\n",
      "standard deviation: 0.004953538577125689\n",
      "avg train time: 0.957722806930542\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5121133063737338\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5100852367617613\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5148318949891006\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.4995216745541933\n",
      "tree depth: 5, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5112581417501223\n",
      "avgfold accuracy: 0.5095620508857823\n",
      "standard deviation: 0.005258132101781701\n",
      "avg train time: 0.9866034030914307\n",
      "tree depth: 5, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5048017408656094\n",
      "tree depth: 5, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5113936689000468\n",
      "tree depth: 5, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5135783462101631\n",
      "tree depth: 5, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5052371331838854\n",
      "tree depth: 5, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5105031299570857\n",
      "avgfold accuracy: 0.5091028038233582\n",
      "standard deviation: 0.0034837338530915294\n",
      "avg train time: 0.9064558982849121\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.685598242422241\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6856969788885098\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.7211934882947185\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6885819979889114\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6847640105434303\n",
      "avgfold accuracy: 0.6931669436275623\n",
      "standard deviation: 0.014072695747752152\n",
      "avg train time: 0.8812204360961914\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6144341311133193\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5728042190481865\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6104013553236001\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5934441277133553\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.6064423052168593\n",
      "avgfold accuracy: 0.5995052276830641\n",
      "standard deviation: 0.015095705315494461\n",
      "avg train time: 0.9236933708190918\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5575662881915152\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5413798208601672\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5800862692662012\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5609355252354726\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5769392168473084\n",
      "avgfold accuracy: 0.563381424080133\n",
      "standard deviation: 0.01404802027488849\n",
      "avg train time: 0.8866844654083252\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5333396754440478\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5229980374420666\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.561878881946962\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.541607394902167\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5458480787549088\n",
      "avgfold accuracy: 0.5411344136980304\n",
      "standard deviation: 0.012980457623403116\n",
      "avg train time: 0.8877035140991211\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5264953871464555\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5230274667836619\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5504007694186934\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5227326820358121\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5325622476280253\n",
      "avgfold accuracy: 0.5310437106025296\n",
      "standard deviation: 0.010305554694160135\n",
      "avg train time: 0.8864821434020996\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5282456595945252\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5151254177431848\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5429333590712989\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5150033671897343\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5207120776142221\n",
      "avgfold accuracy: 0.5244039762425932\n",
      "standard deviation: 0.010451696253365304\n",
      "avg train time: 0.8914333820343018\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5282621715987523\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5040673877758098\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5095403105191378\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5317156984843033\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5240902726756034\n",
      "avgfold accuracy: 0.5195351682107214\n",
      "standard deviation: 0.010811330116354573\n",
      "avg train time: 0.9136579036712646\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.51690347632729\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5077305283379283\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5114290691488109\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5100729711528704\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.514264128374779\n",
      "avgfold accuracy: 0.5120800346683356\n",
      "standard deviation: 0.003206434784518043\n",
      "avg train time: 0.9579092979431152\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5062663681496448\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5062505754970941\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5185169491303758\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5008893070969289\n",
      "tree depth: 5, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.504810192510679\n",
      "avgfold accuracy: 0.5073466784769445\n",
      "standard deviation: 0.005920724023514735\n",
      "avg train time: 0.9235662937164306\n",
      "tree depth: 5, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5148609288953069\n",
      "tree depth: 5, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5055081616772197\n",
      "tree depth: 5, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5190162761244274\n",
      "tree depth: 5, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5018441129530715\n",
      "tree depth: 5, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5150794420933185\n",
      "avgfold accuracy: 0.5112617843486688\n",
      "standard deviation: 0.006472359742536717\n",
      "avg train time: 0.9942551612854004\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6842376282557419\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6844743071014987\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.7336268350724771\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6877120637644258\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6779970255010803\n",
      "avgfold accuracy: 0.6936095719390448\n",
      "standard deviation: 0.020254381993498393\n",
      "avg train time: 0.9168526649475097\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6193778501970932\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5470446982950842\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6260421391178888\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.6109493630015037\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5974031555037141\n",
      "avgfold accuracy: 0.6001634412230568\n",
      "standard deviation: 0.028229263727786936\n",
      "avg train time: 0.8433341026306153\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5557829917349913\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5538663474576119\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5997433788822085\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5440649821492818\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5445611425878141\n",
      "avgfold accuracy: 0.5596037685623816\n",
      "standard deviation: 0.020622507764713797\n",
      "avg train time: 0.9467189311981201\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5425567512581647\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5218316966647348\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5493964917897459\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5451161911087741\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5479146821821178\n",
      "avgfold accuracy: 0.5413631626007075\n",
      "standard deviation: 0.010044349905879334\n",
      "avg train time: 0.9203406810760498\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5268091152267699\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5096784618745588\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5452016480668015\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5246307622764048\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5335725276705634\n",
      "avgfold accuracy: 0.5279785030230197\n",
      "standard deviation: 0.01162235930365602\n",
      "avg train time: 0.9034176349639893\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5205708299934052\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.505118538860272\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5533533116443892\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5213341451489405\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5201909461956317\n",
      "avgfold accuracy: 0.5241135543685277\n",
      "standard deviation: 0.015820473356990626\n",
      "avg train time: 1.0088473796844482\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5255475730856333\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5162966333194311\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5357803494818142\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5093303443758707\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5269544656100793\n",
      "avgfold accuracy: 0.5227818731745657\n",
      "standard deviation: 0.009132600647590293\n",
      "avg train time: 0.9547619342803955\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5257292051321311\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5094849143028406\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5201178558182188\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5038058469174069\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5051043748106282\n",
      "avgfold accuracy: 0.512848439396245\n",
      "standard deviation: 0.008623350360009071\n",
      "avg train time: 1.0442172050476075\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5140072457676731\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5026903473565049\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5129270501309654\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5145056688714841\n",
      "tree depth: 5, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.509209262534186\n",
      "avgfold accuracy: 0.5106679149321627\n",
      "standard deviation: 0.004398918703875923\n",
      "avg train time: 0.9964759349822998\n",
      "tree depth: 5, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5178611725724602\n",
      "tree depth: 5, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5003479162040121\n",
      "tree depth: 5, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5226579105270894\n",
      "tree depth: 5, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5036361036540927\n",
      "tree depth: 5, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5251360011406867\n",
      "avgfold accuracy: 0.5139278208196683\n",
      "standard deviation: 0.010076132988461394\n",
      "avg train time: 0.9752108573913574\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6971435108323751\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6842562049870998\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.7081795986865267\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6714144041919206\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6928106044253097\n",
      "avgfold accuracy: 0.6907608646246463\n",
      "standard deviation: 0.01236889672086658\n",
      "avg train time: 0.9286152362823487\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5828530466649254\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5791528321678953\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.6367226101226582\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.6085226800985248\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5938228648266493\n",
      "avgfold accuracy: 0.6002148067761306\n",
      "standard deviation: 0.020917623549400657\n",
      "avg train time: 0.9212723731994629\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5499459356952505\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5437737082234246\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6017511494460321\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5463025489165029\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5591728239322401\n",
      "avgfold accuracy: 0.5601892332426901\n",
      "standard deviation: 0.021427368360387524\n",
      "avg train time: 0.9848184585571289\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5303278608548415\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5200552838306531\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5639791156288564\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5180029336063986\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5513814982998619\n",
      "avgfold accuracy: 0.5367493384441223\n",
      "standard deviation: 0.018038377952472005\n",
      "avg train time: 0.9102297306060791\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5145208066264174\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.511878801666098\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5472367521407762\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5226265924962408\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.560863753294822\n",
      "avgfold accuracy: 0.5314253412448708\n",
      "standard deviation: 0.019296990948095526\n",
      "avg train time: 0.8969926357269287\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5113653250913415\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5091761770382527\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5326597519425101\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5136454210832203\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5156572117736291\n",
      "avgfold accuracy: 0.5165007773857907\n",
      "standard deviation: 0.00836659741831856\n",
      "avg train time: 0.9438318252563477\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5097074948487551\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5077991366189478\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5247959403067526\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5082482310722424\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5130057093744245\n",
      "avgfold accuracy: 0.5127113024442245\n",
      "standard deviation: 0.006311836292966086\n",
      "avg train time: 0.9665585517883301\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.516919988331517\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5078162886892027\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5212467690221613\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5006983459257004\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5266886024409903\n",
      "avgfold accuracy: 0.5146739988819142\n",
      "standard deviation: 0.009326773710622127\n",
      "avg train time: 0.9811542987823486\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5224201744668374\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5101881491832907\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5154831910682982\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5046757811418925\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5103790604781775\n",
      "avgfold accuracy: 0.5126292712676992\n",
      "standard deviation: 0.005971384671794603\n",
      "avg train time: 0.9341247081756592\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5034526976111633\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5089997815367894\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5276833529245286\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5060858495004567\n",
      "tree depth: 5, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5007407532096665\n",
      "avgfold accuracy: 0.5093924869565208\n",
      "standard deviation: 0.009547538331589348\n",
      "avg train time: 1.0503408432006835\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7160779636795951\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6794464033914156\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.7193593966853475\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6878914934639618\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6895919272653819\n",
      "avgfold accuracy: 0.6984734368971404\n",
      "standard deviation: 0.016118495113990973\n",
      "avg train time: 0.9273111820220947\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5962427184563177\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5661174393222946\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.6137117180458188\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.6023076781151118\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5921213405444798\n",
      "avgfold accuracy: 0.5941001788968046\n",
      "standard deviation: 0.01577032341009775\n",
      "avg train time: 0.953761100769043\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.552556521090833\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5317454127239474\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5985927558089594\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5258037435769702\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5574570410667005\n",
      "avgfold accuracy: 0.5532310948534821\n",
      "standard deviation: 0.025651074404065534\n",
      "avg train time: 0.9314721584320068\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5352501268422143\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5158432770203785\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.556869525777985\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5392946429395106\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.553653662970657\n",
      "avgfold accuracy: 0.5401822471101492\n",
      "standard deviation: 0.01467953290235498\n",
      "avg train time: 0.9863189697265625\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5102755328123546\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5149416197692956\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5732596924103867\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5180993367097483\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5331931899421537\n",
      "avgfold accuracy: 0.5299538743287878\n",
      "standard deviation: 0.0229739940772915\n",
      "avg train time: 0.9079185485839844\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5128794883880581\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.507627615916399\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5326597519425101\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5213977988726833\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5232572347457912\n",
      "avgfold accuracy: 0.5195643779730884\n",
      "standard deviation: 0.008667888784471599\n",
      "avg train time: 0.9798384189605713\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5254980370729521\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5085881318506723\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5421518037762617\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5154277253480198\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5089788477876421\n",
      "avgfold accuracy: 0.5201289091671095\n",
      "standard deviation: 0.012597221602952567\n",
      "avg train time: 0.9741628170013428\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5279533846106119\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.505182272342377\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5158249253363592\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5069866880690781\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5126051818068398\n",
      "avgfold accuracy: 0.5137104904330532\n",
      "standard deviation: 0.008080749949164004\n",
      "avg train time: 0.9491830348968506\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5163470092757435\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5032049094641513\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.517040678017528\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5040392439044641\n",
      "tree depth: 5, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5039417061583218\n",
      "avgfold accuracy: 0.5089147093640418\n",
      "standard deviation: 0.006361968056558397\n",
      "avg train time: 1.0389090538024903\n",
      "tree depth: 5, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5093656838521605\n",
      "tree depth: 5, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5087596525532211\n",
      "tree depth: 5, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5142296422893606\n",
      "tree depth: 5, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5127773318942057\n",
      "tree depth: 5, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5210914153426318\n",
      "avgfold accuracy: 0.5132447451863159\n",
      "standard deviation: 0.004425424336554104\n",
      "avg train time: 0.9806944370269776\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7055845224478196\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6683712213537858\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.7148710773797287\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6749462633419128\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6820628011383103\n",
      "avgfold accuracy: 0.6891671771323115\n",
      "standard deviation: 0.017974415368396803\n",
      "avg train time: 1.0314434051513672\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.611191148464934\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.569204811968173\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6609275450113598\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.6002301681749832\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5633873245154557\n",
      "avgfold accuracy: 0.6009881996269811\n",
      "standard deviation: 0.034989739758970906\n",
      "avg train time: 1.0081764221191407\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5449477395066213\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5334091635386707\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5955316642367307\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5393140158119539\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5666593723450815\n",
      "avgfold accuracy: 0.5559723910878117\n",
      "standard deviation: 0.022747708927345987\n",
      "avg train time: 0.9997776508331299\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5445547037696405\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5206604810884884\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.592645036313026\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5406719619184679\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5525902102943011\n",
      "avgfold accuracy: 0.5502244786767847\n",
      "standard deviation: 0.023674996403683708\n",
      "avg train time: 0.9957656860351562\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5304318739723779\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5042094791367634\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5632956470927346\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5384805210380169\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5421685721018934\n",
      "avgfold accuracy: 0.5357172186683572\n",
      "standard deviation: 0.019133729560204146\n",
      "avg train time: 0.9106778621673584\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5109739780820657\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4989242943728571\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5490225850647609\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5174318951281839\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.523310407379609\n",
      "avgfold accuracy: 0.5199326320054953\n",
      "standard deviation: 0.016646297790226455\n",
      "avg train time: 0.9325770378112793\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5219083023357981\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5059418382325063\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5349770843174705\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.509054511572985\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5165256981259865\n",
      "avgfold accuracy: 0.5176814869169493\n",
      "standard deviation: 0.010301296622996173\n",
      "avg train time: 0.9713403701782226\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5146958088530361\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5040330836353001\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5061648181889993\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5090969473888135\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.509829609928727\n",
      "avgfold accuracy: 0.5087640535989751\n",
      "standard deviation: 0.0036213271179205853\n",
      "avg train time: 1.0190518379211426\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5159837451827479\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.514953897040636\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5142240186485162\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5123741916438344\n",
      "tree depth: 5, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5165611465485317\n",
      "avgfold accuracy: 0.5148193998128533\n",
      "standard deviation: 0.0014656734326995904\n",
      "avg train time: 1.063539171218872\n",
      "tree depth: 5, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5239888148684093\n",
      "tree depth: 5, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5070910269395832\n",
      "tree depth: 5, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5044987818932366\n",
      "tree depth: 5, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5062980285795994\n",
      "tree depth: 5, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.516426482150036\n",
      "avgfold accuracy: 0.5116606268861729\n",
      "standard deviation: 0.007424665962773464\n",
      "avg train time: 1.0202620029449463\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7114413428562565\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6791082367852326\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7129718561624379\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6961221966992315\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6858380581393737\n",
      "avgfold accuracy: 0.6970963381285065\n",
      "standard deviation: 0.013483922598848452\n",
      "avg train time: 1.0422406196594238\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6375808337661479\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.574926200960877\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6452160079683067\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5838882277511785\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5986756350515586\n",
      "avgfold accuracy: 0.6080573810996137\n",
      "standard deviation: 0.0283627856107105\n",
      "avg train time: 0.9558828353881836\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5673943081620338\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5561621971982544\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5934370541956807\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5641800201108866\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5471062997193832\n",
      "avgfold accuracy: 0.5656559758772477\n",
      "standard deviation: 0.015564131047115013\n",
      "avg train time: 1.0090500831604003\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5249383176569365\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5419532416510041\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5770476722573503\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5358707183645606\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5533983947211556\n",
      "avgfold accuracy: 0.5466416689302015\n",
      "standard deviation: 0.017771936105417795\n",
      "avg train time: 0.9057439804077149\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5086292734818213\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5092839642586966\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5468025547546445\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.521883965719241\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5241752300681838\n",
      "avgfold accuracy: 0.5221549976565174\n",
      "standard deviation: 0.013858323011038396\n",
      "avg train time: 0.9446193695068359\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5082214144683185\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5208908604742276\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5445993953670616\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5116200333951421\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5123605084769258\n",
      "avgfold accuracy: 0.5195384424363351\n",
      "standard deviation: 0.013207418037174902\n",
      "avg train time: 0.9206250667572021\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5199648269291776\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5042732126188684\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5260929088243033\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5077814370981282\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5220591176709396\n",
      "avgfold accuracy: 0.5160343006282834\n",
      "standard deviation: 0.008477688409621003\n",
      "avg train time: 1.0215833187103271\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5167548682892463\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5015705879910232\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5147233456425677\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5058524525133995\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.502442277491836\n",
      "avgfold accuracy: 0.5082687063856145\n",
      "standard deviation: 0.006298057768062586\n",
      "avg train time: 0.9164064884185791\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5170190603568795\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5078505928297125\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5112553901943583\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5033814887591214\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.514005394497375\n",
      "avgfold accuracy: 0.5107023853274892\n",
      "standard deviation: 0.004750669729652219\n",
      "avg train time: 0.8935789585113525\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5012945661495908\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5004679806957962\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5114781125282686\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49830256736685763\n",
      "tree depth: 5, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5066003378492115\n",
      "avgfold accuracy: 0.5036287129179449\n",
      "standard deviation: 0.0047801657862963\n",
      "avg train time: 0.9265404224395752\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.705452426414003\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6930232600127467\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7174111320885993\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6764933255841844\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6942285413271176\n",
      "avgfold accuracy: 0.6973217370853302\n",
      "standard deviation: 0.013649610920168965\n",
      "avg train time: 0.8657442092895508\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6038300469441285\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5935652654328009\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6201483019481862\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5861566988625356\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5781903085200977\n",
      "avgfold accuracy: 0.5963781243415498\n",
      "standard deviation: 0.014585875718272742\n",
      "avg train time: 0.9955384731292725\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5563278878744847\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5509970769261323\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5829246385045196\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5480193544220888\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5634761436076988\n",
      "avgfold accuracy: 0.5603490202669847\n",
      "standard deviation: 0.012450593953786098\n",
      "avg train time: 1.3128602027893066\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5422925591905315\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5284375908382668\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5684023053265557\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5320321220675468\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5468795486366219\n",
      "avgfold accuracy: 0.5436088252119046\n",
      "standard deviation: 0.014080719102858678\n",
      "avg train time: 0.8690304279327392\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5222121482317645\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5161985956968164\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5459558698516875\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5119383020138562\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5406939969383653\n",
      "avgfold accuracy: 0.527399782546498\n",
      "standard deviation: 0.013509256302705129\n",
      "avg train time: 0.8955246925354003\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.520537805984951\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5053929719843501\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5380325522488547\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5249605623668115\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5255719771308166\n",
      "avgfold accuracy: 0.5228991739431568\n",
      "standard deviation: 0.01051147471778418\n",
      "avg train time: 0.8856045246124268\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5110565381032011\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5003699430731815\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5353734856058335\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5197755514349763\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.52798246986389\n",
      "avgfold accuracy: 0.5189115976162165\n",
      "standard deviation: 0.012324900071509907\n",
      "avg train time: 0.9443704128265381\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5164081162004778\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.502947628410328\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5229128053179241\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5215887600439119\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5131191839337451\n",
      "avgfold accuracy: 0.5153952987812774\n",
      "standard deviation: 0.007155737034406482\n",
      "avg train time: 1.278403377532959\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5055563519678806\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5100852367617613\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5129921797388852\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5055554018025996\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5140585671311928\n",
      "avgfold accuracy: 0.5096495474804639\n",
      "standard deviation: 0.003586606981011345\n",
      "avg train time: 1.296232032775879\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5127077385259083\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5032735177451708\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5024853876885685\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5091933504921632\n",
      "tree depth: 5, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5113007194643525\n",
      "avgfold accuracy: 0.5077921427832326\n",
      "standard deviation: 0.004171718992109095\n",
      "avg train time: 1.2795971870422362\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6985916261121836\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6658058132879796\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.7067740808224431\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6758143525309274\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6659305012882234\n",
      "avgfold accuracy: 0.6825832748083515\n",
      "standard deviation: 0.017006344679891452\n",
      "avg train time: 1.2647605895996095\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5977502519331555\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5733825146379379\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.6387850477067839\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.6010770394560836\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.6013945686679513\n",
      "avgfold accuracy: 0.6024778844803824\n",
      "standard deviation: 0.020929582400279072\n",
      "avg train time: 1.2832222938537599\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5660452649075878\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5296774146955326\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5908157836504282\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5552740338933017\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5773291494953056\n",
      "avgfold accuracy: 0.5638283293284312\n",
      "standard deviation: 0.020760397780280073\n",
      "avg train time: 1.225003433227539\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5336484624321882\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5254899624279388\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5588451238848844\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5389242520687461\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5282978420030141\n",
      "avgfold accuracy: 0.5370411285633543\n",
      "standard deviation: 0.011835075399503141\n",
      "avg train time: 1.2780983924865723\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5162875910787142\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5207536439121886\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.535215892879843\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5229660790228692\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.549669379098105\n",
      "avgfold accuracy: 0.528978517198344\n",
      "standard deviation: 0.012098546860745348\n",
      "avg train time: 1.2409445762634277\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5230757135437463\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5019013521247804\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.536968768652832\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5141334329652488\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5223710241821613\n",
      "avgfold accuracy: 0.5196900582937538\n",
      "standard deviation: 0.011528384305243996\n",
      "avg train time: 1.320556354522705\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.519618074840409\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5021757852488584\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5241663540968616\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.518577662155555\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5337391748637018\n",
      "avgfold accuracy: 0.5196554102410772\n",
      "standard deviation: 0.010250865964903854\n",
      "avg train time: 1.2920530796051026\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5117995782934194\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5008845051808279\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5266629890671191\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5150245850976485\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5016801364071143\n",
      "avgfold accuracy: 0.5112103588092258\n",
      "standard deviation: 0.009498712290156088\n",
      "avg train time: 1.3196649074554443\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5160332811954291\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.505045055780338\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5124928527448337\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5097122667183277\n",
      "tree depth: 5, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5221194195964425\n",
      "avgfold accuracy: 0.5130805752070742\n",
      "standard deviation: 0.005773727959894203\n",
      "avg train time: 1.3001430034637451\n",
      "tree depth: 5, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5110119431826938\n",
      "tree depth: 5, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5046848623049854\n",
      "tree depth: 5, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5283780687423394\n",
      "tree depth: 5, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5023957785588428\n",
      "tree depth: 5, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5122401026618003\n",
      "avgfold accuracy: 0.5117421510901323\n",
      "standard deviation: 0.009107084275194022\n",
      "avg train time: 1.3543660163879394\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6848106073115154\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6757881376282118\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.727548856360704\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6767903762949843\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6880819036793087\n",
      "avgfold accuracy: 0.6906039762549447\n",
      "standard deviation: 0.01905346560543569\n",
      "avg train time: 1.3926441192626953\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.621128122645163\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.592489559805658\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.6325438526281758\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.586762331755828\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5695659449579075\n",
      "avgfold accuracy: 0.6004979623585464\n",
      "standard deviation: 0.023074376943274095\n",
      "avg train time: 1.2172378540039062\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.549825410573487\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5337962586821071\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5752779255618279\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5517440197787802\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5517074653585736\n",
      "avgfold accuracy: 0.5524702159909551\n",
      "standard deviation: 0.013243960454516642\n",
      "avg train time: 1.3067541122436523\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5429365273553873\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5209520662828214\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5570432047324376\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5290616149595476\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.562147025798134\n",
      "avgfold accuracy: 0.5424280878256657\n",
      "standard deviation: 0.01576438242307912\n",
      "avg train time: 1.258749771118164\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.528422350548849\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5080612924716855\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5439858953856327\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5251399920663474\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5247601290401795\n",
      "avgfold accuracy: 0.5260739319025388\n",
      "standard deviation: 0.01143159662341814\n",
      "avg train time: 1.250439214706421\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5176085514168801\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5083480028671039\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5412834090039983\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5174106772202696\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5299355987317782\n",
      "avgfold accuracy: 0.522917247848006\n",
      "standard deviation: 0.01146595483965697\n",
      "avg train time: 1.3413638114929198\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5163750921920236\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5063020317078587\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5242371073456258\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5094576518233563\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5068164940123852\n",
      "avgfold accuracy: 0.5126376754162498\n",
      "standard deviation: 0.0068213516172060335\n",
      "avg train time: 1.3120689868927002\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5152737290009837\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5013304590074548\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5088946380807847\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5109216874694416\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5113822112290305\n",
      "avgfold accuracy: 0.509560544957539\n",
      "standard deviation: 0.004604115454046553\n",
      "avg train time: 1.3604772567749024\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5159672331785208\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5026045870052305\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.49734577230375193\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5089272041254993\n",
      "tree depth: 5, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.54738315387982\n",
      "avgfold accuracy: 0.5144455900985645\n",
      "standard deviation: 0.01760831742175786\n",
      "avg train time: 1.3574541091918946\n",
      "tree depth: 5, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5101368069586589\n",
      "tree depth: 5, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5029304763400732\n",
      "tree depth: 5, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.525539699504021\n",
      "tree depth: 5, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5023957785588428\n",
      "tree depth: 5, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.509953679407635\n",
      "avgfold accuracy: 0.5101912881538462\n",
      "standard deviation: 0.008356105733317371\n",
      "avg train time: 1.2697077751159669\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7071151977487633\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6796571030333888\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.7092055861847808\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6902872720228046\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6860861970971901\n",
      "avgfold accuracy: 0.6944702712173856\n",
      "standard deviation: 0.011698237533359065\n",
      "avg train time: 1.2884307861328126\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6129364798208298\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6021533972834732\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.6143413042557099\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5909846954307697\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5827099823946102\n",
      "avgfold accuracy: 0.6006251718370785\n",
      "standard deviation: 0.012295762391257722\n",
      "avg train time: 1.1837854385375977\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5544339484805454\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5339677793846559\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6136417494911259\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5589216690190869\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5447206604892674\n",
      "avgfold accuracy: 0.5611371613729362\n",
      "standard deviation: 0.027619739541856723\n",
      "avg train time: 0.9167547225952148\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.543757186474567\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5201459189808421\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5779755729966891\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5374505299864389\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5442528997403749\n",
      "avgfold accuracy: 0.5447164216357824\n",
      "standard deviation: 0.018781953658052544\n",
      "avg train time: 0.9149924755096436\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5232854285065243\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5179358295914738\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5525010031005878\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5196791483316267\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.54109452450595\n",
      "avgfold accuracy: 0.5308991868072325\n",
      "standard deviation: 0.013583008408462416\n",
      "avg train time: 1.2106473922729493\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.522106446386523\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.511222148197498\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5298430925734984\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5183230472605836\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5223710241821613\n",
      "avgfold accuracy: 0.5207731517200529\n",
      "standard deviation: 0.00606332216376416\n",
      "avg train time: 1.1640599250793457\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5229931535226109\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501815591773506\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5225003178010988\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5038482827332356\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5317789167041285\n",
      "avgfold accuracy: 0.516587252506916\n",
      "standard deviation: 0.011724076780665747\n",
      "avg train time: 1.175050687789917\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5230311186232391\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5137998335346445\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5227825461020845\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5112496425243775\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5202406532015471\n",
      "avgfold accuracy: 0.5182207587971787\n",
      "standard deviation: 0.004820175945792868\n",
      "avg train time: 1.0163025856018066\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5181418766443204\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5045819498834562\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5292842596123716\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5041453334440354\n",
      "tree depth: 5, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5265539380424945\n",
      "avgfold accuracy: 0.5165414715253356\n",
      "standard deviation: 0.010600790798487152\n",
      "avg train time: 0.9670120239257812\n",
      "tree depth: 5, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5106321670854711\n",
      "tree depth: 5, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5091541501690833\n",
      "tree depth: 5, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5038257759447324\n",
      "tree depth: 5, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5077505327539922\n",
      "tree depth: 5, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.511814721591258\n",
      "avgfold accuracy: 0.5086354695089075\n",
      "standard deviation: 0.002766938376042812\n",
      "avg train time: 0.9797547340393067\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6922493277612825\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6806936297211074\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.7339363968836137\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6896101440050185\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6860224295437847\n",
      "avgfold accuracy: 0.6965023855829614\n",
      "standard deviation: 0.01911357076709327\n",
      "avg train time: 0.9464766502380371\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.6257201485479944\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5822328023413479\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.6091421829038179\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.592362014409727\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5816004863761216\n",
      "avgfold accuracy: 0.5982115269158017\n",
      "standard deviation: 0.016980380489419956\n",
      "avg train time: 0.9025160312652588\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5682810152981217\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5396131576239147\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5980660953047569\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5621025101707581\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.568034731532659\n",
      "avgfold accuracy: 0.5672195019860421\n",
      "standard deviation: 0.018663320087131462\n",
      "avg train time: 0.9215849876403809\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5431181594018851\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5234096871281837\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5806893066600125\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5364320704065535\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5444761851952337\n",
      "avgfold accuracy: 0.5456250817583738\n",
      "standard deviation: 0.019053292142077356\n",
      "avg train time: 0.9325913429260254\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.527750299467713\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5118273454553334\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5504007694186934\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5194669692524838\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5517785602395442\n",
      "avgfold accuracy: 0.5322447887667536\n",
      "standard deviation: 0.01619600542330314\n",
      "avg train time: 0.929436445236206\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5257622291405852\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5134053359187822\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5422547294819502\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5038270648253212\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.532239746197216\n",
      "avgfold accuracy: 0.523497821112771\n",
      "standard deviation: 0.013580628260720413\n",
      "avg train time: 1.001629400253296\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.540154192097655\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5031019970426219\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5209645407211757\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5082385446360207\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5133247451773313\n",
      "avgfold accuracy: 0.517156803934961\n",
      "standard deviation: 0.012923902984506776\n",
      "avg train time: 0.8866795063018799\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5136935176873587\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5101195409022711\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5264458903740532\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5028298231533501\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.509209262534186\n",
      "avgfold accuracy: 0.5124596069302438\n",
      "standard deviation: 0.007821918481102731\n",
      "avg train time: 1.3152439594268799\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5143705098606687\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5000048747989145\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5126014020913666\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5062980285795994\n",
      "tree depth: 5, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.513377917811149\n",
      "avgfold accuracy: 0.5093305466283395\n",
      "standard deviation: 0.005452435403023983\n",
      "avg train time: 0.9041938304901123\n",
      "tree depth: 5, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5087382276915318\n",
      "tree depth: 5, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5039767526256208\n",
      "tree depth: 5, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.53152037615095\n",
      "tree depth: 5, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5049613003809998\n",
      "tree depth: 5, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5135091165817423\n",
      "avgfold accuracy: 0.512541154686169\n",
      "standard deviation: 0.010063927914182827\n",
      "avg train time: 0.9939542293548584\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7162381426296918\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6905362098257891\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7034090510799221\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7046365741381377\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6586992211248834\n",
      "avgfold accuracy: 0.6947038397596849\n",
      "standard deviation: 0.019757873462311008\n",
      "avg train time: 0.8688377380371094\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6091700541293521\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5831247099946015\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6391267819748448\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5986503565531047\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5619056200602426\n",
      "avgfold accuracy: 0.5983955045424292\n",
      "standard deviation: 0.025845315495346948\n",
      "avg train time: 0.9072331428527832\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5737151534165338\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5290991191057813\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5934748502934495\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5368642699655901\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5825683867403096\n",
      "avgfold accuracy: 0.5631443559043329\n",
      "standard deviation: 0.02552917816721218\n",
      "avg train time: 0.899336576461792\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5385211924068798\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5301356457934999\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5869522116079271\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5419565678650172\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5521719585154439\n",
      "avgfold accuracy: 0.5499475152377535\n",
      "standard deviation: 0.019805160769707154\n",
      "avg train time: 0.8865223884582519\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.523681716607974\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5107810691697856\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5390472924654198\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.528396018413454\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.526461653322349\n",
      "avgfold accuracy: 0.5256735499957965\n",
      "standard deviation: 0.00908452876246444\n",
      "avg train time: 0.9001427173614502\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5150045958411765\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5157649191415299\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5203727506090534\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5192451037371193\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5145051380409102\n",
      "avgfold accuracy: 0.516978501473958\n",
      "standard deviation: 0.002372552431754026\n",
      "avg train time: 1.061099910736084\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5242315538396414\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5044961895321818\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5209428308518691\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5198816409745477\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5078516275578809\n",
      "avgfold accuracy: 0.5154807685512242\n",
      "standard deviation: 0.007805675042536314\n",
      "avg train time: 1.24801664352417\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5200523280424869\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.508056417672771\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.538103305497619\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5191177962896336\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.508588915139645\n",
      "avgfold accuracy: 0.5187837525284311\n",
      "standard deviation: 0.010899324919349527\n",
      "avg train time: 1.0529160976409913\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5128068105512708\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5104797343776236\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5064036267513717\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5052583510917997\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5161428947696743\n",
      "avgfold accuracy: 0.510218283508348\n",
      "standard deviation: 0.004025478435095469\n",
      "avg train time: 0.8780251979827881\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5181418766443204\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.504753470586005\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5110439151421369\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.4995216745541933\n",
      "tree depth: 5, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5051115041023133\n",
      "avgfold accuracy: 0.5077144882057938\n",
      "standard deviation: 0.006363660275185812\n",
      "avg train time: 1.2536272525787353\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7159508087379525\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6809729376440999\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.7015476259604001\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6847415566564268\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6832220041627142\n",
      "avgfold accuracy: 0.6932869866323188\n",
      "standard deviation: 0.013474799016052369\n",
      "avg train time: 0.9882789134979248\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.6108328254641124\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5827130603084845\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.6330648894915339\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5966134373933338\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5929189300517468\n",
      "avgfold accuracy: 0.6032286285418422\n",
      "standard deviation: 0.017429661242355232\n",
      "avg train time: 0.8933300495147705\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.572844958284673\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5510093541974727\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6101633314552989\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5520295390178875\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5627140025229771\n",
      "avgfold accuracy: 0.5697522370956618\n",
      "standard deviation: 0.02171427136280543\n",
      "avg train time: 1.180445146560669\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5401889048338142\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5199915503485482\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.591673715835074\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.538511425382153\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5427002984400714\n",
      "avgfold accuracy: 0.546613178967932\n",
      "standard deviation: 0.023922305225223376\n",
      "avg train time: 0.9472596168518066\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5306300180231028\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5245760279055156\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5429221117896099\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5201035064899122\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5376880103137086\n",
      "avgfold accuracy: 0.5311839349043699\n",
      "standard deviation: 0.00832868472430466\n",
      "avg train time: 1.0000078678131104\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5132427524810538\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5144393349329895\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5371207377379781\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5140273434256774\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5155863149285387\n",
      "avgfold accuracy: 0.5188832967012476\n",
      "standard deviation: 0.00915007280725152\n",
      "avg train time: 1.023073101043701\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5185662476620503\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.503530798798994\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5235206816585085\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.519372411184605\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5112510124584372\n",
      "avgfold accuracy: 0.515248230352519\n",
      "standard deviation: 0.007067213172338496\n",
      "avg train time: 1.034881019592285\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5304087321482718\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5100509326212516\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5056220714563346\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5072413029640495\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5142818525860515\n",
      "avgfold accuracy: 0.5135209783551917\n",
      "standard deviation: 0.008940284429272284\n",
      "avg train time: 0.9653846740722656\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5212692752631163\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5130794465839396\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5248884034248235\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5038579691694572\n",
      "tree depth: 5, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.4986032529373672\n",
      "avgfold accuracy: 0.5123396694757407\n",
      "standard deviation: 0.009983829253487052\n",
      "avg train time: 1.0055252075195313\n",
      "tree depth: 5, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5122288904033232\n",
      "tree depth: 5, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5104625823073687\n",
      "tree depth: 5, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5165252647950144\n",
      "tree depth: 5, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5004764804103359\n",
      "tree depth: 5, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5144662239904626\n",
      "avgfold accuracy: 0.510831888381301\n",
      "standard deviation: 0.005566783742506136\n",
      "avg train time: 0.9780510902404785\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7099998573963271\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6854765296487075\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7111482271406846\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6741612007490845\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6844449747405235\n",
      "avgfold accuracy: 0.6930461579350655\n",
      "standard deviation: 0.014852838567787938\n",
      "avg train time: 0.9463823795318603\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6146916933610735\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5690504433358791\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6509795859221699\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.587854131495678\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5920575729910745\n",
      "avgfold accuracy: 0.602926685421175\n",
      "standard deviation: 0.02807281932069219\n",
      "avg train time: 0.8657826900482177\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5553421087130344\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5279058766603655\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5701656436871623\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5697372669489571\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5711399341332662\n",
      "avgfold accuracy: 0.5588581660285572\n",
      "standard deviation: 0.0165376092092676\n",
      "avg train time: 0.9295557498931885\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5401723928295871\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5217753656550557\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5830934785121991\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5447573317097021\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5457346041955882\n",
      "avgfold accuracy: 0.5471066345804264\n",
      "standard deviation: 0.01996101962101942\n",
      "avg train time: 0.8964803218841553\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5304979219892862\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5135474272797358\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5374134286265814\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5385653926696741\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5202369895377644\n",
      "avgfold accuracy: 0.5280522320206084\n",
      "standard deviation: 0.00975302336496867\n",
      "avg train time: 0.938992977142334\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5109079300651573\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5155419422282164\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5369904785221385\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.513211376488713\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5232926831683364\n",
      "avgfold accuracy: 0.5199888820945123\n",
      "standard deviation: 0.009467237493522752\n",
      "avg train time: 0.8969989776611328\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.514200448726224\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5061305110053099\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5229128053179241\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5181533039972693\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.521509667121489\n",
      "avgfold accuracy: 0.5165813472336434\n",
      "standard deviation: 0.006028853987794476\n",
      "avg train time: 1.0070926189422607\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5120967943695066\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5130451424434298\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5231950336189096\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5167644535466195\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5124456639053864\n",
      "avgfold accuracy: 0.5155094175767704\n",
      "standard deviation: 0.00418924921386346\n",
      "avg train time: 0.9886497497558594\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5124435464582752\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5094849143028406\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.521420447976614\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5052583510917997\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5079579728255165\n",
      "avgfold accuracy: 0.5113130465310092\n",
      "standard deviation: 0.005562293015343658\n",
      "avg train time: 0.9140125751495362\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5012780541453637\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5069881145180539\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5286168773047119\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5030323157962712\n",
      "tree depth: 5, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5062458536237596\n",
      "avgfold accuracy: 0.509232243077632\n",
      "standard deviation: 0.009913898314428798\n",
      "avg train time: 0.9813189506530762\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7089975662306496\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6852289981927134\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.7183116993177869\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6884971263572542\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6828675199372622\n",
      "avgfold accuracy: 0.6967805820071332\n",
      "standard deviation: 0.014201913522113413\n",
      "avg train time: 0.9824952125549317\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.61570061434663\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.57854763491006\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.6546155966839875\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5860736722663493\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5870841989151594\n",
      "avgfold accuracy: 0.6044043434244373\n",
      "standard deviation: 0.028118840576268317\n",
      "avg train time: 0.9788012027740478\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5746959914858102\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5553634523687009\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5892687592888161\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5423578630799177\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5618171970397596\n",
      "avgfold accuracy: 0.5647006526526008\n",
      "standard deviation: 0.01611332698967652\n",
      "avg train time: 0.9476625442504882\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5316934161135145\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5304615351283426\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5568421922678338\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5259328960599268\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5439515881487408\n",
      "avgfold accuracy: 0.5377763255436717\n",
      "standard deviation: 0.01124647420374227\n",
      "avg train time: 0.9288890838623047\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5268091152267699\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5154047256661773\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5413646248403802\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5204102436369339\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5311194572232597\n",
      "avgfold accuracy: 0.5270216333187041\n",
      "standard deviation: 0.008957921046923452\n",
      "avg train time: 0.9276569843292236\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5231417615606546\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5246226093173657\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5319216163860863\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5264130665412042\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5226191631399777\n",
      "avgfold accuracy: 0.5257436433890577\n",
      "standard deviation: 0.0033587009683627212\n",
      "avg train time: 0.9694067955017089\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.512867917476005\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5114794292513212\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5400515700943673\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5094576518233563\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5177238152008381\n",
      "avgfold accuracy: 0.5183160767691776\n",
      "standard deviation: 0.011204252097687224\n",
      "avg train time: 0.9628814697265625\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5195734799199018\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5115994937431053\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5403555082646595\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5123105379200915\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5229807766571147\n",
      "avgfold accuracy: 0.5213639593009745\n",
      "standard deviation: 0.010428883671302989\n",
      "avg train time: 0.9921919345855713\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5142879498395333\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5050622078505929\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.516438425317788\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5230740135979114\n",
      "tree depth: 5, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5178550139714313\n",
      "avgfold accuracy: 0.5153435221154513\n",
      "standard deviation: 0.005900619607600502\n",
      "avg train time: 0.9810948371887207\n",
      "tree depth: 5, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5089363717422566\n",
      "tree depth: 5, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5017078045530622\n",
      "tree depth: 5, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5106097177560052\n",
      "tree depth: 5, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5038695006411498\n",
      "tree depth: 5, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5117792731687127\n",
      "avgfold accuracy: 0.5073805335722373\n",
      "standard deviation: 0.003916743027306208\n",
      "avg train time: 0.9945533275604248\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7243422843407157\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6840626574153816\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7171393663752312\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6977425991014677\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.7007545167033363\n",
      "avgfold accuracy: 0.7048082847872266\n",
      "standard deviation: 0.014355282767156393\n",
      "avg train time: 0.9185376644134522\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.6164981316417035\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.574004863966028\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.6228347021013584\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5958611241801124\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5832628985719632\n",
      "avgfold accuracy: 0.598492344092233\n",
      "standard deviation: 0.01873635391816235\n",
      "avg train time: 0.9303016662597656\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5719202860479569\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5496028844365726\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5859149768279841\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5351995867120546\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5540150784519139\n",
      "avgfold accuracy: 0.5593305624952964\n",
      "standard deviation: 0.017722405637076808\n",
      "avg train time: 0.8770344734191895\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5347877907238562\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5297068440371279\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5736609326455229\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5396765652819675\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5413709825946265\n",
      "avgfold accuracy: 0.5438406230566202\n",
      "standard deviation: 0.015455787574608693\n",
      "avg train time: 0.8798810005187988\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.531582773176099\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5212216246079849\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5329363566026513\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5266137141486545\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5192550286260864\n",
      "avgfold accuracy: 0.5263218994322953\n",
      "standard deviation: 0.005430772657928931\n",
      "avg train time: 0.8720146179199219\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.536080668145742\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5171762637013445\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5619986785751839\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5248756907351544\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5206057323465866\n",
      "avgfold accuracy: 0.5321474067008023\n",
      "standard deviation: 0.01622832216100315\n",
      "avg train time: 0.9456663608551026\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5199813389334046\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5202342070056277\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5180176221363244\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5096698309024991\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5211729071073097\n",
      "avgfold accuracy: 0.5178151812170331\n",
      "standard deviation: 0.004200141955805743\n",
      "avg train time: 0.9285408020019531\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.515554433072844\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5085587025090771\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5275908898064579\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5072200850561353\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5122684217926604\n",
      "avgfold accuracy: 0.514238506447435\n",
      "standard deviation: 0.007285841741360965\n",
      "avg train time: 0.9761858940124511\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5064314881919155\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5085709797804174\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5138171547725354\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5073261745957066\n",
      "tree depth: 5, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5077275580789727\n",
      "avgfold accuracy: 0.5087746710839095\n",
      "standard deviation: 0.002613550293526177\n",
      "avg train time: 0.9279082775115967\n",
      "tree depth: 5, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5108633351446502\n",
      "tree depth: 5, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5031019970426219\n",
      "tree depth: 5, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5228315894815422\n",
      "tree depth: 5, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.506191939040028\n",
      "tree depth: 5, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5102018183654514\n",
      "avgfold accuracy: 0.5106381358148587\n",
      "standard deviation: 0.006715406962072779\n",
      "avg train time: 0.9135650157928467\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6937684321501731\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6850672270879937\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7088469809941864\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.701561822526038\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.7047707833705311\n",
      "avgfold accuracy: 0.6988030492257845\n",
      "standard deviation: 0.008463160464175784\n",
      "avg train time: 0.8765062808990478\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6299274572614291\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5899878852219568\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6534545110231207\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5914399579331913\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.579863513671407\n",
      "avgfold accuracy: 0.6089346650222209\n",
      "standard deviation: 0.028046001838889487\n",
      "avg train time: 0.8961985111236572\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5591630115093673\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5355506446470194\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.6036616179450118\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5286257253295694\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.56232406987498\n",
      "avgfold accuracy: 0.5578650138611895\n",
      "standard deviation: 0.026350574913275804\n",
      "avg train time: 0.8732316493988037\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5342263825801358\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5215229594001469\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5856874155473005\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5375026522384893\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5416794234779457\n",
      "avgfold accuracy: 0.5441237666488037\n",
      "standard deviation: 0.021845391179938018\n",
      "avg train time: 0.940890884399414\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5249267467448835\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5010780527525461\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5670402072010852\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5230721685624407\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5411299729284952\n",
      "avgfold accuracy: 0.5314494296378901\n",
      "standard deviation: 0.02188863081654344\n",
      "avg train time: 0.9354416370391846\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.525493095980778\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5160393522656079\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5312325242091199\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5241118460502403\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5206589049804043\n",
      "avgfold accuracy: 0.52350714469723\n",
      "standard deviation: 0.005057104865750402\n",
      "avg train time: 0.9056025981903076\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5191062027093697\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.504599101953711\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5161505733759579\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5124475318037989\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.510744139623217\n",
      "avgfold accuracy: 0.5126095098932109\n",
      "standard deviation: 0.004948103205051463\n",
      "avg train time: 0.9119581699371337\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5208894991658937\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5109648671436748\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.516563060892783\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5100729711528704\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5152035115722267\n",
      "avgfold accuracy: 0.5147387819854897\n",
      "standard deviation: 0.003934145724620745\n",
      "avg train time: 0.962421464920044\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5141888778141709\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5148509846191067\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5279872910948209\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5097759204420705\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5190248119154228\n",
      "avgfold accuracy: 0.5171655771771184\n",
      "standard deviation: 0.006154532482610312\n",
      "avg train time: 0.9836572647094727\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.508573107649261\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5121140558507518\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5166555240108539\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.500327955054936\n",
      "tree depth: 5, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5130305628773821\n",
      "avgfold accuracy: 0.510140241088637\n",
      "standard deviation: 0.005539597110587423\n",
      "avg train time: 1.0126768112182618\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7008917357919207\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.680590717299578\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.7155818794260015\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6933541822341535\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.687334021177957\n",
      "avgfold accuracy: 0.6955505071859222\n",
      "standard deviation: 0.01204918583057437\n",
      "avg train time: 0.9082295417785644\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.6080472378419111\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5940160940583423\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.633477377008359\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5902517550899916\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5959071924651309\n",
      "avgfold accuracy: 0.604339931292747\n",
      "standard deviation: 0.015741056335436075\n",
      "avg train time: 0.8952004432678222\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5611560229286692\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5458368314529247\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.582902928635213\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.573691639221764\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5650358741996875\n",
      "avgfold accuracy: 0.5657246592876517\n",
      "standard deviation: 0.012453419338556556\n",
      "avg train time: 0.9255133152008057\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5292925456807098\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5194378093014774\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.552148021550838\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5344491185343038\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5292938634621821\n",
      "avgfold accuracy: 0.5329242717059022\n",
      "standard deviation: 0.010771552182251617\n",
      "avg train time: 0.8764863014221191\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.535634844031611\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5101930239822052\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5297940491940407\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5254679471212834\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5237215298667812\n",
      "avgfold accuracy: 0.5249622788391843\n",
      "standard deviation: 0.008451931140314707\n",
      "avg train time: 0.8980435371398926\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5132097284725996\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5195824283359423\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5335715664533868\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5126597108829417\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5222469547032531\n",
      "avgfold accuracy: 0.5202540777696246\n",
      "standard deviation: 0.007603623369217204\n",
      "avg train time: 0.9249059677124023\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.516011828099028\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.509124720827488\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5066585215422064\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5066472015424497\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5111446671908016\n",
      "avgfold accuracy: 0.5099173878403948\n",
      "standard deviation: 0.00348112239474774\n",
      "avg train time: 0.9369380474090576\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5153067530094378\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5146108556355384\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5204869235964308\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5058524525133995\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.517252390788163\n",
      "avgfold accuracy: 0.5147018751085939\n",
      "standard deviation: 0.004871516361998458\n",
      "avg train time: 0.9616388320922852\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5045540608022032\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5122684244830457\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5153963515910719\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5041028976282069\n",
      "tree depth: 5, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5241682988123788\n",
      "avgfold accuracy: 0.5120980066633812\n",
      "standard deviation: 0.0074489322952775595\n",
      "avg train time: 0.9673452377319336\n",
      "tree depth: 5, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5126086665005459\n",
      "tree depth: 5, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5101538450427808\n",
      "tree depth: 5, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5222454230102643\n",
      "tree depth: 5, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5039543722728069\n",
      "tree depth: 5, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5078693517691535\n",
      "avgfold accuracy: 0.5113663317191103\n",
      "standard deviation: 0.006140588873630125\n",
      "avg train time: 0.9996173858642579\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.699841597341267\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6785275940700779\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.7069855558746645\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6838753125028829\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6901024637643849\n",
      "avgfold accuracy: 0.6918665047106554\n",
      "standard deviation: 0.01036422057856504\n",
      "avg train time: 0.9710119247436524\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6555790434645987\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5698688679091988\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6377646838493742\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5973772820782479\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5995795698264612\n",
      "avgfold accuracy: 0.6120338894255761\n",
      "standard deviation: 0.030679209497798943\n",
      "avg train time: 0.9494187831878662\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5640241705720058\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5399390469587574\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5982671077693607\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5720924547274422\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5607820634942638\n",
      "avgfold accuracy: 0.5670209687043659\n",
      "standard deviation: 0.018885848668530984\n",
      "avg train time: 0.9122347831726074\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5336864275328164\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5341638546298854\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5641962143219224\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5299103312761188\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5291663283553714\n",
      "avgfold accuracy: 0.5382246312232228\n",
      "standard deviation: 0.013136123951801094\n",
      "avg train time: 0.9362142086029053\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5319790612775488\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5253650231372401\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5284046175584192\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5191159512541629\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5196592198574538\n",
      "avgfold accuracy: 0.5249047746169649\n",
      "standard deviation: 0.004970564992676994\n",
      "avg train time: 0.9354821681976319\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5163701510998496\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5147480721975775\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5216970526367551\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5109950276294062\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5173870551866587\n",
      "avgfold accuracy: 0.5162394717500494\n",
      "standard deviation: 0.003488486225601094\n",
      "avg train time: 0.9402395248413086\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5220734223780689\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5095878267243699\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5215450835516091\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5232110074816189\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5142215506605488\n",
      "avgfold accuracy: 0.5181277781592432\n",
      "standard deviation: 0.005315537387163355\n",
      "avg train time: 1.0095946311950683\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5204321041397095\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5155027632887921\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5220661204149671\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5036361036540927\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5251005527181415\n",
      "avgfold accuracy: 0.5173475288431406\n",
      "standard deviation: 0.007527223007904268\n",
      "avg train time: 1.0021789073944092\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.515356289022119\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5034278863774647\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5137303152953091\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5138054779103128\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5083833538960588\n",
      "avgfold accuracy: 0.510940664500253\n",
      "standard deviation: 0.004437763400621779\n",
      "avg train time: 0.9988917827606201\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5085896196534881\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5102224533238005\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5137520251646157\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.504866742313121\n",
      "tree depth: 5, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5112120984079895\n",
      "avgfold accuracy: 0.509728587772603\n",
      "standard deviation: 0.0029518653452912194\n",
      "avg train time: 1.0145976066589355\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6912965726082863\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6910899508728598\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.7108442889703924\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6861013477984115\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.695281399083886\n",
      "avgfold accuracy: 0.6949227118667672\n",
      "standard deviation: 0.008476799876972432\n",
      "avg train time: 0.8913254737854004\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.600141240181612\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5895933876060946\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6305955880314276\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6197086688991597\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6099622939684212\n",
      "avgfold accuracy: 0.610000235737343\n",
      "standard deviation: 0.014367065769825833\n",
      "avg train time: 0.9194639205932618\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5626751273175599\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.544305422401326\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5994121072017652\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5563330842535448\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5644970185698245\n",
      "avgfold accuracy: 0.565444551948804\n",
      "standard deviation: 0.01839697792433423\n",
      "avg train time: 0.9048136711120606\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5574770983505009\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5402503118968565\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5413155814609225\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.527927379403869\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5335937175097384\n",
      "avgfold accuracy: 0.5401128177243775\n",
      "standard deviation: 0.0099394905461759\n",
      "avg train time: 0.9242095470428466\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5243372556848829\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5113422126892821\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.567132670319156\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5285542302050756\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5251926394024069\n",
      "avgfold accuracy: 0.5313118016601607\n",
      "standard deviation: 0.018844690253642864\n",
      "avg train time: 0.9060515880584716\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5144481287896301\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.508776804623476\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5193797202617948\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5164983071799556\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5177698585429709\n",
      "avgfold accuracy: 0.5153745638795655\n",
      "standard deviation: 0.003672335016117148\n",
      "avg train time: 0.9156132221221924\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5230311186232391\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5103768219560944\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5392105088322549\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5095637413629277\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5135374357126025\n",
      "avgfold accuracy: 0.5191439252974236\n",
      "standard deviation: 0.011116229553168228\n",
      "avg train time: 0.939629077911377\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5198211599833079\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5114108209703017\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.505708910933561\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5018653308609858\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5222718082062108\n",
      "avgfold accuracy: 0.5122156061908734\n",
      "standard deviation: 0.007862373805539297\n",
      "avg train time: 0.9329792022705078\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5214013712969329\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5035651029395037\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5193202142947195\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5096273950866705\n",
      "tree depth: 5, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5075503159662468\n",
      "avgfold accuracy: 0.5122928799168147\n",
      "standard deviation: 0.006900997853226797\n",
      "avg train time: 0.9683901786804199\n",
      "tree depth: 5, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5067782402806841\n",
      "tree depth: 5, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5035994070800135\n",
      "tree depth: 5, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5169594621811461\n",
      "tree depth: 5, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5067648225537136\n",
      "tree depth: 5, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5103436120556323\n",
      "avgfold accuracy: 0.5088891088302379\n",
      "standard deviation: 0.004564985994844884\n",
      "avg train time: 0.9845572948455811\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6576826978213162\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6217235483480751\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6815881475622432\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.627605881973081\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6195649547785067\n",
      "avgfold accuracy: 0.6416330460966444\n",
      "standard deviation: 0.024224580665038922\n",
      "avg train time: 1.043623447418213\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5519009820139241\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5484316688603261\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5762765795499308\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5447767045821457\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5481308383452914\n",
      "avgfold accuracy: 0.5539033546703236\n",
      "standard deviation: 0.011411640722759384\n",
      "avg train time: 1.052243947982788\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5193654536848289\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5104846091765382\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5473075053895404\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.517430050092713\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5276668996888857\n",
      "avgfold accuracy: 0.5244509036065013\n",
      "standard deviation: 0.012672635408175127\n",
      "avg train time: 1.0433416843414307\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5171016203780149\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5038395360635818\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5192550846867998\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5181629904334911\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5125520091730219\n",
      "avgfold accuracy: 0.5141822481469819\n",
      "standard deviation: 0.005652116175150152\n",
      "avg train time: 1.0338656902313232\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5144695818860312\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5008624783116585\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5172634003514384\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5131168184208341\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5008719519802598\n",
      "avgfold accuracy: 0.5093168461900446\n",
      "standard deviation: 0.007027528322034335\n",
      "avg train time: 1.140053939819336\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5111935752291916\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5025311039252964\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.511825470437174\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5021835994797\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.518234351699841\n",
      "avgfold accuracy: 0.5091936201542406\n",
      "standard deviation: 0.0061023076856488246\n",
      "avg train time: 1.0734667778015137\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5064199172798625\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5008453262414037\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5103114032265574\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5047394348656353\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5158841608922704\n",
      "avgfold accuracy: 0.5076400485011459\n",
      "standard deviation: 0.005121953834879193\n",
      "avg train time: 1.1211041450500487\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5061887492206835\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5028912974006489\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5035274614152847\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5011766713715071\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5087981400470137\n",
      "avgfold accuracy: 0.5045164638910276\n",
      "standard deviation: 0.002679656508601164\n",
      "avg train time: 1.101293182373047\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5100096520170164\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5095143436444358\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5042004673637889\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5007523132132216\n",
      "tree depth: 6, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.503069754178062\n",
      "avgfold accuracy: 0.505509306083305\n",
      "standard deviation: 0.003649298366734235\n",
      "avg train time: 1.0941835403442384\n",
      "tree depth: 6, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5059410691572773\n",
      "tree depth: 6, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5454468475397612\n",
      "tree depth: 6, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.4980244018931006\n",
      "tree depth: 6, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5030747516120997\n",
      "tree depth: 6, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5006486665254011\n",
      "avgfold accuracy: 0.5106271473455279\n",
      "standard deviation: 0.017605730908617865\n",
      "avg train time: 1.0930710315704346\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6435731276638116\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6235734442620908\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6599827733494875\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6247645273480382\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6104832273511315\n",
      "avgfold accuracy: 0.632475419994912\n",
      "standard deviation: 0.017333050387732293\n",
      "avg train time: 1.0742563247680663\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5770621491824557\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5514601828230141\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5890846177467457\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5464086384560743\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5584284070587909\n",
      "avgfold accuracy: 0.5644887990534161\n",
      "standard deviation: 0.016102228602872595\n",
      "avg train time: 1.0274226188659668\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5266109711760449\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5142849663006956\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5526368859572718\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5202617182815339\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5226580771904253\n",
      "avgfold accuracy: 0.5272905237811943\n",
      "standard deviation: 0.013288401415151445\n",
      "avg train time: 1.0631374835968017\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5120472583568254\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5014676755694939\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5202046952954451\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5157344624950415\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.518875888933557\n",
      "avgfold accuracy: 0.5136659961300725\n",
      "standard deviation: 0.006717074651650424\n",
      "avg train time: 1.0501339912414551\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5092335878183439\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.505902659293082\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5175022089138108\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5110914307327559\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5128427258450686\n",
      "avgfold accuracy: 0.5113145225206123\n",
      "standard deviation: 0.003854638532362331\n",
      "avg train time: 1.0955111026763915\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5141558538057168\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5010168469439524\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5069302872555743\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5072103986199135\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5099076360655024\n",
      "avgfold accuracy: 0.507844204538132\n",
      "standard deviation: 0.0042882265006708295\n",
      "avg train time: 1.1119454383850098\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5058254851276879\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49945113375184386\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5125418961242914\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.504951613944778\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502272164670795\n",
      "avgfold accuracy: 0.5050084587238792\n",
      "standard deviation: 0.0043770905526185205\n",
      "avg train time: 1.1470430374145508\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5081487366315312\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5045084668035221\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5139530376292195\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.50138885045065\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.4997447317504986\n",
      "avgfold accuracy: 0.5055487646530843\n",
      "standard deviation: 0.005088527532947452\n",
      "avg train time: 1.1813412189483643\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5063868932714083\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5046799875060709\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5043090167103218\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5032657127833282\n",
      "tree depth: 6, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5031229268118798\n",
      "avgfold accuracy: 0.5043529074166019\n",
      "standard deviation: 0.0011782063227795934\n",
      "avg train time: 1.192401075363159\n",
      "tree depth: 6, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5043443458394253\n",
      "tree depth: 6, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5787458767325848\n",
      "tree depth: 6, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5071908056872534\n",
      "tree depth: 6, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5072103986199135\n",
      "tree depth: 6, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5033178931358784\n",
      "avgfold accuracy: 0.520161864003011\n",
      "standard deviation: 0.029332536856969602\n",
      "avg train time: 1.1141133785247803\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6529602646123731\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6287728686747227\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6807358390184419\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.639546029022408\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6360694630653182\n",
      "avgfold accuracy: 0.6476168928786528\n",
      "standard deviation: 0.018324821645916256\n",
      "avg train time: 1.1107869625091553\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5437687573866201\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5522320259844837\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5890789941059013\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5400893919685605\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5511757390203957\n",
      "avgfold accuracy: 0.5552689816931922\n",
      "standard deviation: 0.017502969909202346\n",
      "avg train time: 1.15310697555542\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5223491853577551\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5124276679142541\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5318999065167797\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5205915183719407\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5190920450967306\n",
      "avgfold accuracy: 0.5212720646514921\n",
      "standard deviation: 0.0062857011362749265\n",
      "avg train time: 1.1318572521209718\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5082428675647196\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5036337112205233\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5295013583054374\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.51374182418657\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.518787267877194\n",
      "avgfold accuracy: 0.5147814058308888\n",
      "standard deviation: 0.008953527119145118\n",
      "avg train time: 1.1318853378295899\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5048512768782906\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.4992281568385304\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5288556858670843\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5105822009428131\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.503895662816189\n",
      "avgfold accuracy: 0.5094825966685815\n",
      "standard deviation: 0.010337719320547863\n",
      "avg train time: 1.1527073860168457\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5114082312841436\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5028912974006489\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5211004235778597\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5111975202723272\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5081423442299275\n",
      "avgfold accuracy: 0.5109479633529814\n",
      "standard deviation: 0.0059351148055164895\n",
      "avg train time: 1.0835668563842773\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5079836165892604\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5012912800680305\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5105502117889299\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5069982195407706\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5141011448454231\n",
      "avgfold accuracy: 0.5081848945664829\n",
      "standard deviation: 0.004232431315920017\n",
      "avg train time: 1.1229544639587403\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.506304333250273\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5062971569089442\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5012752586482442\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5072740523436563\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.502981133121699\n",
      "avgfold accuracy: 0.5048263868545634\n",
      "standard deviation: 0.0022958235700644147\n",
      "avg train time: 1.105825138092041\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5064694532925437\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5065887421032771\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5066046392159755\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.4988330150647146\n",
      "tree depth: 6, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5029456846991538\n",
      "avgfold accuracy: 0.5042883068751329\n",
      "standard deviation: 0.003065212569808231\n",
      "avg train time: 1.0700984001159668\n",
      "tree depth: 6, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5104389641269202\n",
      "tree depth: 6, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.572762331887143\n",
      "tree depth: 6, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5131280625955692\n",
      "tree depth: 6, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.500179429699536\n",
      "tree depth: 6, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5142606627468765\n",
      "avgfold accuracy: 0.5221538902112091\n",
      "standard deviation: 0.025787979295620565\n",
      "avg train time: 1.1467788219451904\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.648158523746764\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6284886859528156\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6811917462738801\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6349435880404801\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6064563657643491\n",
      "avgfold accuracy: 0.6398477819556578\n",
      "standard deviation: 0.02468383547946113\n",
      "avg train time: 1.0768936157226563\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5378722831498499\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5470031722302566\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5655743986758549\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5461867729407098\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5440968474668241\n",
      "avgfold accuracy: 0.5481466948926991\n",
      "standard deviation: 0.009282656927036524\n",
      "avg train time: 1.086675262451172\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5158087429561291\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5107761943708711\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5597191422979922\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5157132445871272\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.518801328424684\n",
      "avgfold accuracy: 0.5241637305273608\n",
      "standard deviation: 0.017963214171037387\n",
      "avg train time: 1.0572442531585693\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5103349510093839\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.509845107778193\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5237812000901875\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5151731104530485\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.526564532962082\n",
      "avgfold accuracy: 0.517139780458579\n",
      "standard deviation: 0.006874884126043984\n",
      "avg train time: 1.053384494781494\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5089363717422566\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5043540981712282\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5038040660754258\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5066587330141422\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5111766499854443\n",
      "avgfold accuracy: 0.5069859837976994\n",
      "standard deviation: 0.002775759775227876\n",
      "avg train time: 1.0937312126159668\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5043278338351982\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5079314783820723\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5095515578008268\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5050152676685209\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5057035323659941\n",
      "avgfold accuracy: 0.5065059340105225\n",
      "standard deviation: 0.0019451506853398915\n",
      "avg train time: 1.137690830230713\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5023513344201235\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5009996948736976\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5125418961242914\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5111550844564987\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5031052026006072\n",
      "avgfold accuracy: 0.5060306424950436\n",
      "standard deviation: 0.004817930462253971\n",
      "avg train time: 1.1551832199096679\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5047406339408751\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5010854552249719\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5079072313743707\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5055669332742921\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.511467366657491\n",
      "avgfold accuracy: 0.5061535240944002\n",
      "standard deviation: 0.003444940680202441\n",
      "avg train time: 1.0888879299163818\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.509927091995881\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.49929676511954996\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5015357770799231\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5032020590595854\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5055794628870859\n",
      "avgfold accuracy: 0.5039082312284051\n",
      "standard deviation: 0.0036445174629029407\n",
      "avg train time: 1.1378495693206787\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5044434178647877\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5063657651899637\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5068217379090414\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.500327955054936\n",
      "tree depth: 6, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5084968284553796\n",
      "avgfold accuracy: 0.5052911408948216\n",
      "standard deviation: 0.002797349650953742\n",
      "avg train time: 1.1877322673797608\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6383206215918973\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6417273759679636\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6822338200005964\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6468528307456711\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6381786442067574\n",
      "avgfold accuracy: 0.6494626585025772\n",
      "standard deviation: 0.016685735152443783\n",
      "avg train time: 1.1239601612091064\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5415660310045403\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5330489700633182\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5759782650204831\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5429731824094318\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5405768587152621\n",
      "avgfold accuracy: 0.5468286614426071\n",
      "standard deviation: 0.014974807633491197\n",
      "avg train time: 1.0662256717681884\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5289342226798883\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5175879133874617\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5316828078237138\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5132634987407633\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5447563069476928\n",
      "avgfold accuracy: 0.527244949915904\n",
      "standard deviation: 0.011116970647171313\n",
      "avg train time: 1.0699774742126464\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5073842433449117\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5051136640613575\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5470526105987059\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5163188774804196\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5142109557409612\n",
      "avgfold accuracy: 0.5180160702452712\n",
      "standard deviation: 0.01509958849732349\n",
      "avg train time: 1.0640432357788085\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5092831238310251\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5079143263118174\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.511716921090641\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5084931595309921\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5139699460748298\n",
      "avgfold accuracy: 0.5102754953678611\n",
      "standard deviation: 0.002256692967784556\n",
      "avg train time: 1.1132771968841553\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5061887492206835\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5024967997847867\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5153585554933031\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5098086698216774\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5096240486851408\n",
      "avgfold accuracy: 0.5086953646011183\n",
      "standard deviation: 0.004270956378972566\n",
      "avg train time: 1.1547247886657714\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5102573320804225\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995540461733731\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5009061908700322\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5028837904408713\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5058098776336296\n",
      "avgfold accuracy: 0.5038822474396657\n",
      "standard deviation: 0.0038195449369559996\n",
      "avg train time: 1.15883469581604\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5062382852333647\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5062285486279247\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5010364500858717\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5026503934538141\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5063238797605351\n",
      "avgfold accuracy: 0.504495511432302\n",
      "standard deviation: 0.002225002244478645\n",
      "avg train time: 1.106832456588745\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5118210313898204\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5011883676465012\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5099206255790388\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.50124032509525\n",
      "tree depth: 6, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5114319182349458\n",
      "avgfold accuracy: 0.5071204535891113\n",
      "standard deviation: 0.00486396293079639\n",
      "avg train time: 1.1212083339691161\n",
      "tree depth: 6, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5118705674025016\n",
      "tree depth: 6, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5043369461009733\n",
      "tree depth: 6, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.49808953150102037\n",
      "tree depth: 6, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5029474441646141\n",
      "tree depth: 6, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.503690101572603\n",
      "avgfold accuracy: 0.5041869181483425\n",
      "standard deviation: 0.004427690608900341\n",
      "avg train time: 1.1853270053863525\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6385616718354243\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6196530226461492\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6693172324572485\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6417508464100222\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6308267601924116\n",
      "avgfold accuracy: 0.6400219067082512\n",
      "standard deviation: 0.01650926882995825\n",
      "avg train time: 1.116970157623291\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.549169871496578\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5533640626213058\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5651619111590298\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5524963329920018\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5501725882695426\n",
      "avgfold accuracy: 0.5540729533076916\n",
      "standard deviation: 0.0057481527907864965\n",
      "avg train time: 1.0941619873046875\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5285379345784386\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.517690825808991\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5399591069762965\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5333900681740606\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.523214657031561\n",
      "avgfold accuracy: 0.5285585185138695\n",
      "standard deviation: 0.007745725445388372\n",
      "avg train time: 1.0443795204162598\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5166937613645121\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5044398585225026\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5278353220096748\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5093091264679563\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5188581647222844\n",
      "avgfold accuracy: 0.5154272466173861\n",
      "standard deviation: 0.00806832925577993\n",
      "avg train time: 1.1508641719818116\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5128233225554979\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5007424138198744\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5177193076068767\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5065526434745707\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5094645307836874\n",
      "avgfold accuracy: 0.5094604436481014\n",
      "standard deviation: 0.005726729451878928\n",
      "avg train time: 1.158305788040161\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5155923981734721\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5026511684170806\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5123465073005321\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5051637930239209\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5017936109664349\n",
      "avgfold accuracy: 0.5075094955762881\n",
      "standard deviation: 0.0054864742219255915\n",
      "avg train time: 1.1658325672149659\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5082643206611207\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5009996948736976\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5012969685175508\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4986632718014004\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5118395750942155\n",
      "avgfold accuracy: 0.504212766189597\n",
      "standard deviation: 0.0049840926759698876\n",
      "avg train time: 1.0609997272491456\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5025825024793025\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5027712329088647\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5105285019196233\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5011766713715071\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5034065141922414\n",
      "avgfold accuracy: 0.5040930845743078\n",
      "standard deviation: 0.003299108708684022\n",
      "avg train time: 1.1534377574920653\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5063538692629542\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5028055370493745\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.50149235734131\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5052486646555779\n",
      "tree depth: 6, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5085500010891973\n",
      "avgfold accuracy: 0.5048900858796828\n",
      "standard deviation: 0.0025122150751443408\n",
      "avg train time: 1.1750268936157227\n",
      "tree depth: 6, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5062052612249105\n",
      "tree depth: 6, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5064515255412381\n",
      "tree depth: 6, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.510072594664185\n",
      "tree depth: 6, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5010705818319358\n",
      "tree depth: 6, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5037609984176933\n",
      "avgfold accuracy: 0.5055121923359925\n",
      "standard deviation: 0.0029984844424969636\n",
      "avg train time: 1.2836946964263916\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6671705955229452\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6218019062269237\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6851373188468345\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6336626721648724\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6395114256801047\n",
      "avgfold accuracy: 0.649456783688336\n",
      "standard deviation: 0.023238055301776084\n",
      "avg train time: 1.0712694644927978\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5607762468314464\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.556787074199856\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5966066951144424\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5419238184854104\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5344408159870405\n",
      "avgfold accuracy: 0.5581069301236392\n",
      "standard deviation: 0.021508497094474523\n",
      "avg train time: 1.0489298343658446\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5097966846897695\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5104160008955186\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5336962020283819\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5226690283120693\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5182944555894637\n",
      "avgfold accuracy: 0.5189744743030407\n",
      "standard deviation: 0.008812726153106704\n",
      "avg train time: 1.1524914264678956\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.516247937250381\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5107541675017017\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5335337703556181\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5074110462273638\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5241611695206938\n",
      "avgfold accuracy: 0.5184216181711516\n",
      "standard deviation: 0.009445284287766099\n",
      "avg train time: 1.166788101196289\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5124930824709564\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5022395187309634\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5148375186299451\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5130531646970913\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5092518402484162\n",
      "avgfold accuracy: 0.5103750249554745\n",
      "standard deviation: 0.0044501787677448425\n",
      "avg train time: 1.121666383743286\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5047406339408751\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.501222671787011\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5063658306536031\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.509023607228849\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5124527931970715\n",
      "avgfold accuracy: 0.5067611073614818\n",
      "standard deviation: 0.003805969480503555\n",
      "avg train time: 1.1144636631011964\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5043278338351982\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5010339990142073\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5071690958179468\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5113460456277272\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5083550347651987\n",
      "avgfold accuracy: 0.5064464018120557\n",
      "standard deviation: 0.0035200599487834245\n",
      "avg train time: 1.170123291015625\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5003252989923674\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5025825601360611\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5015574869492297\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5029050083487855\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5057035323659941\n",
      "avgfold accuracy: 0.5026147773584875\n",
      "standard deviation: 0.0017880298471774682\n",
      "avg train time: 1.1052268028259278\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5024669184497129\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49924530890878527\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5040050785400296\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5055457153663779\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5108824676854953\n",
      "avgfold accuracy: 0.5044290977900803\n",
      "standard deviation: 0.0038417294301593195\n",
      "avg train time: 1.1197827816009522\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.500440883021957\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5027712329088647\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5015574869492297\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.504951613944778\n",
      "tree depth: 6, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5030343057555168\n",
      "avgfold accuracy: 0.5025511045160693\n",
      "standard deviation: 0.00151556564277392\n",
      "avg train time: 1.177028274536133\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6503282261203895\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6520993231251433\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6776642848585955\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6286570909325732\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6379836778827588\n",
      "avgfold accuracy: 0.649346520583892\n",
      "standard deviation: 0.016538465071806105\n",
      "avg train time: 1.0593916416168212\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.565615952797684\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5547925592513754\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5700627179814739\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5545120342438583\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.553557912622609\n",
      "avgfold accuracy: 0.5597082353794002\n",
      "standard deviation: 0.00679863637945557\n",
      "avg train time: 1.0685174942016602\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.523698228612201\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5083185735255088\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5438178400720244\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5258092786833827\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5154302626549878\n",
      "avgfold accuracy: 0.523414836709621\n",
      "standard deviation: 0.011947776316120973\n",
      "avg train time: 1.088568925857544\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5238022417297375\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5002621558527377\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5248232738169036\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5174955488519267\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5199393416099128\n",
      "avgfold accuracy: 0.5172645123722437\n",
      "standard deviation: 0.00890129199893068\n",
      "avg train time: 1.0957937717437745\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5104670470432005\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5028398411898842\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.534901492121933\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5009644922923643\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5097835665865942\n",
      "avgfold accuracy: 0.5117912878467952\n",
      "standard deviation: 0.012142602680085815\n",
      "avg train time: 1.1305827140808105\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5045755138986043\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4992281568385304\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5006239625690465\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5031808411516712\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5160259545824513\n",
      "avgfold accuracy: 0.5047268858080607\n",
      "standard deviation: 0.00595234612717583\n",
      "avg train time: 1.1273953437805175\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5061557252122293\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49962265445439263\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5128892540331967\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5073377060673991\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5115028150800361\n",
      "avgfold accuracy: 0.5075016309694507\n",
      "standard deviation: 0.004668030873280094\n",
      "avg train time: 1.0989195823669433\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5025329664666213\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5062800048386893\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5097469466245862\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5030323157962712\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5034419626147866\n",
      "avgfold accuracy: 0.5050068392681909\n",
      "standard deviation: 0.002703967338557666\n",
      "avg train time: 1.1128614902496339\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5102077960677412\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5028398411898842\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5017311659036825\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5055669332742921\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.503246996290788\n",
      "avgfold accuracy: 0.5047185465452777\n",
      "standard deviation: 0.003015661036107501\n",
      "avg train time: 1.1287861824035645\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5086110727498891\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5026168642765708\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49845859927923236\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.503244494875414\n",
      "tree depth: 6, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5119104719393059\n",
      "avgfold accuracy: 0.5049683006240825\n",
      "standard deviation: 0.004740052786312625\n",
      "avg train time: 1.1547000885009766\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6273202493212566\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6431853019396283\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6702507568374317\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6443394311755644\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.633052881521074\n",
      "avgfold accuracy: 0.643629724158991\n",
      "standard deviation: 0.014746180154161936\n",
      "avg train time: 1.0143956661224365\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5619651111357957\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5512617604523813\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.567094089527316\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.55481877139088\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5464682281135697\n",
      "avgfold accuracy: 0.5563215921239886\n",
      "standard deviation: 0.007387085435596434\n",
      "avg train time: 1.1352836608886718\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5172617993281116\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5155762463687261\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5535487004681484\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5258826188433473\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5311159915953573\n",
      "avgfold accuracy: 0.5286770713207382\n",
      "standard deviation: 0.013669639953484934\n",
      "avg train time: 1.0914883613586426\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.52663736536462\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5072110914313673\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5273794147542364\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.509203036928385\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5175359781685246\n",
      "avgfold accuracy: 0.5175933773294267\n",
      "standard deviation: 0.008435084056944122\n",
      "avg train time: 1.1488373756408692\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.505412685022011\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5007252617496195\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.532149962360841\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5003916087786788\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5148561566384597\n",
      "avgfold accuracy: 0.510707134909922\n",
      "standard deviation: 0.011925504611227926\n",
      "avg train time: 1.293008041381836\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5157740302199699\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5079829345928369\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5061921516991503\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5050364855764352\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5059516713238105\n",
      "avgfold accuracy: 0.5081874546824405\n",
      "standard deviation: 0.003911688238664854\n",
      "avg train time: 1.093688154220581\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5047901699535563\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499433981681589\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5036794305004307\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.500327955054936\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5130376921690672\n",
      "avgfold accuracy: 0.5042538458719158\n",
      "standard deviation: 0.004825292982432638\n",
      "avg train time: 1.107035255432129\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5083138566738019\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5009653907331878\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5012752586482442\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5014100683585642\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5119459203618512\n",
      "avgfold accuracy: 0.5047820989551298\n",
      "standard deviation: 0.00451728747512069\n",
      "avg train time: 1.1676814556121826\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5010683391825858\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5026854725575903\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.498371759802006\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5030323157962712\n",
      "tree depth: 6, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5088513126808315\n",
      "avgfold accuracy: 0.5028018400038569\n",
      "standard deviation: 0.0034436438792384153\n",
      "avg train time: 1.1247934341430663\n",
      "tree depth: 6, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.504509465881696\n",
      "tree depth: 6, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.499279613049295\n",
      "tree depth: 6, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5042004673637889\n",
      "tree depth: 6, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5013039788189928\n",
      "tree depth: 6, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5003473549337669\n",
      "avgfold accuracy: 0.5019281760095079\n",
      "standard deviation: 0.0020846999095009452\n",
      "avg train time: 1.238424587249756\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6545734999344524\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6369175743722794\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6905808724019433\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6396539635974502\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6222802247311168\n",
      "avgfold accuracy: 0.6488012270074484\n",
      "standard deviation: 0.02326849842005282\n",
      "avg train time: 1.101775884628296\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.550073090636893\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5459005649350297\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5726188589188066\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5550618548141588\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5655498763265928\n",
      "avgfold accuracy: 0.5578408491262963\n",
      "standard deviation: 0.00988382458066959\n",
      "avg train time: 1.0383415699005127\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5393285918863013\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5190335620877861\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5395088233617027\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5199974169503409\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5268267324673884\n",
      "avgfold accuracy: 0.5289390253507038\n",
      "standard deviation: 0.00896879108078991\n",
      "avg train time: 1.19309458732605\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5161488652250186\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5063191837781136\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5298704260836494\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5102214965082704\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5165611465485317\n",
      "avgfold accuracy: 0.5158242236287168\n",
      "standard deviation: 0.007996006312009689\n",
      "avg train time: 1.0346670150756836\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5144860938902582\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5026168642765708\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.505584275358566\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5046545632339782\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5067244073281197\n",
      "avgfold accuracy: 0.5068132408174986\n",
      "standard deviation: 0.004065965733920645\n",
      "avg train time: 1.1097652912139893\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5026155264877566\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5031657305247269\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5071908056872534\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5042302050756926\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5034774110373318\n",
      "avgfold accuracy: 0.5041359357625523\n",
      "standard deviation: 0.0016141694050104153\n",
      "avg train time: 1.1564088821411134\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5084129286991643\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5062285486279247\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5037011403697373\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5091933504921632\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5145265259159655\n",
      "avgfold accuracy: 0.508412498820991\n",
      "standard deviation: 0.003604374945458464\n",
      "avg train time: 1.182600498199463\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5045920259028314\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5043026419604636\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5038965291934967\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5055244974584636\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5035305836711496\n",
      "avgfold accuracy: 0.504369255637281\n",
      "standard deviation: 0.0006805128354395053\n",
      "avg train time: 1.0835443019866944\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5122173194912701\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5012055197167561\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5045478252726943\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5035627634941282\n",
      "tree depth: 6, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5084259316102892\n",
      "avgfold accuracy: 0.5059918719170275\n",
      "standard deviation: 0.0038878232962998188\n",
      "avg train time: 1.1421302795410155\n",
      "tree depth: 6, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5121182474659077\n",
      "tree depth: 6, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5011026072952268\n",
      "tree depth: 6, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5072559352951731\n",
      "tree depth: 6, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.50109179973985\n",
      "tree depth: 6, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5087272432019233\n",
      "avgfold accuracy: 0.5060591665996161\n",
      "standard deviation: 0.004347540975497217\n",
      "avg train time: 1.1884098052978516\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6483798096215949\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6374321364799258\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.680285555403848\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6417951272613216\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6372675801401698\n",
      "avgfold accuracy: 0.649032041781372\n",
      "standard deviation: 0.016140265258172786\n",
      "avg train time: 1.0844674110412598\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5689745194756588\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5437516813542552\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5792234981347821\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5482121606287881\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.572440930847851\n",
      "avgfold accuracy: 0.5625205580882671\n",
      "standard deviation: 0.013971804401661729\n",
      "avg train time: 1.0498724460601807\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5381562395861791\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5136160355607553\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5474594744746866\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5290846779029327\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5247246806176343\n",
      "avgfold accuracy: 0.5306082216284376\n",
      "standard deviation: 0.011546505317598633\n",
      "avg train time: 1.1129096508026124\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.523438977636742\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5016735004125524\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5193853439026394\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5029571306008358\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5182449466194285\n",
      "avgfold accuracy: 0.5131399798344397\n",
      "standard deviation: 0.009014490366059495\n",
      "avg train time: 1.1313690185546874\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5106817030981524\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5011540635059915\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5149026482378648\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5126924602625486\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5151574682300939\n",
      "avgfold accuracy: 0.5109176686669302\n",
      "standard deviation: 0.005145668337792576\n",
      "avg train time: 1.0751439571380614\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5048397059662375\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5096001039957102\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5067783181704282\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5071891807119991\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5025203036286114\n",
      "avgfold accuracy: 0.5061855224945973\n",
      "standard deviation: 0.00237752622790873\n",
      "avg train time: 1.169346570968628\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5028466945469356\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49826321045547306\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.503011097888357\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5063593281830802\n",
      "avgfold accuracy: 0.502385195954834\n",
      "standard deviation: 0.0026180899392069324\n",
      "avg train time: 1.1304277420043944\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5194793489867133\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5028055370493745\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5129109639025033\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5034142381387282\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5006132181028559\n",
      "avgfold accuracy: 0.507844661236035\n",
      "standard deviation: 0.007188164897137362\n",
      "avg train time: 1.1323269844055175\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5080992006188499\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5010511510844622\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.501861425119522\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5010281460161071\n",
      "tree depth: 6, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.4979439914923786\n",
      "avgfold accuracy: 0.501996782866264\n",
      "standard deviation: 0.0033321290155441\n",
      "avg train time: 1.1058396816253662\n",
      "tree depth: 6, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5026815745046649\n",
      "tree depth: 6, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.499279613049295\n",
      "tree depth: 6, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5096166874087467\n",
      "tree depth: 6, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5055669332742921\n",
      "tree depth: 6, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5006663907366736\n",
      "avgfold accuracy: 0.5035622397947345\n",
      "standard deviation: 0.0036926287877000964\n",
      "avg train time: 1.2378872394561768\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.667573513444274\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6265725288831836\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6793737408929714\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6304569230343453\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6363779039486375\n",
      "avgfold accuracy: 0.6480709220406823\n",
      "standard deviation: 0.021304356742249647\n",
      "avg train time: 1.0613533973693847\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5565260319252097\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5491177516705213\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5642774301583042\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5509686436221737\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5647416918997384\n",
      "avgfold accuracy: 0.5571263098551895\n",
      "standard deviation: 0.006504563882409295\n",
      "avg train time: 1.0223021984100342\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5295451668362899\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5065813396308514\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5502110042357786\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5328499340399819\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5199888505799481\n",
      "avgfold accuracy: 0.5278352590645701\n",
      "standard deviation: 0.014436342605149161\n",
      "avg train time: 1.0403979778289796\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5237857297255104\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5046162540239659\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5272057357997837\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5118437439459773\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5094574014920024\n",
      "avgfold accuracy: 0.5153817729974479\n",
      "standard deviation: 0.008648061182833443\n",
      "avg train time: 1.0400925159454346\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5127242505301354\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5026340163468257\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5145335804596528\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5105609830348988\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5138281523846491\n",
      "avgfold accuracy: 0.5108561965512324\n",
      "standard deviation: 0.0043251982125157564\n",
      "avg train time: 1.0898059844970702\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5044269058605606\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4993996775410792\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5076901326813048\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5026716113617284\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5082486894975632\n",
      "avgfold accuracy: 0.5044874033884472\n",
      "standard deviation: 0.0032738498148543425\n",
      "avg train time: 1.1420840740203857\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5062878212460459\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5043540981712282\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5095298479315202\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5027989188092141\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5114850908687636\n",
      "avgfold accuracy: 0.5068911554053545\n",
      "standard deviation: 0.0032129118620162077\n",
      "avg train time: 1.1111984252929688\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5084294407033914\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5010511510844622\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5136273895896207\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.4989178866963717\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5065720187183513\n",
      "avgfold accuracy: 0.5057195773584395\n",
      "standard deviation: 0.005266131590494929\n",
      "avg train time: 1.2208987712860107\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5062878212460459\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5012741279977756\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.4981980808475533\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5011554534635928\n",
      "tree depth: 6, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5031052026006072\n",
      "avgfold accuracy: 0.5020041372311149\n",
      "standard deviation: 0.002657123877448569\n",
      "avg train time: 1.2063724040985107\n",
      "tree depth: 6, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5005399550473194\n",
      "tree depth: 6, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5028055370493745\n",
      "tree depth: 6, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5072559352951731\n",
      "tree depth: 6, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5050577034843495\n",
      "tree depth: 6, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5139239027326972\n",
      "avgfold accuracy: 0.5059166067217828\n",
      "standard deviation: 0.004587710604217905\n",
      "avg train time: 1.218628168106079\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6570354772919913\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6508766513381323\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6751025202804183\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6323665347466305\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6258959638307269\n",
      "avgfold accuracy: 0.6482554294975799\n",
      "standard deviation: 0.017647260456090226\n",
      "avg train time: 1.0247905731201172\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5486794524619398\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5455354966607627\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.6155200455331813\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5412660633400677\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.528393394315182\n",
      "avgfold accuracy: 0.5558788904622267\n",
      "standard deviation: 0.03061003853640537\n",
      "avg train time: 1.0851967334747314\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5220519692816677\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5208051001229532\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5624489621897778\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5301455732986466\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5286240070976059\n",
      "avgfold accuracy: 0.5328151223981303\n",
      "standard deviation: 0.015250852439864085\n",
      "avg train time: 1.045243501663208\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.512867917476005\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5070738748693283\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5193202142947195\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5090120757571565\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5276102614271654\n",
      "avgfold accuracy: 0.5151768687648749\n",
      "standard deviation: 0.007493343769346708\n",
      "avg train time: 1.1456177711486817\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.504063641767565\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5027540808386098\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5027676159895541\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5078990581093922\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5086492170651479\n",
      "avgfold accuracy: 0.5052267227540537\n",
      "standard deviation: 0.0025443570922027837\n",
      "avg train time: 1.542290163040161\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5060566531868669\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5045942271547965\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5101377242721047\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5172524654286479\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5110951582207665\n",
      "avgfold accuracy: 0.5098272456526365\n",
      "standard deviation: 0.004437110278487418\n",
      "avg train time: 1.0750606060028076\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5139131148344847\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5012912800680305\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5105502117889299\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.50109179973985\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5029456846991538\n",
      "avgfold accuracy: 0.5059584182260898\n",
      "standard deviation: 0.0052707586481776465\n",
      "avg train time: 1.1135669231414795\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.504492953877469\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5011369114357366\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5133885810272483\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49934224485465734\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5060934650139912\n",
      "avgfold accuracy: 0.5048908312418204\n",
      "standard deviation: 0.004872270735685531\n",
      "avg train time: 1.1456672191619872\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5045920259028314\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5030285139626879\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.510224563749331\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5165850238470835\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5011981170748516\n",
      "avgfold accuracy: 0.5071256489073571\n",
      "standard deviation: 0.005611139658881914\n",
      "avg train time: 1.15708646774292\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5045259778859231\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5010339990142073\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5016226165571496\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5032657127833282\n",
      "tree depth: 6, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5005954938915833\n",
      "avgfold accuracy: 0.5022087600264383\n",
      "standard deviation: 0.0014705715190313185\n",
      "avg train time: 1.129401969909668\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6635759196936173\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6320194847518096\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6871016696720449\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6337341672893662\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6423472994837205\n",
      "avgfold accuracy: 0.6517557081781117\n",
      "standard deviation: 0.020937368504662844\n",
      "avg train time: 1.0865365982055664\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5568629018296302\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5321521876111499\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5671592191352357\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5452319670845672\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5424910735327027\n",
      "avgfold accuracy: 0.5487794698386571\n",
      "standard deviation: 0.012097539213591845\n",
      "avg train time: 1.0593965530395508\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5164857351294391\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.519981800750719\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5661557262003596\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5107500991706565\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5350471028340915\n",
      "avgfold accuracy: 0.5296840928170531\n",
      "standard deviation: 0.019928718726910707\n",
      "avg train time: 1.0254014492034913\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5145686539113936\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5019993897473951\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5212250591528547\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5162340058487624\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5065932085575264\n",
      "avgfold accuracy: 0.5121240634435864\n",
      "standard deviation: 0.006910790768852415\n",
      "avg train time: 1.1090178489685059\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5095308038944313\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5025825601360611\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5145335804596528\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.504336294615264\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5121337573941647\n",
      "avgfold accuracy: 0.5086233992999147\n",
      "standard deviation: 0.004537559757615132\n",
      "avg train time: 1.1179448127746583\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5060566531868669\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5043197940307185\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5062138615684569\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5090448251367633\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5185711117140204\n",
      "avgfold accuracy: 0.5088412491273652\n",
      "standard deviation: 0.005095338365100651\n",
      "avg train time: 1.130005168914795\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.506535501309452\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5029770577519233\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5071690958179468\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5023109069271857\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5007550117930366\n",
      "avgfold accuracy: 0.5039495147199089\n",
      "standard deviation: 0.0024855014017655377\n",
      "avg train time: 1.228402280807495\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5081322246273041\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.501222671787011\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.507473033988239\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5012191071873356\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5030343057555168\n",
      "avgfold accuracy: 0.5042162686690813\n",
      "standard deviation: 0.0030094141283595996\n",
      "avg train time: 1.1275175094604493\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5047736579493292\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.49909094027649137\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.49798098215448744\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5053123183793208\n",
      "tree depth: 6, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.4979085430698334\n",
      "avgfold accuracy: 0.5010132883658924\n",
      "standard deviation: 0.0033211981256872874\n",
      "avg train time: 1.2153764724731446\n",
      "tree depth: 6, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5060566531868669\n",
      "tree depth: 6, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49926246097904015\n",
      "tree depth: 6, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.503983368670723\n",
      "tree depth: 6, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49868448970931467\n",
      "tree depth: 6, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49844026940801134\n",
      "avgfold accuracy: 0.5012854483907913\n",
      "standard deviation: 0.003130357060076903\n",
      "avg train time: 1.1712941169738769\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6656894562346827\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6441335406025612\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6733713543767358\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6446692312659711\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6351088900286954\n",
      "avgfold accuracy: 0.6525944945017292\n",
      "standard deviation: 0.014445067193200996\n",
      "avg train time: 1.0928121089935303\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5545330205059078\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5397454993870392\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.580916867940696\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.562399560881558\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.544089718175139\n",
      "avgfold accuracy: 0.556336933378068\n",
      "standard deviation: 0.014624997087897237\n",
      "avg train time: 1.0868844032287597\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5302172179174259\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5019699604057999\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5446806112034435\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5181629904334911\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.515908816359348\n",
      "avgfold accuracy: 0.5221879192639017\n",
      "standard deviation: 0.014385992307229662\n",
      "avg train time: 1.07121901512146\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5152902410052108\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5002964599932475\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5212250591528547\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5180259965497837\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.524373860055965\n",
      "avgfold accuracy: 0.5158423233514122\n",
      "standard deviation: 0.008348749060239672\n",
      "avg train time: 1.1120238780975342\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5099766280085622\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5024796477145318\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5261909955832187\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5036361036540927\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5128250016337961\n",
      "avgfold accuracy: 0.5110216753188402\n",
      "standard deviation: 0.008508411031092738\n",
      "avg train time: 1.0953902721405029\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5025329664666213\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5060055717146112\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5156407837942888\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5010281460161071\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5109179161080405\n",
      "avgfold accuracy: 0.5072250768199338\n",
      "standard deviation: 0.0054104755713888595\n",
      "avg train time: 1.0992305278778076\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5119035914109558\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.504388402311738\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014055178640837\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49913006577551455\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49774902516838\n",
      "avgfold accuracy: 0.5029153205061344\n",
      "standard deviation: 0.005025082064503716\n",
      "avg train time: 1.1959022998809814\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.506535501309452\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5029770577519233\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5040267884093361\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5052911004714066\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5009854265395804\n",
      "avgfold accuracy: 0.5039631748963397\n",
      "standard deviation: 0.0019091206607358372\n",
      "avg train time: 1.2027167320251464\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5066675973432686\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5097903663631993\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5033930202308139\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.502981133121699\n",
      "avgfold accuracy: 0.5048589835659121\n",
      "standard deviation: 0.002993294120497425\n",
      "avg train time: 1.1915154933929444\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.506535501309452\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4989708757847072\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5017311659036825\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5077620642256847\n",
      "tree depth: 6, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5005423212577654\n",
      "avgfold accuracy: 0.5031083856962584\n",
      "standard deviation: 0.003435178441047397\n",
      "avg train time: 1.15413179397583\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6429175885869027\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6481568746400322\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6874980709604079\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.648507827562985\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6652359894565698\n",
      "avgfold accuracy: 0.6584632702413795\n",
      "standard deviation: 0.01634558182309891\n",
      "avg train time: 1.1123770713806151\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5426013461786718\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5418552040283894\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5727925378732593\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5608718715117298\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5611685305143586\n",
      "avgfold accuracy: 0.5558578980212817\n",
      "standard deviation: 0.01193261396652062\n",
      "avg train time: 1.063452386856079\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5250968078793282\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5143192704412054\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5576189086160979\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5314495521176394\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.523597460387873\n",
      "avgfold accuracy: 0.5304163998884288\n",
      "standard deviation: 0.014662588148780745\n",
      "avg train time: 1.0766465663909912\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5199037200044433\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5115137333918309\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5254528600267947\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5057463629738281\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5150971663045911\n",
      "avgfold accuracy: 0.5155427685402977\n",
      "standard deviation: 0.006776858542808417\n",
      "avg train time: 1.1197163581848144\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5136489227668515\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5026511684170806\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5238085336003385\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5084295058072492\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5081883875720603\n",
      "avgfold accuracy: 0.511345303632716\n",
      "standard deviation: 0.00713715273007064\n",
      "avg train time: 1.0832913398742676\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5085450247329809\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5028569932601391\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5071256760793336\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5115794426147843\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5051009091827258\n",
      "avgfold accuracy: 0.5070416091739927\n",
      "standard deviation: 0.0029710800974234253\n",
      "avg train time: 1.1258503913879394\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5082312966526665\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5028398411898842\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5012101290403244\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5029262262566998\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5003650791450395\n",
      "avgfold accuracy: 0.5031145144569228\n",
      "standard deviation: 0.002737825277034044\n",
      "avg train time: 1.1080355167388916\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5174533135589574\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5024796477145318\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5095081380622137\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5030323157962712\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.498280751506558\n",
      "avgfold accuracy: 0.5061508333277065\n",
      "standard deviation: 0.006695647806299029\n",
      "avg train time: 1.1547320365905762\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5027311105173461\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.503011361892433\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5105285019196233\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5029262262566998\n",
      "tree depth: 6, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5086740705681055\n",
      "avgfold accuracy: 0.5055742542308416\n",
      "standard deviation: 0.0033411784433719535\n",
      "avg train time: 1.12733998298645\n",
      "tree depth: 6, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5046580739197397\n",
      "tree depth: 6, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5011197593654817\n",
      "tree depth: 6, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5019265547274417\n",
      "tree depth: 6, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5013676325427356\n",
      "tree depth: 6, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5113432971785828\n",
      "avgfold accuracy: 0.5040830635467963\n",
      "standard deviation: 0.00384309502390668\n",
      "avg train time: 1.2168323516845703\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.648863598836354\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6341120373229049\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6586094279423281\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6545415548113912\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6385863010660271\n",
      "avgfold accuracy: 0.646942583995801\n",
      "standard deviation: 0.00929516320969384\n",
      "avg train time: 1.0790226459503174\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5517853979843346\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5324143434638876\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5788978500951834\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5516263987675163\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.567038710073491\n",
      "avgfold accuracy: 0.5563525400768826\n",
      "standard deviation: 0.015739870956712032\n",
      "avg train time: 0.9953824996948242\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5384039196495852\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5094211808207356\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5484581284627895\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5192663216450336\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.521406787481756\n",
      "avgfold accuracy: 0.5273912676119801\n",
      "standard deviation: 0.014070137879128562\n",
      "avg train time: 1.1764962196350097\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5144035338691229\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5067136813939759\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.522050034186505\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.511589129051006\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5195139605393705\n",
      "avgfold accuracy: 0.514854067807996\n",
      "standard deviation: 0.0054886287773010115\n",
      "avg train time: 1.1643387794494628\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5093987078606147\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5026168642765708\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5357264671555834\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5063829002112565\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5146434661031886\n",
      "avgfold accuracy: 0.5137536811214428\n",
      "standard deviation: 0.011669772372934968\n",
      "avg train time: 1.1510752201080323\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5061061891995481\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5027540808386098\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.507364484641706\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5094904012029632\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5090108305822849\n",
      "avgfold accuracy: 0.5069451972930225\n",
      "standard deviation: 0.002416199388503216\n",
      "avg train time: 1.2008604526519775\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5066180613305874\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501222671787011\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5131931922034889\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5032232769674997\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5107761224178596\n",
      "avgfold accuracy: 0.5070066649412894\n",
      "standard deviation: 0.004481105742309951\n",
      "avg train time: 1.131545114517212\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5080496646061687\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5043712502414831\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5156190739249822\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5033505844149854\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5067315366198047\n",
      "avgfold accuracy: 0.5076244219614849\n",
      "standard deviation: 0.0043299272319840335\n",
      "avg train time: 1.1341880798339843\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5122668555039513\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5012912800680305\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5011015796937914\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5037325067574424\n",
      "tree depth: 6, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.506589742929624\n",
      "avgfold accuracy: 0.504996392990568\n",
      "standard deviation: 0.004144713017389473\n",
      "avg train time: 1.1354454040527344\n",
      "tree depth: 6, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5025990144835295\n",
      "tree depth: 6, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.4992281568385304\n",
      "tree depth: 6, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5013838079947771\n",
      "tree depth: 6, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.49889666878845745\n",
      "tree depth: 6, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.502981133121699\n",
      "avgfold accuracy: 0.5010177562453987\n",
      "standard deviation: 0.0016846897811957655\n",
      "avg train time: 1.1883087158203125\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6550193240485833\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6471400276960797\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6726549286896184\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6534382235998487\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6303304822767789\n",
      "avgfold accuracy: 0.6517165972621818\n",
      "standard deviation: 0.013642555681393338\n",
      "avg train time: 1.1262513637542724\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5615853350385731\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5408603839536064\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5930245666788556\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5518810136624877\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5438841569315528\n",
      "avgfold accuracy: 0.5582470912530151\n",
      "standard deviation: 0.01881347966482185\n",
      "avg train time: 1.0690163612365722\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5221510413070302\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.512153234790176\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5602401791613504\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5237511416156976\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5324381781491171\n",
      "avgfold accuracy: 0.5301467550046743\n",
      "standard deviation: 0.016367448642754395\n",
      "avg train time: 1.121838665008545\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5204766990602168\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5070395707288186\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5143816113745068\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5091818190204707\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5140691620507803\n",
      "avgfold accuracy: 0.5130297724469586\n",
      "standard deviation: 0.004670198655608736\n",
      "avg train time: 1.1145028114318847\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5078184965469896\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5028226891196294\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5148592284992516\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.504569691602321\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5183584211787492\n",
      "avgfold accuracy: 0.5096857053893882\n",
      "standard deviation: 0.005979003241024204\n",
      "avg train time: 1.1392879962921143\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5099436040001081\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4994854378923536\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5005805428304334\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5028201367171284\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5119281961505785\n",
      "avgfold accuracy: 0.5049515835181204\n",
      "standard deviation: 0.0050422455806301576\n",
      "avg train time: 1.1222171783447266\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5007380990980443\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.503148578454472\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5133668711579417\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5053123183793208\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5116091603476718\n",
      "avgfold accuracy: 0.5068350054874902\n",
      "standard deviation: 0.004869053534037925\n",
      "avg train time: 1.160293197631836\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.506552013313679\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5010511510844622\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5008844810007256\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5031596232437568\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5010563233846709\n",
      "avgfold accuracy: 0.5025407184054589\n",
      "standard deviation: 0.002174345499774168\n",
      "avg train time: 1.0856688499450684\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5027145985131191\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5009653907331878\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5068651576476545\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.49902397623594313\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5054908418307229\n",
      "avgfold accuracy: 0.5030119929921255\n",
      "standard deviation: 0.002869594792205245\n",
      "avg train time: 1.1757146835327148\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5005069310388652\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5009139345224232\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5043090167103218\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.49898154042011456\n",
      "tree depth: 6, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5063947766056254\n",
      "avgfold accuracy: 0.50222123985947\n",
      "standard deviation: 0.002717380193660662\n",
      "avg train time: 1.209915781021118\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6455925332716885\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6326712634214949\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6704461456611911\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6317203110729804\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6357149788398662\n",
      "avgfold accuracy: 0.6432290464534443\n",
      "standard deviation: 0.014468832322845047\n",
      "avg train time: 1.134023332595825\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5591464995051403\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5461406939185981\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.607086153654608\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5471743281764592\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5546071067155948\n",
      "avgfold accuracy: 0.5628309563940801\n",
      "standard deviation: 0.022642545260258648\n",
      "avg train time: 1.1352828025817872\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5153513479299451\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5153875735959225\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5527454353038047\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.531789038644268\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5384324271871578\n",
      "avgfold accuracy: 0.5307411645322196\n",
      "standard deviation: 0.014261982353529769\n",
      "avg train time: 1.1343652725219726\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5136274696704505\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5018621731853561\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5296806609007346\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5071564313323924\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5202761016240922\n",
      "avgfold accuracy: 0.5145205673426052\n",
      "standard deviation: 0.009779751293625098\n",
      "avg train time: 1.0773098945617676\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5062382852333647\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49936537340056947\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5199715103739172\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5025443039142428\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5177380737842082\n",
      "avgfold accuracy: 0.5091715093412603\n",
      "standard deviation: 0.00823055291200912\n",
      "avg train time: 1.164847183227539\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5095968519113395\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5010854552249719\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5093995887156808\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5049940497606067\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5113255729673102\n",
      "avgfold accuracy: 0.5072803037159819\n",
      "standard deviation: 0.003736979334760701\n",
      "avg train time: 1.1482848644256591\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.504509465881696\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.510593631527543\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5007735311211359\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995320412152274\n",
      "avgfold accuracy: 0.5033742941032363\n",
      "standard deviation: 0.003964546125797399\n",
      "avg train time: 1.2159831523895264\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5086771207667975\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5010168469439524\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5043741463182415\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.4991937194992574\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.503778722628966\n",
      "avgfold accuracy: 0.503408111231443\n",
      "standard deviation: 0.0032345720163724444\n",
      "avg train time: 1.2077582359313965\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5010353151741316\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.499433981681589\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.510224563749331\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5011554534635928\n",
      "tree depth: 6, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5004714244126751\n",
      "avgfold accuracy: 0.5024641476962639\n",
      "standard deviation: 0.003927624026025887\n",
      "avg train time: 1.2300182819366454\n",
      "tree depth: 6, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5102903560888766\n",
      "tree depth: 6, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5044570105927575\n",
      "tree depth: 6, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5015791968185364\n",
      "tree depth: 6, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5032657127833282\n",
      "tree depth: 6, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5008436328493997\n",
      "avgfold accuracy: 0.5040871818265796\n",
      "standard deviation: 0.0033496732497863657\n",
      "avg train time: 1.2046028137207032\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6367850051987796\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6273003183063144\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6599771497086431\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6470843826972573\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6502557633392019\n",
      "avgfold accuracy: 0.6442805238500393\n",
      "standard deviation: 0.01126462919843781\n",
      "avg train time: 1.097719383239746\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5665076010259459\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5445112472443845\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.6033198836769509\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.565698023044493\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5548835648042713\n",
      "avgfold accuracy: 0.5669840639592091\n",
      "standard deviation: 0.019867673661594518\n",
      "avg train time: 1.0565211296081543\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5179338504092476\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5236106371723278\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5453375309234855\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.525361857581712\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5313924496840338\n",
      "avgfold accuracy: 0.5287272651541614\n",
      "standard deviation: 0.009348880033402455\n",
      "avg train time: 1.0716571807861328\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.508997478666991\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5070395707288186\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5233252928347492\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5142510539765127\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.522838982966934\n",
      "avgfold accuracy: 0.5152904758348011\n",
      "standard deviation: 0.006786720715750695\n",
      "avg train time: 1.162433958053589\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5112100872334188\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5077771097497784\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5177193076068767\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.510645854666556\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5122755510843455\n",
      "avgfold accuracy: 0.511925582068195\n",
      "standard deviation: 0.0032572746764006127\n",
      "avg train time: 1.1505508422851562\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5082973446695748\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5029770577519233\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5101160144027981\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.503011097888357\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5047995975910915\n",
      "avgfold accuracy: 0.5058402224607489\n",
      "standard deviation: 0.002884593711291905\n",
      "avg train time: 1.1090919971466064\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.506750157364404\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5028912974006489\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5104850821810101\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5048455244052067\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5136048669297903\n",
      "avgfold accuracy: 0.507715385656212\n",
      "standard deviation: 0.0038677333223504215\n",
      "avg train time: 1.1765509128570557\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5081817606399853\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5029770577519233\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5039616588014164\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5038318952627837\n",
      "avgfold accuracy: 0.5041021932340145\n",
      "standard deviation: 0.0022123069186402544\n",
      "avg train time: 1.2069305896759033\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5028301825427086\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5081029990846211\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5018831349888285\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.50124032509525\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5171425798926249\n",
      "avgfold accuracy: 0.5062398443208066\n",
      "standard deviation: 0.00596561632614761\n",
      "avg train time: 1.2784220218658446\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5140121868598472\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.502736928768355\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5047215042271469\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49925737322300023\n",
      "tree depth: 6, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5035305836711496\n",
      "avgfold accuracy: 0.5048517153498998\n",
      "standard deviation: 0.004928398690283261\n",
      "avg train time: 1.2223537921905518\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6417287242825533\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6331907003280559\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6849202201537686\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.647180785800607\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6164597521778996\n",
      "avgfold accuracy: 0.6446960365485769\n",
      "standard deviation: 0.02263901227634598\n",
      "avg train time: 1.106250810623169\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5534811933275492\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.549169207881286\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.6042043646776765\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5658677663078072\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5568509522555297\n",
      "avgfold accuracy: 0.5659146968899698\n",
      "standard deviation: 0.01991610273897733\n",
      "avg train time: 1.076803159713745\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5246394128531442\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5193643262215434\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5526368859572718\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5229872969307835\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5150226057957181\n",
      "avgfold accuracy: 0.5269301055516922\n",
      "standard deviation: 0.013271481748410863\n",
      "avg train time: 1.2221747875213622\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5116344582511486\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5036337112205233\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5123408836596876\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5087786787700994\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5152318307030868\n",
      "avgfold accuracy: 0.5103239125209091\n",
      "standard deviation: 0.003924790551175698\n",
      "avg train time: 1.0519241333007812\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5168753934110099\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5006223493280902\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5201668991976764\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5097237981900202\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5123996205632536\n",
      "avgfold accuracy: 0.51195761213801\n",
      "standard deviation: 0.006711936320455446\n",
      "avg train time: 1.089138889312744\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5135003147288079\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.49951974203286337\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5043958561875481\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5056518049059493\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5083195863426536\n",
      "avgfold accuracy: 0.5062774608395644\n",
      "standard deviation: 0.004603010748628004\n",
      "avg train time: 1.125372552871704\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5141442828936638\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5009653907331878\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.507103966210027\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5050364855764352\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.508301862131381\n",
      "avgfold accuracy: 0.5071103975089388\n",
      "standard deviation: 0.004311783665523835\n",
      "avg train time: 1.2435690402984618\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5063208452545\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5046456833655611\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5041570476251757\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5037325067574424\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5005245970464929\n",
      "avgfold accuracy: 0.5038761360098345\n",
      "standard deviation: 0.001892150128516889\n",
      "avg train time: 1.2687543869018554\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5045920259028314\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5029427536114135\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4986539881029916\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5013464146348213\n",
      "tree depth: 6, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5005423212577654\n",
      "avgfold accuracy: 0.5016155007019647\n",
      "standard deviation: 0.002030565480938427\n",
      "avg train time: 1.174037218093872\n",
      "tree depth: 6, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5011013631910399\n",
      "tree depth: 6, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5008281741711488\n",
      "tree depth: 6, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5011884191710178\n",
      "tree depth: 6, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5012191071873356\n",
      "tree depth: 6, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.49806806097128675\n",
      "avgfold accuracy: 0.5004810249383658\n",
      "standard deviation: 0.0012143227948620647\n",
      "avg train time: 1.1114707946777345\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6212602186789796\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5830463521157531\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.6180319820378296\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5931064862221976\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5911075948740393\n",
      "avgfold accuracy: 0.6013105267857599\n",
      "standard deviation: 0.015379099127853148\n",
      "avg train time: 1.2959467887878418\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.527964955522665\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5065984917011063\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5389009470211182\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5246847295639259\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.524749534120592\n",
      "avgfold accuracy: 0.5245797315858816\n",
      "standard deviation: 0.010391552318250185\n",
      "avg train time: 1.2676695823669433\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5140567817803543\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5042683378199538\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5205142571065818\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5043575125231783\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5135091165817423\n",
      "avgfold accuracy: 0.5113412011623621\n",
      "standard deviation: 0.006245262101523583\n",
      "avg train time: 1.2336453437805175\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5007215870938172\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5046971395763258\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5210135841006334\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5055457153663779\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5099253602767749\n",
      "avgfold accuracy: 0.5083806772827859\n",
      "standard deviation: 0.0069605100652967876\n",
      "avg train time: 1.3230809688568115\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5049718020000541\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5048300535736799\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5034142381387282\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5085854495117426\n",
      "avgfold accuracy: 0.5042917003638214\n",
      "standard deviation: 0.002878868367274651\n",
      "avg train time: 1.3617528438568116\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5048066819577833\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5051774114825853\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5088158642582863\n",
      "avgfold accuracy: 0.5040543804683455\n",
      "standard deviation: 0.0031038568429284743\n",
      "avg train time: 1.291008186340332\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5051369220423247\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499571198243628\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019916843353616\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014285318213954\n",
      "avgfold accuracy: 0.501958603939249\n",
      "standard deviation: 0.0017983254461885186\n",
      "avg train time: 1.3276880741119386\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5031769346314771\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5031828825949818\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.499001346011897\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5013464146348213\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.498280751506558\n",
      "avgfold accuracy: 0.500997665875947\n",
      "standard deviation: 0.0020500208196498003\n",
      "avg train time: 1.3845136642456055\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.49937254383937124\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5022522027670405\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5014949399902213\n",
      "tree depth: 7, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5014994286664858\n",
      "avgfold accuracy: 0.5012129527926886\n",
      "standard deviation: 0.0009677683830247631\n",
      "avg train time: 1.3400105476379394\n",
      "tree depth: 7, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.508346880682256\n",
      "tree depth: 7, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5012741279977756\n",
      "tree depth: 7, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5071473859486402\n",
      "tree depth: 7, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5014949399902213\n",
      "tree depth: 7, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5070682966339841\n",
      "avgfold accuracy: 0.5050663262505755\n",
      "standard deviation: 0.003040930767753778\n",
      "avg train time: 1.3429318428039552\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6049462334116903\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5746468930378844\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6198121913209697\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5748567791215786\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.6052407225141051\n",
      "avgfold accuracy: 0.5959005638812456\n",
      "standard deviation: 0.018085286996264636\n",
      "avg train time: 1.28436336517334\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5391701016639097\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5302165313458598\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5441217782423167\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5231897895737045\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5320836939236651\n",
      "avgfold accuracy: 0.5337563789498911\n",
      "standard deviation: 0.007262659339627085\n",
      "avg train time: 1.2881805896759033\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5131700746442664\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5002450037824828\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.511195884227283\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5060646315925423\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5083407761818286\n",
      "avgfold accuracy: 0.5078032740856806\n",
      "standard deviation: 0.004488910513856383\n",
      "avg train time: 1.3436138153076171\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5081817606399853\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.4995025899626085\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5100074650562652\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5035627634941282\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.512913622690159\n",
      "avgfold accuracy: 0.5068336403686293\n",
      "standard deviation: 0.00476024604464446\n",
      "avg train time: 1.4657023906707765\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5068162053813122\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5079723609822905\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5015161578981356\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5085145526666521\n",
      "avgfold accuracy: 0.5052461242976409\n",
      "standard deviation: 0.003136824191837034\n",
      "avg train time: 1.3894139289855958\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5028136705384815\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5031314263842172\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5022522027670405\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5061998102816269\n",
      "avgfold accuracy: 0.503191140737066\n",
      "standard deviation: 0.0015965560608583646\n",
      "avg train time: 1.3585959434509278\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5030613506018876\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5012741279977756\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5048951831815996\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995756418417144\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49842254519673873\n",
      "avgfold accuracy: 0.5014457697639432\n",
      "standard deviation: 0.002331320676997619\n",
      "avg train time: 1.3079726219177246\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5106536201818722\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5012741279977756\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5021002336818945\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5079530253969132\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5097906958782792\n",
      "avgfold accuracy: 0.506354340627347\n",
      "standard deviation: 0.003918012796202045\n",
      "avg train time: 1.3312779426574708\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5105545481565098\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.4995883503138829\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5017962955116022\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5015798116218785\n",
      "tree depth: 7, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5151823217330516\n",
      "avgfold accuracy: 0.505740265467385\n",
      "standard deviation: 0.006050455844040452\n",
      "avg train time: 1.3919196128845215\n",
      "tree depth: 7, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.504955289995827\n",
      "tree depth: 7, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 7, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.504612954880614\n",
      "tree depth: 7, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5058427660771778\n",
      "tree depth: 7, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5148278375075996\n",
      "avgfold accuracy: 0.5063403298463596\n",
      "standard deviation: 0.0044939471636622885\n",
      "avg train time: 1.266450834274292\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6150103625335619\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5663160422410354\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6309107734834087\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.6017287982361461\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5864178081784858\n",
      "avgfold accuracy: 0.6000767569345277\n",
      "standard deviation: 0.02237333739486354\n",
      "avg train time: 1.2792139053344727\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.519844301807414\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5151302925420993\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5502327141050852\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5202308139373979\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5337497697832894\n",
      "avgfold accuracy: 0.5278375784350572\n",
      "standard deviation: 0.01280411309190384\n",
      "avg train time: 1.285151195526123\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5108633351446502\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5039767526256208\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5205793867145017\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5102851502320133\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5136686344831957\n",
      "avgfold accuracy: 0.5118746518399964\n",
      "standard deviation: 0.005383620933535901\n",
      "avg train time: 1.391978645324707\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5121677834785889\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5098573850495334\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5066263490852821\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5013251967269071\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5102266718684091\n",
      "avgfold accuracy: 0.5080406772417442\n",
      "standard deviation: 0.0038002633681420146\n",
      "avg train time: 1.2931594848632812\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5106371081776452\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5047485957870904\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5047215042271469\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.508301862131381\n",
      "avgfold accuracy: 0.5059935328074455\n",
      "standard deviation: 0.00315365775388204\n",
      "avg train time: 1.282294750213623\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5011508992037211\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5110495387829813\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49942711648631444\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.498280751506558\n",
      "avgfold accuracy: 0.5019267745710994\n",
      "standard deviation: 0.004652180123267053\n",
      "avg train time: 1.3188609600067138\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5066675973432686\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49908818548912337\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013399107650324\n",
      "avgfold accuracy: 0.5015927008453572\n",
      "standard deviation: 0.0027039118430086644\n",
      "avg train time: 1.2962121486663818\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.4996202239027773\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5013255842085402\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5017745856422956\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.4993210269467431\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5046649331925959\n",
      "avgfold accuracy: 0.5013412707785905\n",
      "standard deviation: 0.0019119689940332997\n",
      "avg train time: 1.2279376983642578\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5106536201818722\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5021436534205076\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.505779112353435\n",
      "tree depth: 7, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.4988479262672811\n",
      "avgfold accuracy: 0.5034471278900584\n",
      "standard deviation: 0.0043230181681135245\n",
      "avg train time: 1.2930927753448487\n",
      "tree depth: 7, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5086771207667975\n",
      "tree depth: 7, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5012569759275207\n",
      "tree depth: 7, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5017094560343759\n",
      "tree depth: 7, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.50138885045065\n",
      "tree depth: 7, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5012512897086694\n",
      "avgfold accuracy: 0.5028567385776027\n",
      "standard deviation: 0.002914944535420679\n",
      "avg train time: 1.2723889827728272\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.615246471684915\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5889269845396655\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.633613259865043\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5676136311220583\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.6018625274527238\n",
      "avgfold accuracy: 0.6014525749328812\n",
      "standard deviation: 0.02247983023472695\n",
      "avg train time: 1.3282553195953368\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5276842514508048\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5178966506520495\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5523980773948993\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5203369034769694\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5326225495535281\n",
      "avgfold accuracy: 0.5301876865056502\n",
      "standard deviation: 0.01227684566345635\n",
      "avg train time: 1.3048107624053955\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5103349510093839\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5041825774686794\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.514207932420054\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5081324550964492\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5239201598545624\n",
      "avgfold accuracy: 0.5121556151698258\n",
      "standard deviation: 0.006718429621269828\n",
      "avg train time: 1.247420310974121\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5080992006188499\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5030971222437074\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5122379579539992\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5011978892794214\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5141720416905134\n",
      "avgfold accuracy: 0.5077608423572983\n",
      "standard deviation: 0.005021656413374508\n",
      "avg train time: 1.3169727802276612\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5087596807879329\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5167479871289247\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5035415455862139\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5096311779768259\n",
      "avgfold accuracy: 0.5076846220852148\n",
      "standard deviation: 0.005784951604213009\n",
      "avg train time: 1.3463284969329834\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5032760066568396\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5079723609822905\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5008968054832175\n",
      "avgfold accuracy: 0.5026357322354729\n",
      "standard deviation: 0.0029131933551970984\n",
      "avg train time: 1.3817412853240967\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5050048260085082\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5047215042271469\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013464146348213\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501712119201757\n",
      "avgfold accuracy: 0.502508947017733\n",
      "standard deviation: 0.002033157953140123\n",
      "avg train time: 1.3863752841949464\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.503143910623023\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4989145065346707\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5013251967269071\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5040268615867823\n",
      "avgfold accuracy: 0.5014443605397159\n",
      "standard deviation: 0.0019307481060300681\n",
      "avg train time: 1.313895845413208\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5123329035208597\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.4993634627625716\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5044522426573247\n",
      "avgfold accuracy: 0.5029865229928686\n",
      "standard deviation: 0.005072081343709144\n",
      "avg train time: 1.3610848903656005\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5012499712290835\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5031142743139623\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5074079043803192\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5037749425732709\n",
      "tree depth: 7, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5039382405304194\n",
      "avgfold accuracy: 0.503897066605411\n",
      "standard deviation: 0.0019981225766191185\n",
      "avg train time: 1.3233432292938232\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6066090047464507\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5930360789284108\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.6310354090584037\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5755048478306996\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5889949481046977\n",
      "avgfold accuracy: 0.5990360577337326\n",
      "standard deviation: 0.018824715027879583\n",
      "avg train time: 1.3213152885437012\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5322102293367278\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5206164273501496\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5459888270026831\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5196367125157981\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5295030883695507\n",
      "avgfold accuracy: 0.5295910569149819\n",
      "standard deviation: 0.009543454455619863\n",
      "avg train time: 1.1591116428375243\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5125261064794105\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5043712502414831\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050415286259012\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5081961088201921\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5060863357223062\n",
      "avgfold accuracy: 0.5072442659778587\n",
      "standard deviation: 0.0029411194338288594\n",
      "avg train time: 1.204535961151123\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5077194245216272\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.507473033988239\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5036476351257853\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5031052026006072\n",
      "avgfold accuracy: 0.504337603036487\n",
      "standard deviation: 0.0029791096040965317\n",
      "avg train time: 1.252315378189087\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5049883140042811\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5105067920503167\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5057578944455207\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.501091771807216\n",
      "avgfold accuracy: 0.5047752362717868\n",
      "standard deviation: 0.0034044698954485792\n",
      "avg train time: 1.3094647884368897\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.49943859185627953\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5019265547274417\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.506589742929624\n",
      "avgfold accuracy: 0.5014837197077189\n",
      "standard deviation: 0.0027060701245581434\n",
      "avg train time: 1.3049809455871582\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013160192459919\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5036688530336996\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014285318213954\n",
      "avgfold accuracy: 0.5016910448452034\n",
      "standard deviation: 0.0012606050168899362\n",
      "avg train time: 1.2995553970336915\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5051991213518918\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5042218279107809\n",
      "avgfold accuracy: 0.5031457317209395\n",
      "standard deviation: 0.0022767481341635486\n",
      "avg train time: 1.2421386241912842\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5053350660930497\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5049715727004038\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.49913160522773653\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.4994058985784002\n",
      "tree depth: 7, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5013753591875776\n",
      "avgfold accuracy: 0.5020439003574335\n",
      "standard deviation: 0.0026567085218939367\n",
      "avg train time: 1.341294288635254\n",
      "tree depth: 7, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5126961676138553\n",
      "tree depth: 7, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 7, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.501123289563098\n",
      "tree depth: 7, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5037537246653567\n",
      "tree depth: 7, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5008436328493997\n",
      "avgfold accuracy: 0.5039793535065088\n",
      "standard deviation: 0.004478182790638207\n",
      "avg train time: 1.4000189781188965\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6054928182788888\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5886231220739923\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.612425996809957\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5817719720661629\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.6236983596688048\n",
      "avgfold accuracy: 0.6024024537795611\n",
      "standard deviation: 0.01535593783032075\n",
      "avg train time: 1.3652558326721191\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5241060876257039\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5220449239802192\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5411531497881588\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5168590116144983\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.531250655993853\n",
      "avgfold accuracy: 0.5270827658004866\n",
      "standard deviation: 0.008415612004391329\n",
      "avg train time: 1.2683951377868652\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.518059316623185\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025997122063159\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5168292029653065\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5172736833365622\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.510485405745813\n",
      "avgfold accuracy: 0.5130494641754366\n",
      "standard deviation: 0.00588218983433456\n",
      "avg train time: 1.275929832458496\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5042287618098358\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5129760935104231\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5035627634941282\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5082664137088357\n",
      "avgfold accuracy: 0.505741628637676\n",
      "standard deviation: 0.004529084131476185\n",
      "avg train time: 1.2571245670318603\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5049387779915999\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5021436534205076\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5015798116218785\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5007372875817641\n",
      "avgfold accuracy: 0.502155314207011\n",
      "standard deviation: 0.001462567566855894\n",
      "avg train time: 1.3429678916931151\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5032429826483854\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5021002336818945\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.501622247437707\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5066074671408966\n",
      "avgfold accuracy: 0.502659699556961\n",
      "standard deviation: 0.0022769950204848573\n",
      "avg train time: 1.2421691417694092\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5088257288048411\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019048448581351\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5035415455862139\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5069973997888937\n",
      "avgfold accuracy: 0.5042024475968521\n",
      "standard deviation: 0.0033103303863578964\n",
      "avg train time: 1.2896666049957275\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5051038980338707\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5050401809814234\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5021653632898142\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5037112888495281\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.4987947536334633\n",
      "avgfold accuracy: 0.50296309695762\n",
      "standard deviation: 0.0023438670384612298\n",
      "avg train time: 1.3050913333892822\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5050048260085082\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.49921844470496285\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5034354560466425\n",
      "tree depth: 7, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5044522426573247\n",
      "avgfold accuracy: 0.502363876844621\n",
      "standard deviation: 0.002426082777880053\n",
      "avg train time: 1.3521066665649415\n",
      "tree depth: 7, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5048892419789187\n",
      "tree depth: 7, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5031142743139623\n",
      "tree depth: 7, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.49841517954061915\n",
      "tree depth: 7, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5038496194740564\n",
      "avgfold accuracy: 0.5019518171035228\n",
      "standard deviation: 0.0025355860295197107\n",
      "avg train time: 1.3063898086547852\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6156824136146979\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5817501972488079\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6038835555848507\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5790985156689638\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5928091191562087\n",
      "avgfold accuracy: 0.5946447602547058\n",
      "standard deviation: 0.01370593486138592\n",
      "avg train time: 1.148674488067627\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5234670605530221\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5259481935259059\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5422820629921012\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5194033155287411\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5377057345249813\n",
      "avgfold accuracy: 0.5297612734249503\n",
      "standard deviation: 0.008733024897801075\n",
      "avg train time: 1.1332243442535401\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5084905476281256\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5027540808386098\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5139908337269882\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5084719416230777\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5133850471028341\n",
      "avgfold accuracy: 0.5094184901839272\n",
      "standard deviation: 0.004069675238973409\n",
      "avg train time: 1.2686824321746826\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5044434178647877\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5070605464714139\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5017071190693642\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5116268845589443\n",
      "avgfold accuracy: 0.5049229982102393\n",
      "standard deviation: 0.00416086867550243\n",
      "avg train time: 1.2579256057739259\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5068657413939934\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5051991213518918\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.501622247437707\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5033887899809688\n",
      "avgfold accuracy: 0.5033808758924024\n",
      "standard deviation: 0.0024965040499633114\n",
      "avg train time: 1.316637134552002\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5051369220423247\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49962265445439263\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5038385962970138\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5176743062308029\n",
      "avgfold accuracy: 0.5057005943844536\n",
      "standard deviation: 0.006263186040473011\n",
      "avg train time: 1.330169153213501\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013655552586731\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5021870731591208\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501622247437707\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5072809871692553\n",
      "avgfold accuracy: 0.5024431468082377\n",
      "standard deviation: 0.0025497184696191837\n",
      "avg train time: 1.3475528240203858\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49906647561981676\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5037325067574424\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5068733303099856\n",
      "avgfold accuracy: 0.50327803595877\n",
      "standard deviation: 0.002740901534810358\n",
      "avg train time: 1.273411178588867\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5071464454658537\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5012569759275207\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5024258817214933\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4993846806704859\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5015526013003035\n",
      "avgfold accuracy: 0.5023533170171314\n",
      "standard deviation: 0.0025936559974229097\n",
      "avg train time: 1.3085570812225342\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5010683391825858\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5016877461650693\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49942711648631444\n",
      "tree depth: 7, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5043990700235069\n",
      "avgfold accuracy: 0.5015987232834582\n",
      "standard deviation: 0.0016051268547928176\n",
      "avg train time: 1.2691789150238038\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6105950275850542\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6006367931767259\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6058591536917501\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5948984769232188\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5988919892506124\n",
      "avgfold accuracy: 0.6021762881254723\n",
      "standard deviation: 0.0054874743290491065\n",
      "avg train time: 1.3046534061431885\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5215070731421744\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5260511059474352\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5582702046952954\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.524621075840183\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5321191423462103\n",
      "avgfold accuracy: 0.5325137203942596\n",
      "standard deviation: 0.013332349250320745\n",
      "avg train time: 1.2833124160766602\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5090684677760732\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5075712849067198\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5141862225507474\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5061495032241995\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5076212128113371\n",
      "avgfold accuracy: 0.5089193382538154\n",
      "standard deviation: 0.002790579631330756\n",
      "avg train time: 1.2883500099182128\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5043443458394253\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.50135988834905\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5039182390628032\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5038810321128424\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5142606627468765\n",
      "avgfold accuracy: 0.5055528336221996\n",
      "standard deviation: 0.004479605797603437\n",
      "avg train time: 1.289595651626587\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5067006213517227\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.501231838909631\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5058639839850921\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5119281961505785\n",
      "avgfold accuracy: 0.5051174847669971\n",
      "standard deviation: 0.004293238914041445\n",
      "avg train time: 1.2309067726135254\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5014646272840355\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.503302947086766\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4991967348356563\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5044699668685972\n",
      "avgfold accuracy: 0.5016316886544339\n",
      "standard deviation: 0.002022101663113353\n",
      "avg train time: 1.2868288993835448\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5032099586399312\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4989145065346707\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4992785911309145\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017830160468474\n",
      "avgfold accuracy: 0.5005823278456571\n",
      "standard deviation: 0.0016469057321841574\n",
      "avg train time: 1.2889110088348388\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5071299334616266\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.501969974466055\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5038385962970138\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5045940363475054\n",
      "avgfold accuracy: 0.5038059290966579\n",
      "standard deviation: 0.0020185432236466366\n",
      "avg train time: 1.2696490287780762\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5087431687837057\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5030456660329428\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4992835743128826\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.505779112353435\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5063947766056254\n",
      "avgfold accuracy: 0.5046492596177183\n",
      "standard deviation: 0.003238690482108392\n",
      "avg train time: 1.326719045639038\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.503391590686429\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5030628181031976\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5019048448581351\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.50138885045065\n",
      "tree depth: 7, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49872385678837294\n",
      "avgfold accuracy: 0.5016943921773569\n",
      "standard deviation: 0.0016561883267624852\n",
      "avg train time: 1.2963473320007324\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5838817320191779\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5820466572420554\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.6220265979902415\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5958417513076689\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5991081454137861\n",
      "avgfold accuracy: 0.596580976794586\n",
      "standard deviation: 0.01433129398417305\n",
      "avg train time: 1.2113287925720215\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5267430672098615\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5227310067904143\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5447079447135945\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5183636380409413\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5335548034592907\n",
      "avgfold accuracy: 0.5292200920428204\n",
      "standard deviation: 0.00921704699290059\n",
      "avg train time: 1.221663808822632\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5052310529755132\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5096001039957102\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025505172964883\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5084931595309921\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5192020540281488\n",
      "avgfold accuracy: 0.5090153775653705\n",
      "standard deviation: 0.005665660154450847\n",
      "avg train time: 1.2389982700347901\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5063868932714083\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.4988493769267509\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.503477891862471\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5059162229012653\n",
      "avgfold accuracy: 0.503222067560546\n",
      "standard deviation: 0.0028105311928186315\n",
      "avg train time: 1.3207962036132812\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5066675973432686\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.4988493769267509\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.501473722082307\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5084968284553796\n",
      "avgfold accuracy: 0.503386634701606\n",
      "standard deviation: 0.0036025592436161186\n",
      "avg train time: 1.2752328872680665\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5015141632967167\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5032686429462562\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.504873473312293\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5124244740662113\n",
      "avgfold accuracy: 0.5047490873750026\n",
      "standard deviation: 0.0040273323926485645\n",
      "avg train time: 1.2763095378875733\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5032925186610666\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4993846806704859\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5120877140520319\n",
      "avgfold accuracy: 0.5037515174846277\n",
      "standard deviation: 0.004369593552281238\n",
      "avg train time: 1.4639846324920653\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5013655552586731\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.49962265445439263\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5021870731591208\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.4995119881179716\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5046294847700507\n",
      "avgfold accuracy: 0.5014633511520418\n",
      "standard deviation: 0.00188434188462966\n",
      "avg train time: 1.320655393600464\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5069648134193558\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.504800051997855\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.501861425119522\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5038385962970138\n",
      "tree depth: 7, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.50676698504235\n",
      "avgfold accuracy: 0.5048463743752192\n",
      "standard deviation: 0.0019028512183070017\n",
      "avg train time: 1.3279261589050293\n",
      "tree depth: 7, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5013160192459919\n",
      "tree depth: 7, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5017962955116022\n",
      "tree depth: 7, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 7, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5041154826431453\n",
      "avgfold accuracy: 0.5020566992051585\n",
      "standard deviation: 0.0010407940223089974\n",
      "avg train time: 1.3570831775665284\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6063778366872716\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5870868382234788\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6154163351334215\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5885040452402697\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.6090797470685738\n",
      "avgfold accuracy: 0.6012929604706031\n",
      "standard deviation: 0.011413413035515822\n",
      "avg train time: 1.158718729019165\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5270848782064561\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.518038742013003\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5587261119507337\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5185449127759482\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.528180901815791\n",
      "avgfold accuracy: 0.5301151093523864\n",
      "standard deviation: 0.01490785716465096\n",
      "avg train time: 1.1891135692596435\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5053466370051027\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5076570452579943\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5116735013520278\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5085568132547349\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5057672999193994\n",
      "avgfold accuracy: 0.5078002593578518\n",
      "standard deviation: 0.0022692762877639314\n",
      "avg train time: 1.2598896026611328\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5043773698478794\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5066745024545515\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5045261154033877\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5017495548851927\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5004714244126751\n",
      "avgfold accuracy: 0.5035597934007372\n",
      "standard deviation: 0.002196297267640477\n",
      "avg train time: 1.2664643764495849\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5049718020000541\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5080592004595168\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5038598142049281\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.49844026940801134\n",
      "avgfold accuracy: 0.5033759294388729\n",
      "standard deviation: 0.003237715122040357\n",
      "avg train time: 1.2447064399719239\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.503143910623023\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5038385962970138\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.501091771807216\n",
      "avgfold accuracy: 0.5018628687759525\n",
      "standard deviation: 0.0014843897924453448\n",
      "avg train time: 1.3221402168273926\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5051204100380977\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5049887247706587\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5081243300674366\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016859011614498\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013399107650324\n",
      "avgfold accuracy: 0.5042518553605351\n",
      "standard deviation: 0.0025042750004807646\n",
      "avg train time: 1.2707824230194091\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5070143494320372\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4995883503138829\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4991967348356563\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5014949399902213\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5040445857980549\n",
      "avgfold accuracy: 0.5022677920739704\n",
      "standard deviation: 0.002928989714608823\n",
      "avg train time: 1.2752354145050049\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5068987654024476\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5030456660329428\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5021436534205076\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5038173783890996\n",
      "tree depth: 7, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5073518840143457\n",
      "avgfold accuracy: 0.5046514694518687\n",
      "standard deviation: 0.0020931331380064496\n",
      "avg train time: 1.3306330680847167\n",
      "tree depth: 7, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5087101447752517\n",
      "tree depth: 7, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.4989145065346707\n",
      "tree depth: 7, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.499469552302143\n",
      "tree depth: 7, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5019248097370281\n",
      "avgfold accuracy: 0.5017523464590541\n",
      "standard deviation: 0.0036261215735520144\n",
      "avg train time: 1.419467830657959\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6021276217810347\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5878023503752692\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.6315130261831486\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5777733189420566\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5931884568846183\n",
      "avgfold accuracy: 0.5984809548332255\n",
      "standard deviation: 0.01830445426338288\n",
      "avg train time: 1.2047633171081542\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5322977304500371\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5214397267223838\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5510086457592778\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5249181265509829\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5242709804162318\n",
      "avgfold accuracy: 0.5307870419797827\n",
      "standard deviation: 0.01072819619694421\n",
      "avg train time: 1.2521687984466552\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5089363717422566\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5042683378199538\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5146421298061857\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501419754794786\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5083656296847863\n",
      "avgfold accuracy: 0.5075264447695936\n",
      "standard deviation: 0.004499600477238348\n",
      "avg train time: 1.3423593521118165\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5138470668175764\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.503148578454472\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5188482208108192\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5079742433048275\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.506235258704172\n",
      "avgfold accuracy: 0.5100106736183735\n",
      "standard deviation: 0.005626887405881493\n",
      "avg train time: 1.2967333316802978\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5088092168006141\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.49880595718813775\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5120877140520319\n",
      "avgfold accuracy: 0.5045804314927921\n",
      "standard deviation: 0.0050083000206975\n",
      "avg train time: 1.3560880184173585\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5030943746103418\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5050058768409136\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.49926186444357606\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.49934224485465734\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5068024334648952\n",
      "avgfold accuracy: 0.5027013588428767\n",
      "standard deviation: 0.0030132323561513643\n",
      "avg train time: 1.3434865474700928\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5069648134193558\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5018184644693926\n",
      "avgfold accuracy: 0.501762488017461\n",
      "standard deviation: 0.002806272360568148\n",
      "avg train time: 1.4043161392211914\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5014316032755813\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.499001346011897\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.49870613257710034\n",
      "avgfold accuracy: 0.5004601740058406\n",
      "standard deviation: 0.0013171623670452793\n",
      "avg train time: 1.4161564350128173\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5070143494320372\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.4991967348356563\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5077408463177704\n",
      "tree depth: 7, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5095425569204629\n",
      "avgfold accuracy: 0.5046337196342168\n",
      "standard deviation: 0.004326126276254884\n",
      "avg train time: 1.3816091537475585\n",
      "tree depth: 7, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5012004352164023\n",
      "tree depth: 7, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.504800051997855\n",
      "tree depth: 7, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 7, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5037112888495281\n",
      "tree depth: 7, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5042218279107809\n",
      "avgfold accuracy: 0.5032328193744601\n",
      "standard deviation: 0.00132653931349139\n",
      "avg train time: 1.4802982330322265\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5964507446913907\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5928817102961169\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.6304709524564325\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5856202547993985\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5998668208706053\n",
      "avgfold accuracy: 0.6010580966227888\n",
      "standard deviation: 0.015446314586691615\n",
      "avg train time: 1.2350813388824462\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5409418272083805\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5163015081183456\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5614559318425192\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5272830007656897\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5224879643693845\n",
      "avgfold accuracy: 0.5336940464608638\n",
      "standard deviation: 0.016077504647492986\n",
      "avg train time: 1.3465943336486816\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5092831238310251\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5079314783820723\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5151848765388505\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5039331543648926\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5134559439479245\n",
      "avgfold accuracy: 0.5099577154129531\n",
      "standard deviation: 0.004010408020351774\n",
      "avg train time: 1.349784517288208\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5044269058605606\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5029427536114135\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.4985671486257653\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5078893716731704\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49750088621056365\n",
      "avgfold accuracy: 0.5022654131962947\n",
      "standard deviation: 0.003824594821850884\n",
      "avg train time: 1.384090518951416\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5071464454658537\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5031657305247269\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5078203918971443\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5008613570606723\n",
      "avgfold accuracy: 0.5041274780588036\n",
      "standard deviation: 0.0028465628633574185\n",
      "avg train time: 1.3653899669647216\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5051534340465519\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4993487039208024\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5147392164512367\n",
      "avgfold accuracy: 0.5041197908738432\n",
      "standard deviation: 0.005688544526456091\n",
      "avg train time: 1.3577861785888672\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5088257288048411\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015142569813439\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015373758060498\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.504434518446052\n",
      "avgfold accuracy: 0.5030843550793433\n",
      "standard deviation: 0.0033298947837774894\n",
      "avg train time: 1.4492188930511474\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5069648134193558\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.49969126273541215\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.49926186444357606\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5079742433048275\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5074227808594362\n",
      "avgfold accuracy: 0.5042629929525215\n",
      "standard deviation: 0.003923505477763432\n",
      "avg train time: 1.3096819877624513\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5032594946526124\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5084499781070354\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.49929980903882876\n",
      "tree depth: 7, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5016057739341214\n",
      "avgfold accuracy: 0.5028087104725334\n",
      "standard deviation: 0.0030884152162207537\n",
      "avg train time: 1.3605925559997558\n",
      "tree depth: 7, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.501117875195267\n",
      "tree depth: 7, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5030628181031976\n",
      "tree depth: 7, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.49921844470496285\n",
      "tree depth: 7, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5037537246653567\n",
      "tree depth: 7, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.504434518446052\n",
      "avgfold accuracy: 0.5023174762229672\n",
      "standard deviation: 0.0019049016172106926\n",
      "avg train time: 1.5341378688812255\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5869645357174665\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5881796959208766\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6410267878862068\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5842835266007989\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6010472137341843\n",
      "avgfold accuracy: 0.6003003519719067\n",
      "standard deviation: 0.021168341352225374\n",
      "avg train time: 1.2992848396301269\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.526632424272446\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5133710317782725\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5646850787283562\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.515394975968413\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5363586944682638\n",
      "avgfold accuracy: 0.5312884410431502\n",
      "standard deviation: 0.018635220189135895\n",
      "avg train time: 1.252217435836792\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5127242505301354\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5077942618200333\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5147072594141056\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.504569691602321\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5107867173374472\n",
      "avgfold accuracy: 0.5101164361408085\n",
      "standard deviation: 0.0035912254078434604\n",
      "avg train time: 1.2131474971771241\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5102903560888766\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5130629329876494\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5099359772691631\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5003650791450395\n",
      "avgfold accuracy: 0.506679412887381\n",
      "standard deviation: 0.005520543174204177\n",
      "avg train time: 1.2227476596832276\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5047901699535563\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49932699405149583\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5119459203618512\n",
      "avgfold accuracy: 0.5034204831397628\n",
      "standard deviation: 0.004696009166567664\n",
      "avg train time: 1.2162604331970215\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5068987654024476\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5049715727004038\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49915331509704314\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5066429155634418\n",
      "avgfold accuracy: 0.5038662504033744\n",
      "standard deviation: 0.0030062072825392363\n",
      "avg train time: 1.3235150814056396\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5089743368428848\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5024258817214933\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5060337272484063\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5041686552769631\n",
      "avgfold accuracy: 0.5046508149266261\n",
      "standard deviation: 0.0026375800529425306\n",
      "avg train time: 1.3370946884155273\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5032000346652367\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49895792627328384\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5013399107650324\n",
      "avgfold accuracy: 0.501668968646386\n",
      "standard deviation: 0.0022894897647239404\n",
      "avg train time: 1.333874797821045\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5031108866145688\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5031314263842172\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5020351040739747\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5044876910798699\n",
      "avgfold accuracy: 0.5028647403733189\n",
      "standard deviation: 0.0010158734056286561\n",
      "avg train time: 1.3257335662841796\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5069152774066746\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5062971569089442\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5049603127895194\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.505779112353435\n",
      "tree depth: 7, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5043281731784165\n",
      "avgfold accuracy: 0.505656006527398\n",
      "standard deviation: 0.000922943463398077\n",
      "avg train time: 1.3376466274261474\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.613672890191169\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5920412588536278\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6378410607389831\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5687690845856512\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5966126952881323\n",
      "avgfold accuracy: 0.6017873979315127\n",
      "standard deviation: 0.02303521355953748\n",
      "avg train time: 1.2908192157745362\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5278163474846214\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5120674744389017\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5580748158715362\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5252151772617829\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5403926853467311\n",
      "avgfold accuracy: 0.5327133000807146\n",
      "standard deviation: 0.015547549956442096\n",
      "avg train time: 1.153062915802002\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5144200458733499\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.504388402311738\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5140342534656014\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.497496286866115\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5212863816666303\n",
      "avgfold accuracy: 0.5103250740366868\n",
      "standard deviation: 0.008372561409447398\n",
      "avg train time: 1.190093421936035\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.510604084169191\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5012398238572658\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5104633723117035\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5012827609110785\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5036192047275125\n",
      "avgfold accuracy: 0.5054418491953502\n",
      "standard deviation: 0.004245972903531624\n",
      "avg train time: 1.3141580104827881\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.49940556784782536\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5112666374760472\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5037961604811853\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5065542945070788\n",
      "avgfold accuracy: 0.5044936618024922\n",
      "standard deviation: 0.00414210088417721\n",
      "avg train time: 1.2682276248931885\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5050543620211895\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5014994286664858\n",
      "avgfold accuracy: 0.5010860698637724\n",
      "standard deviation: 0.0021181398723518047\n",
      "avg train time: 1.284224796295166\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5124815115589033\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020568139432813\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4986529599432825\n",
      "avgfold accuracy: 0.5029123943257375\n",
      "standard deviation: 0.0049390972892664475\n",
      "avg train time: 1.2632262229919433\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5126961676138553\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5078855215050642\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5058427660771778\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5045585879249602\n",
      "avgfold accuracy: 0.5061451524134468\n",
      "standard deviation: 0.004231889965638239\n",
      "avg train time: 1.3185597896575927\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5066345733348144\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5031142743139623\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.502121943551201\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.503477891862471\n",
      "tree depth: 7, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5129384761931166\n",
      "avgfold accuracy: 0.5056574318511131\n",
      "standard deviation: 0.003941728923431028\n",
      "avg train time: 1.3284016132354737\n",
      "tree depth: 7, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.504955289995827\n",
      "tree depth: 7, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 7, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49904476575051016\n",
      "tree depth: 7, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5058427660771778\n",
      "tree depth: 7, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5044699668685972\n",
      "avgfold accuracy: 0.5031688395487421\n",
      "standard deviation: 0.002518864184829081\n",
      "avg train time: 1.3300956726074218\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6171586118107865\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5742818247636174\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6265036700141716\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5950045664627902\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6014548705934541\n",
      "avgfold accuracy: 0.602880708728964\n",
      "standard deviation: 0.018132101983749007\n",
      "avg train time: 1.2610759735107422\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5211438090491788\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5238507661558961\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5365184850382381\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5189268351184052\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5268338617590735\n",
      "avgfold accuracy: 0.5254547514241583\n",
      "standard deviation: 0.006133169343745563\n",
      "avg train time: 1.2400035858154297\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5032215295519843\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5045084668035221\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5102406499777932\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5088538639655348\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5147604062904118\n",
      "avgfold accuracy: 0.5083169833178492\n",
      "standard deviation: 0.004146682552878989\n",
      "avg train time: 1.2194852352142334\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5067996933770852\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5049201164896392\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5042221772330955\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5014949399902213\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5033533415584236\n",
      "avgfold accuracy: 0.5041580537296929\n",
      "standard deviation: 0.0017494483683956898\n",
      "avg train time: 1.2927131175994873\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5127787276349907\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5138879080212997\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4985288904643743\n",
      "avgfold accuracy: 0.5049173122054469\n",
      "standard deviation: 0.006894042670907455\n",
      "avg train time: 1.3563056468963623\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5050378500169623\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4993052841821892\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4989719957461893\n",
      "avgfold accuracy: 0.5013335758562387\n",
      "standard deviation: 0.002177703934469815\n",
      "avg train time: 1.2678210258483886\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5011013631910399\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499001346011897\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015798116218785\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013930833988502\n",
      "avgfold accuracy: 0.5009316938972058\n",
      "standard deviation: 0.0009810190244485884\n",
      "avg train time: 1.198451280593872\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5012995072417648\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5032514908760013\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.510962699305755\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49925737322300023\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5018184644693926\n",
      "avgfold accuracy: 0.5033179070231828\n",
      "standard deviation: 0.004031313943131997\n",
      "avg train time: 1.2642942905426025\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5069483014151288\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5048951831815996\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5035839814020424\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.504345897389689\n",
      "avgfold accuracy: 0.504230080761553\n",
      "standard deviation: 0.0018111754094447155\n",
      "avg train time: 1.3622183799743652\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5010518271783587\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5045912450113075\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49953320602588586\n",
      "tree depth: 7, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5040623100093276\n",
      "avgfold accuracy: 0.5017996918482621\n",
      "standard deviation: 0.0021339793147255905\n",
      "avg train time: 1.2491838455200195\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6196023884363933\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5854304898811813\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6273503549171284\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.579119733576878\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.6157758352658335\n",
      "avgfold accuracy: 0.6054557604154829\n",
      "standard deviation: 0.019393824106070566\n",
      "avg train time: 1.1948768138885497\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5167829512055264\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5295770299475147\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5631436780075886\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5189153036467126\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5203681883083577\n",
      "avgfold accuracy: 0.52975743022314\n",
      "standard deviation: 0.01725526449287936\n",
      "avg train time: 1.1669204711914063\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035352576322987\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5027883849791196\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5259956067594593\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5045060378785782\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.505607782017946\n",
      "avgfold accuracy: 0.5084866138534803\n",
      "standard deviation: 0.008805489550780355\n",
      "avg train time: 1.3668198585510254\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5118705674025016\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.499433981681589\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5130195132490363\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5098723235454202\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5057567049998118\n",
      "avgfold accuracy: 0.5079906181756717\n",
      "standard deviation: 0.004941477571119336\n",
      "avg train time: 1.2257739543914794\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5069152774066746\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5013255842085402\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.49932699405149583\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.49921493740717165\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5063593281830802\n",
      "avgfold accuracy: 0.5026284242513925\n",
      "standard deviation: 0.0033628999420188726\n",
      "avg train time: 1.2084037780761718\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5032925186610666\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.503302947086766\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5022739126363471\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5036051993099567\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.4988479262672811\n",
      "avgfold accuracy: 0.5022645007922836\n",
      "standard deviation: 0.0017666767688548606\n",
      "avg train time: 1.2581430912017821\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5067996933770852\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5032000346652367\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49934224485465734\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5042218279107809\n",
      "avgfold accuracy: 0.5025868429195738\n",
      "standard deviation: 0.002886871038460317\n",
      "avg train time: 1.3712043285369873\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5051699460507789\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5047649239657601\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49822757887274016\n",
      "avgfold accuracy: 0.5019325708791935\n",
      "standard deviation: 0.002727505116488838\n",
      "avg train time: 1.2820188045501708\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5053846021057309\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5033372512272758\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49921844470496285\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.507634756778199\n",
      "tree depth: 7, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5066429155634418\n",
      "avgfold accuracy: 0.5044435940759221\n",
      "standard deviation: 0.0029810136181379772\n",
      "avg train time: 1.2938676834106446\n",
      "tree depth: 7, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5014481152798085\n",
      "tree depth: 7, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 7, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5049386029202129\n",
      "tree depth: 7, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5016859011614498\n",
      "tree depth: 7, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5068378818874404\n",
      "avgfold accuracy: 0.5029237832109157\n",
      "standard deviation: 0.002585660793812835\n",
      "avg train time: 1.2384943962097168\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6143779652807592\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5779669921949053\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.6344165250293867\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5854389800643918\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5975095007713498\n",
      "avgfold accuracy: 0.6019419926681585\n",
      "standard deviation: 0.020376681398155398\n",
      "avg train time: 1.1388288974761962\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5237312526206552\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5350974688960747\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5496031278951942\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5230509506545263\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5364721690275844\n",
      "avgfold accuracy: 0.533590993818807\n",
      "standard deviation: 0.009749523904496385\n",
      "avg train time: 1.2380815982818603\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5071910403863609\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.500793870030639\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5120425691302398\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5043787304310925\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5085960444313301\n",
      "avgfold accuracy: 0.5066004508819325\n",
      "standard deviation: 0.0038105186930386965\n",
      "avg train time: 1.2565043449401856\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5098115079662915\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.499571198243628\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5132366119421021\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5036688530336996\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5036546531500578\n",
      "avgfold accuracy: 0.5059885648671558\n",
      "standard deviation: 0.0048818005205891035\n",
      "avg train time: 1.3332153797149657\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5011508992037211\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5050905720053589\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5066074671408966\n",
      "avgfold accuracy: 0.502436712497701\n",
      "standard deviation: 0.0028785666681513788\n",
      "avg train time: 1.27532000541687\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5070638854447184\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5023607521135734\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5037961604811853\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5044876910798699\n",
      "avgfold accuracy: 0.5034833807850028\n",
      "standard deviation: 0.0024264025718756014\n",
      "avg train time: 1.3186575889587402\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.503143910623023\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4989188231123715\n",
      "avgfold accuracy: 0.5005671190335879\n",
      "standard deviation: 0.001608398510188802\n",
      "avg train time: 1.3075101852416993\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5087266567794787\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.49942711648631444\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5019070855257556\n",
      "avgfold accuracy: 0.5018519503758218\n",
      "standard deviation: 0.0035608118861904575\n",
      "avg train time: 1.3073413848876954\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5068162053813122\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5017528757729891\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.4994058985784002\n",
      "tree depth: 7, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5040268615867823\n",
      "avgfold accuracy: 0.5023489120531321\n",
      "standard deviation: 0.0027778068467019627\n",
      "avg train time: 1.2849457263946533\n",
      "tree depth: 7, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5011839232121753\n",
      "tree depth: 7, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.50135988834905\n",
      "tree depth: 7, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.4990230558812036\n",
      "tree depth: 7, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 7, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5042750005445987\n",
      "avgfold accuracy: 0.5010919891289142\n",
      "standard deviation: 0.0018256332243424202\n",
      "avg train time: 1.3718300342559815\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6083873601108005\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6027121936775663\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.6402669424604763\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5932974473934262\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5987607904800192\n",
      "avgfold accuracy: 0.6086849468244577\n",
      "standard deviation: 0.01654357158686238\n",
      "avg train time: 1.1208213806152343\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5311914261668232\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5140448373171274\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5466996290489561\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5220015867305049\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5379893219053428\n",
      "avgfold accuracy: 0.5303853602337509\n",
      "standard deviation: 0.011505874062483937\n",
      "avg train time: 1.1406389236450196\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5095142918902041\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.507691349398504\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.522679620396396\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5078778402014779\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5110348562952636\n",
      "avgfold accuracy: 0.5117595916363691\n",
      "standard deviation: 0.005593428814114307\n",
      "avg train time: 1.2108673572540283\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5152456460847036\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5078855215050642\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5081227686602274\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5062884313379898\n",
      "avgfold accuracy: 0.5078010336717129\n",
      "standard deviation: 0.004427054725333113\n",
      "avg train time: 1.2682682037353517\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5069483014151288\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5135188402430878\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5016057739341214\n",
      "avgfold accuracy: 0.5046224493848498\n",
      "standard deviation: 0.005087997817426911\n",
      "avg train time: 1.2874626636505127\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5015471873051709\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.499433981681589\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.502121943551201\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.505779112353435\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.501446256032668\n",
      "avgfold accuracy: 0.5020656961848128\n",
      "standard deviation: 0.002067418786528046\n",
      "avg train time: 1.3607483386993409\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5049222659873729\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.505351090437038\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499469552302143\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4986175115207373\n",
      "avgfold accuracy: 0.5016274886667955\n",
      "standard deviation: 0.002893497619573722\n",
      "avg train time: 1.2915093421936035\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.504955289995827\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5031828825949818\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5022956225056537\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5044876910798699\n",
      "avgfold accuracy: 0.5032960159780593\n",
      "standard deviation: 0.0012810544250800439\n",
      "avg train time: 1.302048397064209\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5068657413939934\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4989145065346707\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4992785911309145\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5016943949904844\n",
      "avgfold accuracy: 0.5016329157219755\n",
      "standard deviation: 0.002841405665644575\n",
      "avg train time: 1.2813457012176515\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5012664832333107\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5050688621360523\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5060761630642349\n",
      "tree depth: 7, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.49868840836582773\n",
      "avgfold accuracy: 0.5021616663210186\n",
      "standard deviation: 0.002920903587624551\n",
      "avg train time: 1.3474297523498535\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.594721925339722\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5885962204059083\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6378844804775962\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.6080577311598816\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5994697589309231\n",
      "avgfold accuracy: 0.6057460232628062\n",
      "standard deviation: 0.01728156470091852\n",
      "avg train time: 1.2005587577819825\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5345499928447982\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5188277372447275\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5583136244339086\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5231685716657902\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5343098152523273\n",
      "avgfold accuracy: 0.5338339482883103\n",
      "standard deviation: 0.013703505695641814\n",
      "avg train time: 1.214548110961914\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5093656838521605\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5028226891196294\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5178495668227162\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5021835994797\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5107158204923569\n",
      "avgfold accuracy: 0.5085874719533127\n",
      "standard deviation: 0.0057474200544385965\n",
      "avg train time: 1.2711663722991944\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5122008074870431\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5127589948173572\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5034354560466425\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5008968054832175\n",
      "avgfold accuracy: 0.5061509729209679\n",
      "standard deviation: 0.0052388112607800906\n",
      "avg train time: 1.1782220363616944\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5013160192459919\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5012055197167561\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.504504405534081\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5060761630642349\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5039205163191467\n",
      "avgfold accuracy: 0.5034045247760421\n",
      "standard deviation: 0.001887385212255999\n",
      "avg train time: 1.219593620300293\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5070473734404912\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5017475676243022\n",
      "avgfold accuracy: 0.5020954201875558\n",
      "standard deviation: 0.0027410169434835705\n",
      "avg train time: 1.290583038330078\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5068987654024476\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5051991213518918\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49929980903882876\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49872385678837294\n",
      "avgfold accuracy: 0.5023237314985259\n",
      "standard deviation: 0.0032244478137191173\n",
      "avg train time: 1.3457191467285157\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5032429826483854\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5012912800680305\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.504982022658826\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5014100683585642\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.501534877089031\n",
      "avgfold accuracy: 0.5024922461645673\n",
      "standard deviation: 0.00143475267381723\n",
      "avg train time: 1.353139877319336\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5069483014151288\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5020133942046681\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.4993210269467431\n",
      "tree depth: 7, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.4990074441687345\n",
      "avgfold accuracy: 0.5017437326730688\n",
      "standard deviation: 0.0028501370040958374\n",
      "avg train time: 1.3239126205444336\n",
      "tree depth: 7, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5015471873051709\n",
      "tree depth: 7, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.499433981681589\n",
      "tree depth: 7, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5048951831815996\n",
      "tree depth: 7, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.50138885045065\n",
      "tree depth: 7, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5038496194740564\n",
      "avgfold accuracy: 0.5022229644186131\n",
      "standard deviation: 0.001934818095464201\n",
      "avg train time: 1.3287015914916993\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.630151995500729\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5795032760454187\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6090883005775871\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5760874177806068\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.6079028198328973\n",
      "avgfold accuracy: 0.6005467619474477\n",
      "standard deviation: 0.02022190465564234\n",
      "avg train time: 1.2319472312927247\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5206368780103136\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5240394389286998\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5585958527348942\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5109622782497993\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5271528975619802\n",
      "avgfold accuracy: 0.5282774690971375\n",
      "standard deviation: 0.016103567390415484\n",
      "avg train time: 1.1598464488983153\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5134838027245808\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5044570105927575\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5031366837677661\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.505100139300178\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5156111684314965\n",
      "avgfold accuracy: 0.5083577609633558\n",
      "standard deviation: 0.005137622474902886\n",
      "avg train time: 1.2390687942504883\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5106371081776452\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.50135988834905\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5016877461650693\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.503477891862471\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.502892512065336\n",
      "avgfold accuracy: 0.5040110293239144\n",
      "standard deviation: 0.00340195327690339\n",
      "avg train time: 1.255990982055664\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5070473734404912\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5169867956912971\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5040623100093276\n",
      "avgfold accuracy: 0.5058508195051152\n",
      "standard deviation: 0.006097789643614742\n",
      "avg train time: 1.3609181880950927\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5050708740254165\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49953320602588586\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5068201576761677\n",
      "avgfold accuracy: 0.5030303671072586\n",
      "standard deviation: 0.0025979610613547136\n",
      "avg train time: 1.284546184539795\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5087761927921599\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5033372512272758\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49893621640397723\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4987770294221907\n",
      "avgfold accuracy: 0.5022940310382451\n",
      "standard deviation: 0.003665356314809486\n",
      "avg train time: 1.335472822189331\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5049883140042811\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.501861425119522\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5039234679286709\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5014108076101228\n",
      "avgfold accuracy: 0.5027362239147372\n",
      "standard deviation: 0.00145181784931121\n",
      "avg train time: 1.3954771995544433\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49930649582246295\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49908818548912337\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5014949399902213\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5071037450565293\n",
      "avgfold accuracy: 0.5013334954046988\n",
      "standard deviation: 0.003007508837421227\n",
      "avg train time: 1.3531217575073242\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5011508992037211\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5011712155762463\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5083197188911959\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014100683585642\n",
      "tree depth: 7, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5064656734507158\n",
      "avgfold accuracy: 0.5037035150960886\n",
      "standard deviation: 0.003070086088174774\n",
      "avg train time: 1.5662458896636964\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6231376460686919\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5826935611128263\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.6021363034527062\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5645291930737368\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5993279652407424\n",
      "avgfold accuracy: 0.5943649337897406\n",
      "standard deviation: 0.019694096596419127\n",
      "avg train time: 1.1340840339660645\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5219363852520783\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5138218604038138\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5361277073907196\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5375469330897886\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5256534688954945\n",
      "avgfold accuracy: 0.527017271006379\n",
      "standard deviation: 0.008895707413355151\n",
      "avg train time: 1.2523944854736329\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5070589443525443\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5063829172602186\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5139257041190685\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5094691832950489\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5158770316005854\n",
      "avgfold accuracy: 0.510542756125493\n",
      "standard deviation: 0.0037547994473223242\n",
      "avg train time: 1.270133113861084\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5136984587795327\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5042221772330955\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5032869306912425\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5053667723518147\n",
      "avgfold accuracy: 0.5056142887933548\n",
      "standard deviation: 0.00423605576465458\n",
      "avg train time: 1.2357289791107178\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.49940556784782536\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4991750249663497\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5040083395603281\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.508744967413196\n",
      "avgfold accuracy: 0.5021981716765203\n",
      "standard deviation: 0.003729156963356917\n",
      "avg train time: 1.2753613948822022\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5069483014151288\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5024041718521867\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.501473722082307\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5018007402581199\n",
      "avgfold accuracy: 0.5024670700826819\n",
      "standard deviation: 0.0024135577500716634\n",
      "avg train time: 1.2309381484985351\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5030778626061146\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5033544032975307\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5018397152502154\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.50138885045065\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5071746419016198\n",
      "avgfold accuracy: 0.5033670947012261\n",
      "standard deviation: 0.0020409591011141385\n",
      "avg train time: 1.2920101642608643\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5012499712290835\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4996398065246475\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4987770294221907\n",
      "avgfold accuracy: 0.49965349454888186\n",
      "standard deviation: 0.0008531231518423201\n",
      "avg train time: 1.2919087886810303\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5028962305596169\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5048858123491294\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4991967348356563\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5057366765376065\n",
      "tree depth: 7, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5014994286664858\n",
      "avgfold accuracy: 0.5028429765896989\n",
      "standard deviation: 0.0023515830100055714\n",
      "avg train time: 1.3573178768157959\n",
      "tree depth: 7, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.49932300782669\n",
      "tree depth: 7, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5031828825949818\n",
      "tree depth: 7, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5017311659036825\n",
      "tree depth: 7, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.50138885045065\n",
      "tree depth: 7, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5041154826431453\n",
      "avgfold accuracy: 0.5019482778838299\n",
      "standard deviation: 0.0016415337218242635\n",
      "avg train time: 1.3898922443389892\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5731091503523061\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5670781358046758\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.590783611193504\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5395492578344818\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5625864674161665\n",
      "avgfold accuracy: 0.5666213245202268\n",
      "standard deviation: 0.016584893875730606\n",
      "avg train time: 1.5024214744567872\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.512410522449821\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.505902659293082\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5179146964306359\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5125227169992344\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5213750027229933\n",
      "avgfold accuracy: 0.5140251195791533\n",
      "standard deviation: 0.005291277517059448\n",
      "avg train time: 1.503974485397339\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.503143910623023\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5052859608291183\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017495548851927\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5066960881972595\n",
      "avgfold accuracy: 0.5036710934750854\n",
      "standard deviation: 0.0020263128697871418\n",
      "avg train time: 1.4727652072906494\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5050873860296435\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5071923661128923\n",
      "avgfold accuracy: 0.5036589273504252\n",
      "standard deviation: 0.00215294290598711\n",
      "avg train time: 1.410239839553833\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5052690180761413\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5026212705452525\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5021906729061171\n",
      "avgfold accuracy: 0.5023691782087447\n",
      "standard deviation: 0.0017363851343092367\n",
      "avg train time: 1.5104305744171143\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.507212493482762\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5057635779538631\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.49929103154909604\n",
      "avgfold accuracy: 0.5023309921548498\n",
      "standard deviation: 0.003430563019363751\n",
      "avg train time: 1.4522951602935792\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49943554339802876\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5048244510940493\n",
      "avgfold accuracy: 0.5022415981011493\n",
      "standard deviation: 0.001814892844850046\n",
      "avg train time: 1.3682989120483398\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5070803974489454\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5020568139432813\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.4993846806704859\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5132220635734782\n",
      "avgfold accuracy: 0.5043179174007795\n",
      "standard deviation: 0.0052223724670986725\n",
      "avg train time: 1.4368955612182617\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5032857950165112\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.4994058985784002\n",
      "tree depth: 8, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.4989188231123715\n",
      "avgfold accuracy: 0.5011945836275253\n",
      "standard deviation: 0.002590918946810217\n",
      "avg train time: 1.3583734035491943\n",
      "tree depth: 8, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5052194820634601\n",
      "tree depth: 8, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 8, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5139964573678326\n",
      "tree depth: 8, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5037749425732709\n",
      "tree depth: 8, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5072632629579827\n",
      "avgfold accuracy: 0.5063296674904212\n",
      "standard deviation: 0.004284140802777219\n",
      "avg train time: 1.3883634090423584\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5622738981239361\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5574903090803061\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5893451361784248\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5363781031190324\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5652485647349587\n",
      "avgfold accuracy: 0.5621472022473316\n",
      "standard deviation: 0.016941636491780335\n",
      "avg train time: 1.325733232498169\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.51825746067391\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5009653907331878\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5176324681296504\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5069982195407706\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5109994078727184\n",
      "avgfold accuracy: 0.5109705893900475\n",
      "standard deviation: 0.006532437130755518\n",
      "avg train time: 1.4432984352111817\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5029953025849793\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050744851219332\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.50810262019813\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5012335654973968\n",
      "avgfold accuracy: 0.5038268620759435\n",
      "standard deviation: 0.002516313045126565\n",
      "avg train time: 1.3873740673065185\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5019248097370281\n",
      "avgfold accuracy: 0.5012671352587038\n",
      "standard deviation: 0.0021353167482987094\n",
      "avg train time: 1.4481661796569825\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5033420546737478\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5056550286073301\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5020488792159363\n",
      "avgfold accuracy: 0.5021273957939498\n",
      "standard deviation: 0.0022273831341885794\n",
      "avg train time: 1.3955326557159424\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5083848484991156\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.4991137894363701\n",
      "avgfold accuracy: 0.5017270672279686\n",
      "standard deviation: 0.0034191171927619324\n",
      "avg train time: 1.4079331398010253\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5054836741310933\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5055898989994104\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5043104489671439\n",
      "avgfold accuracy: 0.5034101987499217\n",
      "standard deviation: 0.0022463018129585564\n",
      "avg train time: 1.3854385375976563\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5108682762368242\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5020351040739747\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5015373758060498\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5045940363475054\n",
      "avgfold accuracy: 0.5037623631102081\n",
      "standard deviation: 0.0038730507128528343\n",
      "avg train time: 1.3512099266052247\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5013160192459919\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5054162200449577\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 8, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5073164355918005\n",
      "avgfold accuracy: 0.503451570684096\n",
      "standard deviation: 0.002456961994245495\n",
      "avg train time: 1.3909071445465089\n",
      "tree depth: 8, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5049057539831457\n",
      "tree depth: 8, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 8, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 8, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.4993846806704859\n",
      "tree depth: 8, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5045763121362329\n",
      "avgfold accuracy: 0.5015370113907923\n",
      "standard deviation: 0.0026249937489460358\n",
      "avg train time: 1.4572583198547364\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5699322157208291\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5482209692183531\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5695907244975734\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5492615245528095\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5649012098011917\n",
      "avgfold accuracy: 0.5603813287581514\n",
      "standard deviation: 0.009674565892118053\n",
      "avg train time: 1.496424102783203\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5126912265216813\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5074683724851905\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5202103189362897\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5088538639655348\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5091738141116408\n",
      "avgfold accuracy: 0.5116795192040674\n",
      "standard deviation: 0.00459988560712034\n",
      "avg train time: 1.316957378387451\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5013655552586731\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49913160522773653\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016859011614498\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.504345897389689\n",
      "avgfold accuracy: 0.5015914911335236\n",
      "standard deviation: 0.00165676310766307\n",
      "avg train time: 1.4938397884368897\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.50699783742781\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5084065583684222\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5046649331925959\n",
      "avgfold accuracy: 0.5038963163606625\n",
      "standard deviation: 0.0036244551753053075\n",
      "avg train time: 1.460699701309204\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5072455174912162\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5019602581595733\n",
      "avgfold accuracy: 0.5021435462492165\n",
      "standard deviation: 0.0027294110475510978\n",
      "avg train time: 1.507634449005127\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5014646272840355\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5046472089813232\n",
      "avgfold accuracy: 0.5010468702422904\n",
      "standard deviation: 0.001926185422347961\n",
      "avg train time: 1.5414072036743165\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034741507075644\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49930875576036865\n",
      "avgfold accuracy: 0.500783033571876\n",
      "standard deviation: 0.0016301944431493447\n",
      "avg train time: 1.4538581848144532\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5016627713347604\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.4989365473236441\n",
      "avgfold accuracy: 0.500735064231059\n",
      "standard deviation: 0.0012471546147833897\n",
      "avg train time: 1.4621068954467773\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.49940556784782536\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.508548952911248\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5020785238125879\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 8, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.49934420418291386\n",
      "avgfold accuracy: 0.5017863345376752\n",
      "standard deviation: 0.0035336060188932783\n",
      "avg train time: 1.6636225700378418\n",
      "tree depth: 8, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5085780487414351\n",
      "tree depth: 8, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 8, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.4989145065346707\n",
      "tree depth: 8, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5038173783890996\n",
      "tree depth: 8, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5102338011600942\n",
      "avgfold accuracy: 0.5042504299261934\n",
      "standard deviation: 0.004556720956165176\n",
      "avg train time: 1.3508306980133056\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5541697564129122\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5349062684497597\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5834786325188732\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5582869768171294\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5647345626080533\n",
      "avgfold accuracy: 0.5591152393613456\n",
      "standard deviation: 0.01572494714097736\n",
      "avg train time: 1.323802375793457\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5152291340804765\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.49931391718980483\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5174587891751976\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5062768106716852\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5074439706986111\n",
      "avgfold accuracy: 0.509144524363155\n",
      "standard deviation: 0.006540733548400432\n",
      "avg train time: 1.3558494567871093\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5051038980338707\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4991098953584299\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49810350939383197\n",
      "avgfold accuracy: 0.5006838798244339\n",
      "standard deviation: 0.00246455568995895\n",
      "avg train time: 1.3407018661499024\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5012004352164023\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5055464792607972\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5018132086089355\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5015526013003035\n",
      "avgfold accuracy: 0.5019951015648799\n",
      "standard deviation: 0.0018983914385827252\n",
      "avg train time: 1.4365200042724608\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.49948599787309467\n",
      "avgfold accuracy: 0.501431492689453\n",
      "standard deviation: 0.0023053120723963046\n",
      "avg train time: 1.4648513317108154\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.503589734737154\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5018007402581199\n",
      "avgfold accuracy: 0.5015176209386643\n",
      "standard deviation: 0.0014820508185354518\n",
      "avg train time: 1.4204636573791505\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015141632967167\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022087830284274\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020488792159363\n",
      "avgfold accuracy: 0.5014913675965826\n",
      "standard deviation: 0.0008235804246416429\n",
      "avg train time: 1.4206553936004638\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5071299334616266\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.49943554339802876\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.49920241049273306\n",
      "avgfold accuracy: 0.5010386452798936\n",
      "standard deviation: 0.0030524984118088985\n",
      "avg train time: 1.4468061923980713\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5069648134193558\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5082111695446629\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5082925119235417\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.4989897199574619\n",
      "avgfold accuracy: 0.5044264651020359\n",
      "standard deviation: 0.004191815558559953\n",
      "avg train time: 1.4318406105041503\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5032264706441584\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5015142569813439\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.49915331509704314\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 8, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5042750005445987\n",
      "avgfold accuracy: 0.5015574241849374\n",
      "standard deviation: 0.001985628677316428\n",
      "avg train time: 1.326059055328369\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5680993832516239\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5515925245861386\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5666816020104909\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5399196487052463\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5458692685940839\n",
      "avgfold accuracy: 0.5544324854295167\n",
      "standard deviation: 0.0112146461651013\n",
      "avg train time: 1.245973825454712\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5031719935393031\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5042854898902087\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5284866180888723\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5071043090803421\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5196451593099638\n",
      "avgfold accuracy: 0.5125387139817381\n",
      "standard deviation: 0.00990172840465196\n",
      "avg train time: 1.3038008689880372\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050213380127353\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5099423354483454\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5060549451563207\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5034951352486043\n",
      "avgfold accuracy: 0.5048375729062327\n",
      "standard deviation: 0.0033477707986252552\n",
      "avg train time: 1.3951392650604248\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5052194820634601\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.5016766707792117\n",
      "avgfold accuracy: 0.5011860514448389\n",
      "standard deviation: 0.0021671212093179997\n",
      "avg train time: 1.4259312152862549\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5017029297541477\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5020134307933911\n",
      "avgfold accuracy: 0.5026720835992841\n",
      "standard deviation: 0.0013275154172732388\n",
      "avg train time: 1.386975383758545\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5020488792159363\n",
      "avgfold accuracy: 0.5005583696192452\n",
      "standard deviation: 0.0010417543256905056\n",
      "avg train time: 1.427020215988159\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5089248008302035\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5040083395603281\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4991492378589153\n",
      "avgfold accuracy: 0.5029052982334294\n",
      "standard deviation: 0.003480153384644853\n",
      "avg train time: 1.516965627670288\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5053515780972767\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5022956225056537\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5018132086089355\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4991137894363701\n",
      "avgfold accuracy: 0.5020519952664257\n",
      "standard deviation: 0.0019885064231560863\n",
      "avg train time: 1.4182043552398682\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5018132086089355\n",
      "tree depth: 8, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5015880497228488\n",
      "avgfold accuracy: 0.5012270069305481\n",
      "standard deviation: 0.0014243068905698648\n",
      "avg train time: 1.4325714588165284\n",
      "tree depth: 8, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5033090306652936\n",
      "tree depth: 8, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 8, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5106153413968496\n",
      "tree depth: 8, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.4993846806704859\n",
      "tree depth: 8, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5100565590473682\n",
      "avgfold accuracy: 0.5049485304398604\n",
      "standard deviation: 0.004573939820576396\n",
      "avg train time: 1.4656565189361572\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5665191719379989\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5582155708299255\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5929923942219313\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5436212511185528\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5676342039650744\n",
      "avgfold accuracy: 0.5657965184146965\n",
      "standard deviation: 0.016079354608676405\n",
      "avg train time: 1.295971155166626\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5076203524962648\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5060913320658856\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5139257041190685\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5000945580678788\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5130482870886547\n",
      "avgfold accuracy: 0.5081560467675504\n",
      "standard deviation: 0.005035088410740294\n",
      "avg train time: 1.3491398334503173\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5086771207667975\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015142569813439\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5046780844885338\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5034566739545567\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5065720187183513\n",
      "avgfold accuracy: 0.5049796309819167\n",
      "standard deviation: 0.0024749252421496664\n",
      "avg train time: 1.407590627670288\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5069483014151288\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5023607521135734\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.4988479262672811\n",
      "avgfold accuracy: 0.5015148374335489\n",
      "standard deviation: 0.0029638833022946937\n",
      "avg train time: 1.4165638446807862\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5027081100224788\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.4990251683800071\n",
      "avgfold accuracy: 0.5009557015613852\n",
      "standard deviation: 0.0017691343298733083\n",
      "avg train time: 1.4381547927856446\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4993796526054591\n",
      "avgfold accuracy: 0.5008190167993349\n",
      "standard deviation: 0.0023226697166213277\n",
      "avg train time: 1.3955718994140625\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.507195981478535\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4992378589152783\n",
      "avgfold accuracy: 0.5011578260438101\n",
      "standard deviation: 0.0030267388743666543\n",
      "avg train time: 1.3957274913787843\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5089743368428848\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.4989365473236441\n",
      "avgfold accuracy: 0.501933747962651\n",
      "standard deviation: 0.0036897959456929637\n",
      "avg train time: 1.4005763530731201\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5090403848597931\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5024693014601064\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.506012509340492\n",
      "tree depth: 8, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5047890026715041\n",
      "avgfold accuracy: 0.5047376477502401\n",
      "standard deviation: 0.0027048302187331875\n",
      "avg train time: 1.4453398704528808\n",
      "tree depth: 8, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5050543620211895\n",
      "tree depth: 8, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 8, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.4994789631366419\n",
      "tree depth: 8, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5014100683585642\n",
      "tree depth: 8, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5099679379910053\n",
      "avgfold accuracy: 0.5031239492626136\n",
      "standard deviation: 0.003961208461346242\n",
      "avg train time: 1.4280983924865722\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5624885541788881\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5502840924478531\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5937249061375108\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5437794629101744\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5570459185795282\n",
      "avgfold accuracy: 0.5614645868507908\n",
      "standard deviation: 0.017315128661091007\n",
      "avg train time: 1.303095769882202\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5089528837464837\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.51099429648527\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5300714385482532\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5065950792903994\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5088973560229643\n",
      "avgfold accuracy: 0.5131022108186741\n",
      "standard deviation: 0.008598121565860425\n",
      "avg train time: 1.329801368713379\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5033255426695208\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5111797979988209\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014949399902213\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5116091603476718\n",
      "avgfold accuracy: 0.5058453220818218\n",
      "standard deviation: 0.004578883107701994\n",
      "avg train time: 1.3851024150848388\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5053515780972767\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4991750249663497\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5042750005445987\n",
      "avgfold accuracy: 0.5016760845135962\n",
      "standard deviation: 0.0025946556993286627\n",
      "avg train time: 1.4102414131164551\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5053680901015039\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5034401636488051\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5024475915907999\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5048421753053218\n",
      "avgfold accuracy: 0.5031983862213718\n",
      "standard deviation: 0.0019468624212484297\n",
      "avg train time: 1.3393083572387696\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5054176261141851\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49943554339802876\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5019248097370281\n",
      "avgfold accuracy: 0.5016220590908587\n",
      "standard deviation: 0.002131887894552186\n",
      "avg train time: 1.3912188053131103\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5090568968640201\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5032686429462562\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5018132086089355\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49929103154909604\n",
      "avgfold accuracy: 0.5032058681288507\n",
      "standard deviation: 0.003220917032446564\n",
      "avg train time: 1.395819330215454\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5014976512924897\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5017298434130296\n",
      "avgfold accuracy: 0.5008166165010887\n",
      "standard deviation: 0.0009957693482176126\n",
      "avg train time: 1.428844690322876\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5072785414996703\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5030799701734525\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5022739126363471\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5018980802405927\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5017652918355747\n",
      "avgfold accuracy: 0.5032591592771275\n",
      "standard deviation: 0.0020611855426217306\n",
      "avg train time: 1.4236225605010986\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5033255426695208\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5032514908760013\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5021653632898142\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 8, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5072455387467101\n",
      "avgfold accuracy: 0.5035517416750307\n",
      "standard deviation: 0.0019431597219237882\n",
      "avg train time: 1.4330373764038087\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.56202621806053\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5506148565816105\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5962376273362304\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5471627967047666\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5481768816874242\n",
      "avgfold accuracy: 0.5608436760741123\n",
      "standard deviation: 0.018473619461590308\n",
      "avg train time: 1.3277125358581543\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5075543044793566\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5163432147312812\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5291596240373766\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5102427144161846\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5109816836614458\n",
      "avgfold accuracy: 0.514856308265129\n",
      "standard deviation: 0.007699518897043262\n",
      "avg train time: 1.296645736694336\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5032594946526124\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5021870731591208\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5037255499951482\n",
      "avgfold accuracy: 0.5020701908198513\n",
      "standard deviation: 0.0014283896552122987\n",
      "avg train time: 1.3965567111968995\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5053185540888226\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5045585879249602\n",
      "avgfold accuracy: 0.5027303549399004\n",
      "standard deviation: 0.0020619057323258178\n",
      "avg train time: 1.4247656345367432\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.499768831940821\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4993052841821892\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4995214462956398\n",
      "avgfold accuracy: 0.49963406310814945\n",
      "standard deviation: 0.00020887282378741722\n",
      "avg train time: 1.4641935348510742\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5073941255292598\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.501760514179977\n",
      "standard deviation: 0.0029666721333029646\n",
      "avg train time: 1.4215840339660644\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025561409373328\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017652918355747\n",
      "avgfold accuracy: 0.5018176430722436\n",
      "standard deviation: 0.0020417461468732858\n",
      "avg train time: 1.3921610832214355\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.502491011329413\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5013399107650324\n",
      "avgfold accuracy: 0.501390159522509\n",
      "standard deviation: 0.0008783615120592036\n",
      "avg train time: 1.5017770290374757\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5090403848597931\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.501222671787011\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5021653632898142\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4988833746898263\n",
      "avgfold accuracy: 0.502173243712049\n",
      "standard deviation: 0.003626092437958335\n",
      "avg train time: 1.4333812236785888\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5051038980338707\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5021436534205076\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49934224485465734\n",
      "tree depth: 8, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5045054152911425\n",
      "avgfold accuracy: 0.5025116024741514\n",
      "standard deviation: 0.0020964486665728975\n",
      "avg train time: 1.4273112297058106\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5687598634207068\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5480322964455494\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5826102377466098\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5406410575743319\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5678468945003455\n",
      "avgfold accuracy: 0.5615780699375088\n",
      "standard deviation: 0.015199176740708083\n",
      "avg train time: 1.369236993789673\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.518488628733089\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5041139691876599\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.535270559900145\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5044211662469211\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5187235003237887\n",
      "avgfold accuracy: 0.5162035648783208\n",
      "standard deviation: 0.011490040314157464\n",
      "avg train time: 1.2902947902679442\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.50316042262725\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5032343388057464\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017745856422956\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5037078257838755\n",
      "avgfold accuracy: 0.5027211019672893\n",
      "standard deviation: 0.0008138198896202088\n",
      "avg train time: 1.3229604244232178\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.506766669368631\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5115054460384196\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5043281731784165\n",
      "avgfold accuracy: 0.5044170431037261\n",
      "standard deviation: 0.00446091413081631\n",
      "avg train time: 1.4025821685791016\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.5012245451992908\n",
      "standard deviation: 0.002260900678781296\n",
      "avg train time: 1.3717842102050781\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5026864001531723\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5010196196100374\n",
      "standard deviation: 0.001746684031480216\n",
      "avg train time: 1.3697646141052247\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5054176261141851\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5033887074380404\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025778508066393\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502066603427209\n",
      "avgfold accuracy: 0.5026434781598033\n",
      "standard deviation: 0.0018365601306330507\n",
      "avg train time: 1.3958553791046142\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5014481152798085\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.4993487039208024\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5016766707792117\n",
      "avgfold accuracy: 0.5003678482281637\n",
      "standard deviation: 0.0009889411710922737\n",
      "avg train time: 1.3924367904663086\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5070969094531724\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.49926186444357606\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 8, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.5009742193846909\n",
      "standard deviation: 0.003066780372076766\n",
      "avg train time: 1.4220032215118408\n",
      "tree depth: 8, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 8, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 8, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.502121943551201\n",
      "tree depth: 8, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.49942711648631444\n",
      "tree depth: 8, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5100742832586408\n",
      "avgfold accuracy: 0.502947972158496\n",
      "standard deviation: 0.00386185214034261\n",
      "avg train time: 1.3641349792480468\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5674273321704879\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5521756949748045\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5779755729966891\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5371534792756391\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5555819383355877\n",
      "avgfold accuracy: 0.5580628035506416\n",
      "standard deviation: 0.013872457349147153\n",
      "avg train time: 1.2930431842803956\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5143374858522146\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5019479335366305\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5293550128611358\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5147602837664554\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5222257648640781\n",
      "avgfold accuracy: 0.5165252961761029\n",
      "standard deviation: 0.00913781437569189\n",
      "avg train time: 1.37109055519104\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5087431687837057\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5077118425506114\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5013039788189928\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.500825908638127\n",
      "avgfold accuracy: 0.5036483714772679\n",
      "standard deviation: 0.0037911108390445234\n",
      "avg train time: 1.3810740470886231\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.507212493482762\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5058938371697026\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5044522426573247\n",
      "avgfold accuracy: 0.5034121304626414\n",
      "standard deviation: 0.0031149834062196863\n",
      "avg train time: 1.3886112213134765\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.507195981478535\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5047535542489588\n",
      "avgfold accuracy: 0.502607853219989\n",
      "standard deviation: 0.0029633966381264636\n",
      "avg train time: 1.423740530014038\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5018980802405927\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5107478032869995\n",
      "avgfold accuracy: 0.5031419966626391\n",
      "standard deviation: 0.004062891482424616\n",
      "avg train time: 1.4565627098083496\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5052194820634601\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5024475915907999\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019779823708459\n",
      "avgfold accuracy: 0.501861749323304\n",
      "standard deviation: 0.0019940222802904166\n",
      "avg train time: 1.4147538661956787\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5053185540888226\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5022956225056537\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4989897199574619\n",
      "avgfold accuracy: 0.5015361418284803\n",
      "standard deviation: 0.0022419495951103157\n",
      "avg train time: 1.433250617980957\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5013255842085402\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.49926186444357606\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 8, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.5019666198842188\n",
      "standard deviation: 0.0019269246186241408\n",
      "avg train time: 1.418168830871582\n",
      "tree depth: 8, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5069648134193558\n",
      "tree depth: 8, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 8, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5050688621360523\n",
      "tree depth: 8, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5037325067574424\n",
      "tree depth: 8, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.4987947536334633\n",
      "avgfold accuracy: 0.5028607309784981\n",
      "standard deviation: 0.0031219705220616365\n",
      "avg train time: 1.5198729991912843\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5523963421407363\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5629984707575256\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5777801841729299\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5508643991180731\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5560001901144449\n",
      "avgfold accuracy: 0.560007917260742\n",
      "standard deviation: 0.009822410682995749\n",
      "avg train time: 1.2617747783660889\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5131865866484935\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5038395360635818\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5305924754116113\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5061070674083709\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5076034886000645\n",
      "avgfold accuracy: 0.5122658308264244\n",
      "standard deviation: 0.009668339447301118\n",
      "avg train time: 1.3539723873138427\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5049718020000541\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5051122818746655\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5059912914325778\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5013399107650324\n",
      "avgfold accuracy: 0.5037653261264289\n",
      "standard deviation: 0.001982355379682665\n",
      "avg train time: 1.3609806537628173\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5046472089813232\n",
      "avgfold accuracy: 0.5014526685680933\n",
      "standard deviation: 0.001807697824986104\n",
      "avg train time: 1.4632630348205566\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5023324665962979\n",
      "avgfold accuracy: 0.5013314109772385\n",
      "standard deviation: 0.0015493776142524623\n",
      "avg train time: 1.3949806213378906\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5073115655081244\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5026212705452525\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.4991492378589153\n",
      "avgfold accuracy: 0.5016790960285813\n",
      "standard deviation: 0.003072241522191273\n",
      "avg train time: 1.380247449874878\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5070143494320372\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017495548851927\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019602581595733\n",
      "avgfold accuracy: 0.5020451829217407\n",
      "standard deviation: 0.00265963483116322\n",
      "avg train time: 1.3772130489349366\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.4993487039208024\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5018893613144829\n",
      "avgfold accuracy: 0.5015487506525494\n",
      "standard deviation: 0.0020635072397189465\n",
      "avg train time: 1.4927293300628661\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.511049908283322\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5085802373228749\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5019602581595733\n",
      "avgfold accuracy: 0.5045282087782084\n",
      "standard deviation: 0.00445336243975843\n",
      "avg train time: 1.4261396408081055\n",
      "tree depth: 8, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5031769346314771\n",
      "tree depth: 8, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5013427362787951\n",
      "tree depth: 8, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5021002336818945\n",
      "tree depth: 8, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5016010295297927\n",
      "tree depth: 8, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5017652918355747\n",
      "avgfold accuracy: 0.5019972451915068\n",
      "standard deviation: 0.0006389749495347094\n",
      "avg train time: 1.3768826007843018\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5749370417293372\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5460892377078335\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5950492081652127\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5439067703576601\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5528772633025651\n",
      "avgfold accuracy: 0.5625719042525217\n",
      "standard deviation: 0.01960471105190235\n",
      "avg train time: 1.2675867080688477\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5162644492546081\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5057825948012977\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5290293648215371\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5061070674083709\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5101663699429063\n",
      "avgfold accuracy: 0.5134699692457441\n",
      "standard deviation: 0.008650152910481733\n",
      "avg train time: 1.2429596424102782\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5080374905902102\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5013044623424872\n",
      "avgfold accuracy: 0.5035475179050867\n",
      "standard deviation: 0.0026619633301257282\n",
      "avg train time: 1.320183563232422\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5071464454658537\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5056333187380236\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49918468628146045\n",
      "avgfold accuracy: 0.5027442496652464\n",
      "standard deviation: 0.003141757337294452\n",
      "avg train time: 1.4015503883361817\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5018893613144829\n",
      "avgfold accuracy: 0.5015266446840464\n",
      "standard deviation: 0.0015031758482657654\n",
      "avg train time: 1.461129093170166\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.50699783742781\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.502243845539935\n",
      "avgfold accuracy: 0.5017603024316396\n",
      "standard deviation: 0.002778479701403973\n",
      "avg train time: 1.3694225311279298\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025778508066393\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5018344265168498\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5014083958126514\n",
      "standard deviation: 0.0010663173328101156\n",
      "avg train time: 1.388627815246582\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.50496624478423\n",
      "avgfold accuracy: 0.501421111573743\n",
      "standard deviation: 0.0023147093853798408\n",
      "avg train time: 1.393928337097168\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5087266567794787\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.49960550238413776\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.4993052841821892\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 8, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5046649331925959\n",
      "avgfold accuracy: 0.5028316041926331\n",
      "standard deviation: 0.0035187470323145525\n",
      "avg train time: 1.450340175628662\n",
      "tree depth: 8, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5033255426695208\n",
      "tree depth: 8, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 8, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 8, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 8, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5073518840143457\n",
      "avgfold accuracy: 0.5028774430742335\n",
      "standard deviation: 0.0025538462218240633\n",
      "avg train time: 1.4424349784851074\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5874466362045206\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5377069307002197\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5894753953942643\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5347982914971541\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5802713685665568\n",
      "avgfold accuracy: 0.5659397244725431\n",
      "standard deviation: 0.024448928697884884\n",
      "avg train time: 1.2505313873291015\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5207788562284781\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5028226891196294\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5116735013520278\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.504336294615264\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5168695874318509\n",
      "avgfold accuracy: 0.51129618574945\n",
      "standard deviation: 0.006947898135788566\n",
      "avg train time: 1.2939478874206543\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050378500169623\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5047649239657601\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017071190693642\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501269013919942\n",
      "avgfold accuracy: 0.5028654936187765\n",
      "standard deviation: 0.0016704405521148703\n",
      "avg train time: 1.4233628273010255\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5013820672629001\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5017029297541477\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5073518840143457\n",
      "avgfold accuracy: 0.5019370797898665\n",
      "standard deviation: 0.002841890304867089\n",
      "avg train time: 1.4122020244598388\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5034906627117915\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5057418680845566\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5018980802405927\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5050016932067752\n",
      "avgfold accuracy: 0.5032093087784884\n",
      "standard deviation: 0.0021110404762491224\n",
      "avg train time: 1.2774133205413818\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5073611015208056\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5033715553677856\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5041356470078138\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5023324665962979\n",
      "avgfold accuracy: 0.5033446306735916\n",
      "standard deviation: 0.002545290718088439\n",
      "avg train time: 1.4646806240081787\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5035071747160186\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49962265445439263\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502243845539935\n",
      "avgfold accuracy: 0.5009369724858486\n",
      "standard deviation: 0.0016334794688170422\n",
      "avg train time: 1.4098631858825683\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5053185540888226\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.503302947086766\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.4994572532673353\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5018344265168498\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.4988656504785537\n",
      "avgfold accuracy: 0.5017557662876655\n",
      "standard deviation: 0.0023970542032895423\n",
      "avg train time: 1.3516669273376465\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.505242541090505\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4985466146756469\n",
      "avgfold accuracy: 0.5017175696403615\n",
      "standard deviation: 0.002892196038437349\n",
      "avg train time: 1.372102975845337\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5070308614362642\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5048083437043733\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5039022500207566\n",
      "tree depth: 8, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5073164355918005\n",
      "avgfold accuracy: 0.5045566915258233\n",
      "standard deviation: 0.0027411345074926674\n",
      "avg train time: 1.3975926876068114\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5662764329667669\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5605236978419085\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.6000038973138876\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5458163820699453\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5650004257771423\n",
      "avgfold accuracy: 0.5675241671939302\n",
      "standard deviation: 0.01779378254471717\n",
      "avg train time: 1.3531440258026124\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5096133639155667\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5061256362063954\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5231138177825277\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5091509146763347\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.513643780980238\n",
      "avgfold accuracy: 0.5123295027122124\n",
      "standard deviation: 0.005899111295578384\n",
      "avg train time: 1.2524746417999268\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5032760066568396\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050471522667458\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49858206309819214\n",
      "avgfold accuracy: 0.5016752555890466\n",
      "standard deviation: 0.0023339857103850826\n",
      "avg train time: 1.3875046253204346\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5088917768217495\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.4992835743128826\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.49920241049273306\n",
      "avgfold accuracy: 0.5014066641086921\n",
      "standard deviation: 0.0037523427640082613\n",
      "avg train time: 1.3650580883026122\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5033090306652936\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.502751529761092\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.4992378589152783\n",
      "avgfold accuracy: 0.5009633523391439\n",
      "standard deviation: 0.0017076104917425911\n",
      "avg train time: 1.41330943107605\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.4995440927445617\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.500048229815435\n",
      "standard deviation: 0.0007831705377974414\n",
      "avg train time: 1.4763604640960692\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015471873051709\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995006730059485\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502066603427209\n",
      "avgfold accuracy: 0.5005263834745535\n",
      "standard deviation: 0.001063019850490019\n",
      "avg train time: 1.4342947959899903\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.51083525222837\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5018893613144829\n",
      "avgfold accuracy: 0.5027666324552141\n",
      "standard deviation: 0.00415169396690693\n",
      "avg train time: 1.3739864826202393\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5052690180761413\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5013255842085402\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5116574151235658\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5039871216524138\n",
      "tree depth: 8, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5017475676243022\n",
      "avgfold accuracy: 0.5047973413369926\n",
      "standard deviation: 0.0037227345996817657\n",
      "avg train time: 1.4682134151458741\n",
      "tree depth: 8, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49950463987318783\n",
      "tree depth: 8, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.503011361892433\n",
      "tree depth: 8, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5020785238125879\n",
      "tree depth: 8, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.501712119201757\n",
      "avgfold accuracy: 0.5011891880690846\n",
      "standard deviation: 0.0013874121142462396\n",
      "avg train time: 1.3930903911590575\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5643115044637451\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5640667739122428\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5781275420818353\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5436000332106384\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5560533627482628\n",
      "avgfold accuracy: 0.561231843283345\n",
      "standard deviation: 0.011319745311960455\n",
      "avg train time: 1.3160776138305663\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5132856586738559\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5112687296093481\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5322585117073739\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5065102076587422\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5163201368824003\n",
      "avgfold accuracy: 0.5159286489063442\n",
      "standard deviation: 0.008766127568019148\n",
      "avg train time: 1.3311876773834228\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5051204100380977\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5048517634429864\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5153063912119598\n",
      "avgfold accuracy: 0.5053295193829174\n",
      "standard deviation: 0.0053814724196245235\n",
      "avg train time: 1.3752087593078612\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4994572532673353\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5018344265168498\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.5007702617795655\n",
      "standard deviation: 0.0016075596127311485\n",
      "avg train time: 1.4610964775085449\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4996367359070044\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5054162200449577\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5017830160468474\n",
      "avgfold accuracy: 0.5013076065136784\n",
      "standard deviation: 0.0021977146050559614\n",
      "avg train time: 1.453105640411377\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5015306753009439\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5023824619828801\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.502066603427209\n",
      "avgfold accuracy: 0.5015822472745036\n",
      "standard deviation: 0.0008610639213269699\n",
      "avg train time: 1.3572984218597413\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5062459063275492\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5048067268827766\n",
      "avgfold accuracy: 0.5034603676146652\n",
      "standard deviation: 0.0025566823283839288\n",
      "avg train time: 1.413074541091919\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4995119881179716\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.5006618646252232\n",
      "standard deviation: 0.0016165124203216432\n",
      "avg train time: 1.3834990501403808\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5053350660930497\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5019425339483008\n",
      "avgfold accuracy: 0.5012367206917875\n",
      "standard deviation: 0.0022388549231802304\n",
      "avg train time: 1.4457473278045654\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5053185540888226\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5019482645967484\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 8, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5073696082256183\n",
      "avgfold accuracy: 0.503571382848456\n",
      "standard deviation: 0.0023610096982949365\n",
      "avg train time: 1.4714168548583983\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5590474274797779\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5446092848669992\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5724073838665853\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.545710292530374\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5695767379133752\n",
      "avgfold accuracy: 0.5582702253314222\n",
      "standard deviation: 0.01159891139489358\n",
      "avg train time: 1.2769907474517823\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5168423694025557\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5094114312229066\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5231789473904476\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5086629027943063\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5159833768682209\n",
      "avgfold accuracy: 0.5148158055356874\n",
      "standard deviation: 0.005338123255666407\n",
      "avg train time: 1.3339438438415527\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5068987654024476\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49921844470496285\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036900709416139\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501269013919942\n",
      "avgfold accuracy: 0.5021878156813855\n",
      "standard deviation: 0.0028112969870390937\n",
      "avg train time: 1.3752562046051025\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49932699405149583\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.4987770294221907\n",
      "avgfold accuracy: 0.5010826990278264\n",
      "standard deviation: 0.0024354049350934492\n",
      "avg train time: 1.4115203380584718\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.507229005486989\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5019957065821186\n",
      "avgfold accuracy: 0.5017013481222212\n",
      "standard deviation: 0.0028969776658871456\n",
      "avg train time: 1.443090057373047\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5090734088682473\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5022261213286623\n",
      "avgfold accuracy: 0.5025603756465413\n",
      "standard deviation: 0.003420835038880237\n",
      "avg train time: 1.4765357494354248\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5053020420845955\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5021877959029492\n",
      "standard deviation: 0.00179155981582192\n",
      "avg train time: 1.448861026763916\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5019779823708459\n",
      "avgfold accuracy: 0.5012806881466959\n",
      "standard deviation: 0.0021698278760214802\n",
      "avg train time: 1.463437557220459\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5051038980338707\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5031142743139623\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 8, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.5012982872292036\n",
      "standard deviation: 0.002383384975040074\n",
      "avg train time: 1.4126285076141358\n",
      "tree depth: 8, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5050873860296435\n",
      "tree depth: 8, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.4990230558812036\n",
      "tree depth: 8, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5015798116218785\n",
      "tree depth: 8, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5043990700235069\n",
      "avgfold accuracy: 0.5019869909847877\n",
      "standard deviation: 0.002406935897139779\n",
      "avg train time: 1.3698761940002442\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.568137348352252\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5665072426873502\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5809876211894602\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.549303960368638\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5843230836562968\n",
      "avgfold accuracy: 0.5698518512507995\n",
      "standard deviation: 0.012407769225753158\n",
      "avg train time: 1.2552009105682373\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5088207877126671\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5025997122063159\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5404262615134238\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5055766197105139\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5058027483419446\n",
      "avgfold accuracy: 0.512645225896973\n",
      "standard deviation: 0.014029321474451826\n",
      "avg train time: 1.3821841716766357\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5031108866145688\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5053945101756512\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501622247437707\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5042750005445987\n",
      "avgfold accuracy: 0.5031765195226721\n",
      "standard deviation: 0.0015115919416883693\n",
      "avg train time: 1.3145220756530762\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5052029700592331\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.49906061680255226\n",
      "avgfold accuracy: 0.5010667627095626\n",
      "standard deviation: 0.002243747788294819\n",
      "avg train time: 1.4203609943389892\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5053020420845955\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.50135470381421\n",
      "standard deviation: 0.002176358232828943\n",
      "avg train time: 1.4997502326965333\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5054836741310933\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5051966595307739\n",
      "avgfold accuracy: 0.5023796450516136\n",
      "standard deviation: 0.002518129959182193\n",
      "avg train time: 1.3961694717407227\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016132353220791\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.500565851732168\n",
      "standard deviation: 0.0010786476767982228\n",
      "avg train time: 1.4726564407348632\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5051259413326978\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.5030227100188581\n",
      "standard deviation: 0.0012009364915243625\n",
      "avg train time: 1.6251171588897706\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5054011141099579\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5082762991525827\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 8, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.49895427153491667\n",
      "avgfold accuracy: 0.502849617791654\n",
      "standard deviation: 0.003502015633357442\n",
      "avg train time: 1.6548246383666991\n",
      "tree depth: 8, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5070638854447184\n",
      "tree depth: 8, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5021870731591208\n",
      "tree depth: 8, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5017071190693642\n",
      "tree depth: 8, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5045585879249602\n",
      "avgfold accuracy: 0.5030655985650719\n",
      "standard deviation: 0.0025069052862192877\n",
      "avg train time: 1.5521128177642822\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5743921455898439\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5659926805797039\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5898500868133207\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5361022703161469\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5783359639099412\n",
      "avgfold accuracy: 0.5689346294417913\n",
      "standard deviation: 0.01812316086233838\n",
      "avg train time: 1.2920495986938476\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.507669888508946\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5025311039252964\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5271245199634019\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5086629027943063\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5292763372867897\n",
      "avgfold accuracy: 0.5150529504957481\n",
      "standard deviation: 0.010955970761932188\n",
      "avg train time: 1.3714610576629638\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050543620211895\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017311659036825\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035203276782996\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5069796755776211\n",
      "avgfold accuracy: 0.5034022196113429\n",
      "standard deviation: 0.0025239616114084594\n",
      "avg train time: 1.4426192760467529\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5053076706984247\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5070328482114389\n",
      "avgfold accuracy: 0.5034365522644915\n",
      "standard deviation: 0.0030535264618329113\n",
      "avg train time: 1.347156810760498\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5016462593305333\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5047003816151411\n",
      "avgfold accuracy: 0.5021368743016165\n",
      "standard deviation: 0.0015607927470026766\n",
      "avg train time: 1.4232189178466796\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5053185540888226\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5018716371032104\n",
      "avgfold accuracy: 0.501735890599047\n",
      "standard deviation: 0.002040532207367045\n",
      "avg train time: 1.4253217697143554\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5033420546737478\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4993796526054591\n",
      "avgfold accuracy: 0.5011886306433244\n",
      "standard deviation: 0.0015114585699465602\n",
      "avg train time: 1.3963140964508056\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49932699405149583\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5050016932067752\n",
      "avgfold accuracy: 0.5017931107332843\n",
      "standard deviation: 0.002157127792183552\n",
      "avg train time: 1.4573651790618896\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5013820672629001\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5015142569813439\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5047890026715041\n",
      "avgfold accuracy: 0.5013729559069258\n",
      "standard deviation: 0.0019050163288115351\n",
      "avg train time: 1.449643087387085\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5107526922072346\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5013427362787951\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5016010295297927\n",
      "tree depth: 8, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.49906061680255226\n",
      "avgfold accuracy: 0.5024645754864486\n",
      "standard deviation: 0.004258801063513989\n",
      "avg train time: 1.4342592239379883\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5611114280081619\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5633243600923684\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5831964042178875\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5322424561112187\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5536571285985594\n",
      "avgfold accuracy: 0.5587063554056393\n",
      "standard deviation: 0.016448133569198234\n",
      "avg train time: 1.3654110908508301\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5130049546019957\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5077771097497784\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5405348108599567\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5105609830348988\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5140762913424654\n",
      "avgfold accuracy: 0.517190829917819\n",
      "standard deviation: 0.011872121512324687\n",
      "avg train time: 1.3634785652160644\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5089908488471119\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5044826956647745\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5039022500207566\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5003296307224943\n",
      "avgfold accuracy: 0.503867949345653\n",
      "standard deviation: 0.002970925143020119\n",
      "avg train time: 1.4314441680908203\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5052690180761413\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5033887074380404\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.49907834101382487\n",
      "avgfold accuracy: 0.5014180364048249\n",
      "standard deviation: 0.002460318221376764\n",
      "avg train time: 1.387030839920044\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49966323998582063\n",
      "avgfold accuracy: 0.5004987007448887\n",
      "standard deviation: 0.0015242990725159026\n",
      "avg train time: 1.512638235092163\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5073115655081244\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.49936192839418647\n",
      "avgfold accuracy: 0.5015804690929254\n",
      "standard deviation: 0.0029692712057725716\n",
      "avg train time: 1.482274627685547\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49978534394504803\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995006730059485\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020466055959926\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5010316378134504\n",
      "standard deviation: 0.0011536785470350928\n",
      "avg train time: 1.467605209350586\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5026646902838657\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.5019191428340379\n",
      "standard deviation: 0.0004243270140459846\n",
      "avg train time: 1.4546136856079102\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5090073608513389\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.508471687976342\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 8, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5046826574038684\n",
      "avgfold accuracy: 0.5046553777600363\n",
      "standard deviation: 0.0037108975837122707\n",
      "avg train time: 1.4221213817596436\n",
      "tree depth: 8, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5031934466357042\n",
      "tree depth: 8, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 8, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.4995006730059485\n",
      "tree depth: 8, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5037749425732709\n",
      "tree depth: 8, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.4990428925912797\n",
      "avgfold accuracy: 0.5013983815294075\n",
      "standard deviation: 0.0018987670098014836\n",
      "avg train time: 1.4196872234344482\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5759112499787346\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5587007035959767\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5706705943220584\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5440880450926668\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5619944391524857\n",
      "avgfold accuracy: 0.5622730064283845\n",
      "standard deviation: 0.010954934373202054\n",
      "avg train time: 1.2797221660614013\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5169084174194639\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5007595658901293\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5320848327529213\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5027989188092141\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5159124800231306\n",
      "avgfold accuracy: 0.5136928429789719\n",
      "standard deviation: 0.011309104610962096\n",
      "avg train time: 1.3353596210479737\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5030943746103418\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5012569759275207\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5108107302206089\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4994907702100573\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5013930833988502\n",
      "avgfold accuracy: 0.5032091868734757\n",
      "standard deviation: 0.003968189133722109\n",
      "avg train time: 1.369246244430542\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5070638854447184\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49939212365941554\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5062459063275492\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5023501908075705\n",
      "avgfold accuracy: 0.5029898387635449\n",
      "standard deviation: 0.0031659957584488866\n",
      "avg train time: 1.3916918754577636\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5091889928978367\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5025480474885489\n",
      "standard deviation: 0.0034569722433468947\n",
      "avg train time: 1.507500982284546\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49946827366182206\n",
      "avgfold accuracy: 0.5008459202537094\n",
      "standard deviation: 0.0015635702126324863\n",
      "avg train time: 1.4423633575439454\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5090568968640201\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502751529761092\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5031426289955391\n",
      "standard deviation: 0.003125000919311417\n",
      "avg train time: 1.4844130516052245\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5016132353220791\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49932647997164126\n",
      "avgfold accuracy: 0.5003165905211929\n",
      "standard deviation: 0.0010092433821911296\n",
      "avg train time: 1.4323248863220215\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5051534340465519\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5032343388057464\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4995440927445617\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4994058985784002\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5016412223566665\n",
      "avgfold accuracy: 0.5017957973063853\n",
      "standard deviation: 0.002197671900131745\n",
      "avg train time: 1.4354110717773438\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.506981325423583\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49943554339802876\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5039446858365852\n",
      "tree depth: 8, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5072455387467101\n",
      "avgfold accuracy: 0.5038105484210462\n",
      "standard deviation: 0.003053011556609837\n",
      "avg train time: 1.4950416564941407\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5784607284495829\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5540378681601607\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5698295330599459\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5445220896871743\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5644155268051465\n",
      "avgfold accuracy: 0.562253149232402\n",
      "standard deviation: 0.01188818243839576\n",
      "avg train time: 1.3138856887817383\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5127077385259083\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.4989880278549621\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.532149962360841\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.504336294615264\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5137927039621039\n",
      "avgfold accuracy: 0.5123949454638158\n",
      "standard deviation: 0.011287156897912223\n",
      "avg train time: 1.3195976734161377\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501349043254446\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5076467129426917\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5064302250281706\n",
      "avgfold accuracy: 0.5033931290859567\n",
      "standard deviation: 0.0030690732019816153\n",
      "avg train time: 1.426735782623291\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5070473734404912\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5053293805677314\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5047003816151411\n",
      "avgfold accuracy: 0.5033176470043436\n",
      "standard deviation: 0.0030098945378727146\n",
      "avg train time: 1.386749267578125\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5089578248386577\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.5020821377788446\n",
      "standard deviation: 0.0035559490016324703\n",
      "avg train time: 1.4171260833740233\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5016297473263063\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.502155224483572\n",
      "avgfold accuracy: 0.5013766733087603\n",
      "standard deviation: 0.0008963775617304917\n",
      "avg train time: 1.45008225440979\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5054671621268663\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5086670768001013\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49955689471818504\n",
      "avgfold accuracy: 0.5034394725787366\n",
      "standard deviation: 0.003233314910021518\n",
      "avg train time: 1.4214464187622071\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5052690180761413\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4993052841821892\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4992733073378235\n",
      "avgfold accuracy: 0.5010119722618842\n",
      "standard deviation: 0.0022714400191809108\n",
      "avg train time: 1.474071168899536\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.503391590686429\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5031142743139623\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5023607521135734\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 8, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5014994286664858\n",
      "avgfold accuracy: 0.502018042595513\n",
      "standard deviation: 0.0013218648117808431\n",
      "avg train time: 1.4907919883728027\n",
      "tree depth: 8, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5049222659873729\n",
      "tree depth: 8, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 8, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5054596397835709\n",
      "tree depth: 8, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5018132086089355\n",
      "tree depth: 8, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5043990700235069\n",
      "avgfold accuracy: 0.503618257862895\n",
      "standard deviation: 0.0016406270051922687\n",
      "avg train time: 1.4878711700439453\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5313846291253741\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5220449239802192\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5878262300210351\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5247156339080619\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5472482914454441\n",
      "avgfold accuracy: 0.5426439416960268\n",
      "standard deviation: 0.024228722976908854\n",
      "avg train time: 1.7416549682617188\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5049222659873729\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5082980090218893\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5057578944455207\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5055971870983584\n",
      "avgfold accuracy: 0.5051939098085402\n",
      "standard deviation: 0.0022193596030966145\n",
      "avg train time: 1.6294891834259033\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49932647997164126\n",
      "avgfold accuracy: 0.5000674299958995\n",
      "standard deviation: 0.0007873404727544117\n",
      "avg train time: 1.6004255294799805\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5073115655081244\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5017029297541477\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.501665770297623\n",
      "standard deviation: 0.002921161248275373\n",
      "avg train time: 1.5828945636749268\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5053680901015039\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5017200818244025\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5017904325805489\n",
      "standard deviation: 0.002034069101572502\n",
      "avg train time: 1.6136661529541017\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5053680901015039\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5008190733822183\n",
      "standard deviation: 0.002279562314811915\n",
      "avg train time: 1.512089443206787\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5039659037444995\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020843276384815\n",
      "avgfold accuracy: 0.5013814119326578\n",
      "standard deviation: 0.0016365088994186564\n",
      "avg train time: 1.4515920639038087\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.503375078682202\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5021870731591208\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5016589465679392\n",
      "avgfold accuracy: 0.5017227782440028\n",
      "standard deviation: 0.001178738275722533\n",
      "avg train time: 1.5092652320861817\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5032925186610666\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5050254423974392\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.5013880878924886\n",
      "standard deviation: 0.0023346308687813714\n",
      "avg train time: 1.4539376258850099\n",
      "tree depth: 9, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5048892419789187\n",
      "tree depth: 9, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 9, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5021002336818945\n",
      "tree depth: 9, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.49929980903882876\n",
      "tree depth: 9, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5150050796203256\n",
      "avgfold accuracy: 0.5045411417759565\n",
      "standard deviation: 0.005528840760254107\n",
      "avg train time: 1.5014674663543701\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5288351506545258\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5272443483928511\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5345211770620322\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5283535825976254\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5426648500175262\n",
      "avgfold accuracy: 0.5323238217449122\n",
      "standard deviation: 0.0057532583902085\n",
      "avg train time: 1.5795045852661134\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5068657413939934\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5113968966918867\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5015161578981356\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5042927247558713\n",
      "avgfold accuracy: 0.5047765695934168\n",
      "standard deviation: 0.00409173292647298\n",
      "avg train time: 1.5857471466064452\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5048244510940493\n",
      "avgfold accuracy: 0.501618033343819\n",
      "standard deviation: 0.0018024335019768529\n",
      "avg train time: 1.5840638637542725\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.4995214462956398\n",
      "avgfold accuracy: 0.5009426744375147\n",
      "standard deviation: 0.0015410561000605892\n",
      "avg train time: 1.5323691844940186\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5034576387033374\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.49920241049273306\n",
      "avgfold accuracy: 0.5004140738216678\n",
      "standard deviation: 0.0015406099670858201\n",
      "avg train time: 1.6354272365570068\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5034741507075644\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5020843276384815\n",
      "avgfold accuracy: 0.5013281805090105\n",
      "standard deviation: 0.0014361165545750034\n",
      "avg train time: 1.47527174949646\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5013820672629001\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4992378589152783\n",
      "avgfold accuracy: 0.5004305215439987\n",
      "standard deviation: 0.0010609025934560444\n",
      "avg train time: 1.4703743934631348\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5025561409373328\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.49932647997164126\n",
      "avgfold accuracy: 0.5017238653126233\n",
      "standard deviation: 0.001366909157840164\n",
      "avg train time: 1.5937407970428468\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5016627713347604\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.4995119881179716\n",
      "tree depth: 9, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5018716371032104\n",
      "avgfold accuracy: 0.5008303289387122\n",
      "standard deviation: 0.0010763214556207372\n",
      "avg train time: 1.4308575630187987\n",
      "tree depth: 9, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.504955289995827\n",
      "tree depth: 9, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.50135988834905\n",
      "tree depth: 9, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.504982022658826\n",
      "tree depth: 9, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.49925737322300023\n",
      "tree depth: 9, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5041154826431453\n",
      "avgfold accuracy: 0.5029340113739696\n",
      "standard deviation: 0.002265891579079184\n",
      "avg train time: 1.4699551105499267\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5479628064602966\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5218734032776704\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5542160827758082\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5289785883633613\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5401905897310475\n",
      "avgfold accuracy: 0.5386442941216368\n",
      "standard deviation: 0.01188608389431453\n",
      "avg train time: 1.524358606338501\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5049883140042811\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5037445601083506\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5059871197463557\n",
      "avgfold accuracy: 0.5032212356301571\n",
      "standard deviation: 0.002264331549288066\n",
      "avg train time: 1.5844298362731934\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035071747160186\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5020466055959926\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.505143486896956\n",
      "avgfold accuracy: 0.5020727165394434\n",
      "standard deviation: 0.0020758544122591485\n",
      "avg train time: 1.590825891494751\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5054836741310933\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5008318027304328\n",
      "standard deviation: 0.002331276882271377\n",
      "avg train time: 1.5680580615997315\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5034906627117915\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5050725900518657\n",
      "avgfold accuracy: 0.5016124569591109\n",
      "standard deviation: 0.0022371725294597702\n",
      "avg train time: 1.4725956916809082\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5034576387033374\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5049307963616848\n",
      "avgfold accuracy: 0.5015108549093122\n",
      "standard deviation: 0.0022424077612990725\n",
      "avg train time: 1.4492955684661866\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5052194820634601\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019779823708459\n",
      "avgfold accuracy: 0.501321360084758\n",
      "standard deviation: 0.002124908741305549\n",
      "avg train time: 1.4582265377044679\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5023824619828801\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5020724831208951\n",
      "standard deviation: 0.0018695507409441597\n",
      "avg train time: 1.5219213485717773\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5014316032755813\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5053945101756512\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 9, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.501712119201757\n",
      "avgfold accuracy: 0.5020616233452958\n",
      "standard deviation: 0.0018044886873875956\n",
      "avg train time: 1.5254899978637695\n",
      "tree depth: 9, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5090734088682473\n",
      "tree depth: 9, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 9, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.4991750249663497\n",
      "tree depth: 9, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 9, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.4992378589152783\n",
      "avgfold accuracy: 0.5018484403742305\n",
      "standard deviation: 0.003752207250592026\n",
      "avg train time: 1.4174078941345214\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5497576738288736\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5346000588586832\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5345428869313388\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5181629904334911\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5557628441120963\n",
      "avgfold accuracy: 0.5385652908328966\n",
      "standard deviation: 0.013184702766947165\n",
      "avg train time: 1.4920829772949218\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5084294407033914\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5048083437043733\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.49942711648631444\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5119636445731237\n",
      "avgfold accuracy: 0.5052354213178113\n",
      "standard deviation: 0.0045383364004184116\n",
      "avg train time: 1.5376259326934814\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016462593305333\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5026429804145591\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5041144290998995\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5074582292819813\n",
      "avgfold accuracy: 0.5031552275551398\n",
      "standard deviation: 0.002548696777182332\n",
      "avg train time: 1.5073066711425782\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.49948599787309467\n",
      "avgfold accuracy: 0.5005388221069339\n",
      "standard deviation: 0.0010198290778684418\n",
      "avg train time: 1.6229631900787354\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.4996202239027773\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5051080384744108\n",
      "avgfold accuracy: 0.500839713386753\n",
      "standard deviation: 0.0021376028759249053\n",
      "avg train time: 1.563427209854126\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.5005938465651063\n",
      "standard deviation: 0.0015085855246286483\n",
      "avg train time: 1.528501272201538\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996202239027773\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995214462956398\n",
      "avgfold accuracy: 0.49973218724308877\n",
      "standard deviation: 0.00014793560351694605\n",
      "avg train time: 1.532942819595337\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5025561409373328\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4993796526054591\n",
      "avgfold accuracy: 0.5017907023997179\n",
      "standard deviation: 0.001374032689239821\n",
      "avg train time: 1.5848409175872802\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.49958719989432315\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.4993487039208024\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5017652918355747\n",
      "avgfold accuracy: 0.5003982152079847\n",
      "standard deviation: 0.001043600551026226\n",
      "avg train time: 1.4387587547302245\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5030943746103418\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5083414287605025\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 9, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5013930833988502\n",
      "avgfold accuracy: 0.5024609586615842\n",
      "standard deviation: 0.0031924078831302547\n",
      "avg train time: 1.5326237678527832\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5427763484052907\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5198274321184251\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5613473824959863\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5438140573252521\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5361388746413075\n",
      "avgfold accuracy: 0.5407808189972523\n",
      "standard deviation: 0.013392223681060182\n",
      "avg train time: 1.433855152130127\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5031934466357042\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5012752586482442\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.505927637708835\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5031938236569702\n",
      "avgfold accuracy: 0.5026528554629821\n",
      "standard deviation: 0.002101417508027834\n",
      "avg train time: 1.5734981060028077\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5018893613144829\n",
      "avgfold accuracy: 0.501335043962954\n",
      "standard deviation: 0.0014079634704590458\n",
      "avg train time: 1.5768377780914307\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4997358079323668\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.49993487039208023\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.49929103154909604\n",
      "avgfold accuracy: 0.49975577607552674\n",
      "standard deviation: 0.00024563611608730327\n",
      "avg train time: 1.5158664226531982\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5036722947582893\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.5081140250990674\n",
      "avgfold accuracy: 0.5030390045886426\n",
      "standard deviation: 0.00282463037323134\n",
      "avg train time: 1.4850194454193115\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5026646902838657\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5052143837420464\n",
      "avgfold accuracy: 0.5022002086773382\n",
      "standard deviation: 0.0020970914596088863\n",
      "avg train time: 1.5331192016601562\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5021729486948445\n",
      "avgfold accuracy: 0.5009866136136901\n",
      "standard deviation: 0.0010193767280610732\n",
      "avg train time: 1.6508362293243408\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5033255426695208\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5024041718521867\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5073873324368909\n",
      "avgfold accuracy: 0.502518590699365\n",
      "standard deviation: 0.002822733430553679\n",
      "avg train time: 1.5412225246429443\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.4996862719196856\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5025561409373328\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5037749425732709\n",
      "tree depth: 9, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.4990428925912797\n",
      "avgfold accuracy: 0.5013046097584296\n",
      "standard deviation: 0.0017566035869634765\n",
      "avg train time: 1.5069836616516112\n",
      "tree depth: 9, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5049387779915999\n",
      "tree depth: 9, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.50135988834905\n",
      "tree depth: 9, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.49889279666536407\n",
      "tree depth: 9, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 9, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5042395521220535\n",
      "avgfold accuracy: 0.5022870369816463\n",
      "standard deviation: 0.0021589132230205345\n",
      "avg train time: 1.4773903846740724\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5491632416766989\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5167303098747177\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5414353780891444\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5208558197031338\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.53524206915809\n",
      "avgfold accuracy: 0.532685363700357\n",
      "standard deviation: 0.012240276230266143\n",
      "avg train time: 1.491882848739624\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5123329035208597\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5106587611354628\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.4995756418417144\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.503158375234425\n",
      "avgfold accuracy: 0.5054274052584553\n",
      "standard deviation: 0.0051102517831237235\n",
      "avg train time: 1.548210573196411\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.502860079107625\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5022615697512075\n",
      "avgfold accuracy: 0.5016736445288273\n",
      "standard deviation: 0.0015502755828518103\n",
      "avg train time: 1.5810048580169678\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5050903142631382\n",
      "avgfold accuracy: 0.5020596319297905\n",
      "standard deviation: 0.00205872136461633\n",
      "avg train time: 1.5776695728302002\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5016132353220791\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5020488792159363\n",
      "avgfold accuracy: 0.50061375307417\n",
      "standard deviation: 0.0010078360619616845\n",
      "avg train time: 1.4478614330291748\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5054011141099579\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.50087641345873\n",
      "standard deviation: 0.0022647141012617474\n",
      "avg train time: 1.5137568950653075\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997358079323668\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022261213286623\n",
      "avgfold accuracy: 0.5002627100013664\n",
      "standard deviation: 0.0009852411011695246\n",
      "avg train time: 1.5164403438568115\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.507212493482762\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5033544032975307\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5041144290998995\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.50496624478423\n",
      "avgfold accuracy: 0.5044364003464896\n",
      "standard deviation: 0.0016049485015581237\n",
      "avg train time: 1.5152761459350585\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5015141632967167\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5018132086089355\n",
      "tree depth: 9, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.500740859088643\n",
      "standard deviation: 0.0010949208770978471\n",
      "avg train time: 1.5200881958007812\n",
      "tree depth: 9, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5013325312502189\n",
      "tree depth: 9, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.503148578454472\n",
      "tree depth: 9, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.502339042244267\n",
      "tree depth: 9, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 9, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5043813458122343\n",
      "avgfold accuracy: 0.5025859669476941\n",
      "standard deviation: 0.0010869700085852781\n",
      "avg train time: 1.4459665298461915\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5643312688324412\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5244216592732217\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5491255107704492\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5185121633963413\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5506902540602306\n",
      "avgfold accuracy: 0.5414161712665367\n",
      "standard deviation: 0.01722760133108901\n",
      "avg train time: 1.455761480331421\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5051369220423247\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5031828825949818\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5046780844885338\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5034991097703854\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.506058016591446\n",
      "avgfold accuracy: 0.5045110030975344\n",
      "standard deviation: 0.0010583736241837932\n",
      "avg train time: 1.532738208770752\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016462593305333\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5026429804145591\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5076531956059799\n",
      "avgfold accuracy: 0.5023226737796375\n",
      "standard deviation: 0.0028758937567198275\n",
      "avg train time: 1.5125413894653321\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5041993007315566\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49936192839418647\n",
      "avgfold accuracy: 0.5013150350447314\n",
      "standard deviation: 0.0017124450378139895\n",
      "avg train time: 1.535656785964966\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5016368451272265\n",
      "standard deviation: 0.0014485138311568614\n",
      "avg train time: 1.504131031036377\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49966323998582063\n",
      "avgfold accuracy: 0.5005444425531855\n",
      "standard deviation: 0.0015006977901841666\n",
      "avg train time: 1.5237011432647705\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5054176261141851\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5027298198917854\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4992555831265509\n",
      "avgfold accuracy: 0.5017531167280846\n",
      "standard deviation: 0.002221248304311907\n",
      "avg train time: 1.5175681114196777\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5034906627117915\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5019957065821186\n",
      "avgfold accuracy: 0.5013393883458079\n",
      "standard deviation: 0.0014821668579377961\n",
      "avg train time: 1.5066838264465332\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.50699783742781\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4993487039208024\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49918468628146045\n",
      "avgfold accuracy: 0.5013163735510691\n",
      "standard deviation: 0.0029501341065898542\n",
      "avg train time: 1.5225126266479492\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5068822533982206\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5067088065950612\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4992835743128826\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 9, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5042927247558713\n",
      "avgfold accuracy: 0.5033698180886643\n",
      "standard deviation: 0.0033056670891758034\n",
      "avg train time: 1.4548263072967529\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5453093148719118\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5297362733787232\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5513994234067964\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5248969086430687\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5413072150412211\n",
      "avgfold accuracy: 0.5385298270683443\n",
      "standard deviation: 0.009823287637321512\n",
      "avg train time: 1.4333086967468263\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5051699460507789\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5053076706984247\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5038810321128424\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5092057969062835\n",
      "avgfold accuracy: 0.5046614329429013\n",
      "standard deviation: 0.0030374405504096625\n",
      "avg train time: 1.4841729640960692\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5050725900518657\n",
      "avgfold accuracy: 0.5012077642909941\n",
      "standard deviation: 0.0020528772104532805\n",
      "avg train time: 1.5636749744415284\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4998046111762407\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49948599787309467\n",
      "avgfold accuracy: 0.5001332265786218\n",
      "standard deviation: 0.0007476114289517592\n",
      "avg train time: 1.532355833053589\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5050903142631382\n",
      "avgfold accuracy: 0.5012429793512181\n",
      "standard deviation: 0.002073128403685017\n",
      "avg train time: 1.5938052177429198\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5070803974489454\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5020678235039069\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.501682414935597\n",
      "standard deviation: 0.0028461488140212182\n",
      "avg train time: 1.5236899852752686\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020678235039069\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5009419922547675\n",
      "standard deviation: 0.0015894243701031307\n",
      "avg train time: 1.4759413242340087\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5051369220423247\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5053076706984247\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5018361886806652\n",
      "avgfold accuracy: 0.5023973815657314\n",
      "standard deviation: 0.002418296526885001\n",
      "avg train time: 1.5439610481262207\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5053350660930497\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.502339042244267\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5020311550046638\n",
      "avgfold accuracy: 0.5018540214699049\n",
      "standard deviation: 0.0020477988232148457\n",
      "avg train time: 1.515784788131714\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5051204100380977\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5023607521135734\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 9, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014994286664858\n",
      "avgfold accuracy: 0.5020036289421667\n",
      "standard deviation: 0.0017876734911383902\n",
      "avg train time: 1.5373765468597411\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5425666334425127\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5317307883272037\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5380325522488547\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5205278646481979\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5492085496050174\n",
      "avgfold accuracy: 0.5364132776543572\n",
      "standard deviation: 0.00978240128633584\n",
      "avg train time: 1.521489429473877\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5012829952375377\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5013427362787951\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5022739126363471\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5085854495117426\n",
      "avgfold accuracy: 0.5026079035196446\n",
      "standard deviation: 0.0031152005932926796\n",
      "avg train time: 1.4884453773498536\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49948599787309467\n",
      "avgfold accuracy: 0.5005627556536457\n",
      "standard deviation: 0.0010300637575340694\n",
      "avg train time: 1.5723699569702148\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5025274329202964\n",
      "avgfold accuracy: 0.5007818015048683\n",
      "standard deviation: 0.0011230133734092932\n",
      "avg train time: 1.6062662124633789\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5053846021057309\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5014169394472102\n",
      "standard deviation: 0.002209090386666545\n",
      "avg train time: 1.5644264221191406\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5041144290998995\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.49957461892945765\n",
      "avgfold accuracy: 0.5013653907922121\n",
      "standard deviation: 0.0020206479703704697\n",
      "avg train time: 1.4998373985290527\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020466055959926\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5005112819205185\n",
      "standard deviation: 0.0011188093058326655\n",
      "avg train time: 1.4878482818603516\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5053185540888226\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.50496624478423\n",
      "avgfold accuracy: 0.5028963287587975\n",
      "standard deviation: 0.002039294331992909\n",
      "avg train time: 1.612386703491211\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.4997523199365939\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5032686429462562\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.4992835743128826\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5018980802405927\n",
      "tree depth: 9, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5047535542489588\n",
      "avgfold accuracy: 0.5017912343370569\n",
      "standard deviation: 0.0020695420863247197\n",
      "avg train time: 1.543685007095337\n",
      "tree depth: 9, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 9, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5031828825949818\n",
      "tree depth: 9, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5017962955116022\n",
      "tree depth: 9, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.49953320602588586\n",
      "tree depth: 9, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5043813458122343\n",
      "avgfold accuracy: 0.5028358520050145\n",
      "standard deviation: 0.0020247494747670817\n",
      "avg train time: 1.5362212657928467\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5486728226420607\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5277025794908182\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5518987504008479\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5264555023570329\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5340368227915534\n",
      "avgfold accuracy: 0.5377532955364627\n",
      "standard deviation: 0.010599992065555989\n",
      "avg train time: 1.519024658203125\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5047736579493292\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.49969126273541215\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5169867956912971\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5078469358573419\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5078764810608386\n",
      "avgfold accuracy: 0.5074350266588438\n",
      "standard deviation: 0.005632931846490024\n",
      "avg train time: 1.5296279907226562\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5056550286073301\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5022261213286623\n",
      "avgfold accuracy: 0.5018306574526308\n",
      "standard deviation: 0.00211631995328822\n",
      "avg train time: 1.5987485885620116\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5058938371697026\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.4998936547323644\n",
      "avgfold accuracy: 0.502183102089236\n",
      "standard deviation: 0.0022883782864108215\n",
      "avg train time: 1.6232386589050294\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5016462593305333\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5017372338946574\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5027081100224788\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5010787254685638\n",
      "standard deviation: 0.0012302620685701036\n",
      "avg train time: 1.578044557571411\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.4998679039661834\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.4997579513839273\n",
      "standard deviation: 0.0001675636515422355\n",
      "avg train time: 1.5232076168060302\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997523199365939\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.5008210187877176\n",
      "standard deviation: 0.0012445933801010388\n",
      "avg train time: 1.5402750968933105\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4997523199365939\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4991669620701879\n",
      "avgfold accuracy: 0.49960411580899217\n",
      "standard deviation: 0.00023318862036283083\n",
      "avg train time: 1.450923776626587\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5034906627117915\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.501497104911089\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.502491011329413\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.4995544239338001\n",
      "tree depth: 9, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5131334425171152\n",
      "avgfold accuracy: 0.5040333290806418\n",
      "standard deviation: 0.004732405725272288\n",
      "avg train time: 1.5189443111419678\n",
      "tree depth: 9, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 9, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 9, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5022087830284274\n",
      "tree depth: 9, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5039234679286709\n",
      "tree depth: 9, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.49934420418291386\n",
      "avgfold accuracy: 0.5020909056175696\n",
      "standard deviation: 0.002302480978199455\n",
      "avg train time: 1.5702399730682373\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5340166676173578\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5313828721231916\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5703779034334551\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5164983071799556\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.53524206915809\n",
      "avgfold accuracy: 0.5375035639024099\n",
      "standard deviation: 0.017757695129808588\n",
      "avg train time: 1.4763474941253663\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5031108866145688\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5014456487003244\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5036688530336996\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.509578005343008\n",
      "avgfold accuracy: 0.504006777317867\n",
      "standard deviation: 0.002886865456477185\n",
      "avg train time: 1.5048117637634277\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016627713347604\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5026429804145591\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5020311550046638\n",
      "avgfold accuracy: 0.5012146542928152\n",
      "standard deviation: 0.0011442453482007433\n",
      "avg train time: 1.6014090538024903\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.5005121316876928\n",
      "standard deviation: 0.0009718156550712213\n",
      "avg train time: 1.5522016525268554\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5023147423850253\n",
      "avgfold accuracy: 0.501105025355018\n",
      "standard deviation: 0.0015699445507999625\n",
      "avg train time: 1.5713946342468261\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5053350660930497\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5008592545894913\n",
      "standard deviation: 0.0022439057180414917\n",
      "avg train time: 1.4954869270324707\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49966975991545853\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5023856392301157\n",
      "avgfold accuracy: 0.5007318297550741\n",
      "standard deviation: 0.001157866798642353\n",
      "avg train time: 1.5330604553222655\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5050744851219332\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.49936192839418647\n",
      "avgfold accuracy: 0.5011327450581614\n",
      "standard deviation: 0.0021293396119379192\n",
      "avg train time: 1.5386029720306396\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5033090306652936\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5054379299142643\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 9, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5074405050707087\n",
      "avgfold accuracy: 0.5031233741070011\n",
      "standard deviation: 0.0030752565562236905\n",
      "avg train time: 1.5626291275024413\n",
      "tree depth: 9, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.49942207985205245\n",
      "tree depth: 9, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5031142743139623\n",
      "tree depth: 9, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.502121943551201\n",
      "tree depth: 9, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 9, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.49918468628146045\n",
      "avgfold accuracy: 0.501114264195191\n",
      "standard deviation: 0.001547856732242147\n",
      "avg train time: 1.5462798118591308\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5348257558244844\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5274844773764193\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5466127895717297\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5217469718355335\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5262489627870778\n",
      "avgfold accuracy: 0.531383791479049\n",
      "standard deviation: 0.008697228414507954\n",
      "avg train time: 1.541152334213257\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5086110727498891\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5049603127895194\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.49845799361928395\n",
      "avgfold accuracy: 0.5029964330724432\n",
      "standard deviation: 0.0034817521975843164\n",
      "avg train time: 1.5770694255828857\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5006393097763187\n",
      "standard deviation: 0.0010921995643270758\n",
      "avg train time: 1.592819881439209\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.500605764142323\n",
      "standard deviation: 0.001503382572497865\n",
      "avg train time: 1.63662691116333\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5034906627117915\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5017372338946574\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5009270703016162\n",
      "standard deviation: 0.0014852826682249462\n",
      "avg train time: 1.5672378063201904\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4998183679535022\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.5020311550046638\n",
      "avgfold accuracy: 0.5010646901710218\n",
      "standard deviation: 0.0010084120673263454\n",
      "avg train time: 1.5503005027770995\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5032594946526124\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5023324665962979\n",
      "avgfold accuracy: 0.5010160010080604\n",
      "standard deviation: 0.0014835050503656552\n",
      "avg train time: 1.6868406772613525\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5033544032975307\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5048067268827766\n",
      "avgfold accuracy: 0.5026149874213979\n",
      "standard deviation: 0.002450302718269367\n",
      "avg train time: 1.5933979511260987\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5014316032755813\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.49939212365941554\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.4999742150237987\n",
      "standard deviation: 0.0007437959244432564\n",
      "avg train time: 1.5923593521118165\n",
      "tree depth: 9, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5070969094531724\n",
      "tree depth: 9, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 9, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5054162200449577\n",
      "tree depth: 9, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5017071190693642\n",
      "tree depth: 9, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5048244510940493\n",
      "avgfold accuracy: 0.503764344549646\n",
      "standard deviation: 0.0026491287812943935\n",
      "avg train time: 1.5982384204864502\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5567274283404035\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5251763503644363\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5616947404048916\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5324892296054392\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5427534710738892\n",
      "avgfold accuracy: 0.543768243957812\n",
      "standard deviation: 0.013879511309001778\n",
      "avg train time: 1.4142541885375977\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5087761927921599\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.4986539881029916\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5009322539057627\n",
      "avgfold accuracy: 0.5019325717482876\n",
      "standard deviation: 0.0035734267372741924\n",
      "avg train time: 1.510165023803711\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5034576387033374\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.504877623727867\n",
      "avgfold accuracy: 0.5015713252880643\n",
      "standard deviation: 0.0021683866503420758\n",
      "avg train time: 1.5651185035705566\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5027081100224788\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49953917050691243\n",
      "avgfold accuracy: 0.5011925737874761\n",
      "standard deviation: 0.0012440665442759356\n",
      "avg train time: 1.6798118114471436\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5036392707498352\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5009507442486549\n",
      "standard deviation: 0.0015288246880325327\n",
      "avg train time: 1.55861496925354\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5024388118639335\n",
      "avgfold accuracy: 0.5011499045742912\n",
      "standard deviation: 0.0010849585892468445\n",
      "avg train time: 1.5908161640167235\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5004542637398308\n",
      "standard deviation: 0.0014949497103188625\n",
      "avg train time: 1.5659175395965577\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5015306753009439\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49929103154909604\n",
      "avgfold accuracy: 0.5003559811032872\n",
      "standard deviation: 0.0009932390209580787\n",
      "avg train time: 1.6000158309936523\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.503358566677975\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.49924015457426946\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4991669620701879\n",
      "avgfold accuracy: 0.5010285655368484\n",
      "standard deviation: 0.001620951751382857\n",
      "avg train time: 1.6352928161621094\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5051204100380977\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5014799528408341\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5022956225056537\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5062034705117205\n",
      "tree depth: 9, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5018361886806652\n",
      "avgfold accuracy: 0.5033871289153943\n",
      "standard deviation: 0.0019063140282282256\n",
      "avg train time: 1.6011110305786134\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5364604442429646\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5371483148542345\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5554535453262837\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.532319486342125\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5519060953463548\n",
      "avgfold accuracy: 0.5426575772223925\n",
      "standard deviation: 0.009218507897264548\n",
      "avg train time: 1.49770245552063\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5048727299746916\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5231411512926788\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5016859011614498\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5006132181028559\n",
      "avgfold accuracy: 0.5060317263798765\n",
      "standard deviation: 0.00872456180894955\n",
      "avg train time: 1.6070898532867433\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5077595408736155\n",
      "avgfold accuracy: 0.5025153940853565\n",
      "standard deviation: 0.002979246116400764\n",
      "avg train time: 1.743013334274292\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5024742602864787\n",
      "avgfold accuracy: 0.5012177592606024\n",
      "standard deviation: 0.001073353017223181\n",
      "avg train time: 1.5304353713989258\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5026864001531723\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.49953917050691243\n",
      "avgfold accuracy: 0.5007266174067693\n",
      "standard deviation: 0.0012388995759479427\n",
      "avg train time: 1.5349294185638427\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5034741507075644\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5004883996659142\n",
      "standard deviation: 0.0015026763600994133\n",
      "avg train time: 1.6487622261047363\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996367359070044\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5026212705452525\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.5003261733083924\n",
      "standard deviation: 0.0011520536224647164\n",
      "avg train time: 1.631760025024414\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5032429826483854\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5026646902838657\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5009901910557177\n",
      "standard deviation: 0.0016167005605768087\n",
      "avg train time: 1.5441389560699463\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5109343242537325\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5022522027670405\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5016010295297927\n",
      "tree depth: 9, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5028164041891536\n",
      "standard deviation: 0.004193047641728003\n",
      "avg train time: 1.450974178314209\n",
      "tree depth: 9, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5126466316011741\n",
      "tree depth: 9, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 9, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5021653632898142\n",
      "tree depth: 9, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 9, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5046117605587781\n",
      "avgfold accuracy: 0.5037603408247483\n",
      "standard deviation: 0.004804105234011192\n",
      "avg train time: 1.5052852630615234\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5339390486883965\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5243653282635424\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.554910798593619\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.533465253369496\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5382657799940194\n",
      "avgfold accuracy: 0.5369892417818146\n",
      "standard deviation: 0.010038895488250276\n",
      "avg train time: 1.471641969680786\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.503143910623023\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49969126273541215\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5050471522667458\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.506058016591446\n",
      "avgfold accuracy: 0.5031634409098611\n",
      "standard deviation: 0.002264448011987393\n",
      "avg train time: 1.5649777889251708\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5027949494997053\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5049307963616848\n",
      "avgfold accuracy: 0.5021970646393293\n",
      "standard deviation: 0.001981224573369021\n",
      "avg train time: 1.4867568016052246\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5017288193516687\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.5001558287058758\n",
      "standard deviation: 0.0007886537802441998\n",
      "avg train time: 1.4819876194000243\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5010577490791471\n",
      "standard deviation: 0.0015804710331632302\n",
      "avg train time: 1.5944727897644042\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5017543859649123\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.5012912461102088\n",
      "standard deviation: 0.0022201871015559856\n",
      "avg train time: 1.552995491027832\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5004497315777439\n",
      "standard deviation: 0.001486706283324072\n",
      "avg train time: 1.5268747806549072\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5014976512924897\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5019248097370281\n",
      "avgfold accuracy: 0.5005372707156954\n",
      "standard deviation: 0.0009713450284310567\n",
      "avg train time: 1.5594666004180908\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5013770404193049\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5017298434130296\n",
      "avgfold accuracy: 0.5014933521829593\n",
      "standard deviation: 0.002103985042843549\n",
      "avg train time: 1.610300350189209\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5032429826483854\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.50135988834905\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 9, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5019779823708459\n",
      "avgfold accuracy: 0.5022068181246844\n",
      "standard deviation: 0.0006336854502879108\n",
      "avg train time: 1.522373342514038\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5364224791423364\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5370282503624503\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.555279866371831\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5161588206533271\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5484003651781629\n",
      "avgfold accuracy: 0.5386579563416215\n",
      "standard deviation: 0.013304637008186687\n",
      "avg train time: 1.521148681640625\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5071134214573996\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5045695351420009\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5017071190693642\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5013221865537598\n",
      "avgfold accuracy: 0.5032487342548247\n",
      "standard deviation: 0.0022679370112711184\n",
      "avg train time: 1.560271978378296\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4996862719196856\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.5002802330468921\n",
      "standard deviation: 0.0009234878512816462\n",
      "avg train time: 1.9446237564086915\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.4998046111762407\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49997878209208574\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49936192839418647\n",
      "avgfold accuracy: 0.5001541177053248\n",
      "standard deviation: 0.0008231157943811878\n",
      "avg train time: 1.4842694759368897\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5054671621268663\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4997695852534562\n",
      "avgfold accuracy: 0.5013042241982125\n",
      "standard deviation: 0.0022023700757629818\n",
      "avg train time: 1.5033266067504882\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.502243845539935\n",
      "avgfold accuracy: 0.5010859512741621\n",
      "standard deviation: 0.0010614521153588631\n",
      "avg train time: 1.4908154010772705\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49965324791123145\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020466055959926\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49930875576036865\n",
      "avgfold accuracy: 0.5004904199432026\n",
      "standard deviation: 0.001116198255579338\n",
      "avg train time: 1.5358198165893555\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5027298198917854\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49934420418291386\n",
      "avgfold accuracy: 0.5014911784543448\n",
      "standard deviation: 0.001622328050645711\n",
      "avg train time: 1.4635263919830321\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49960371189855024\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.4994572532673353\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 9, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49906061680255226\n",
      "avgfold accuracy: 0.49997044146882824\n",
      "standard deviation: 0.0010048873898154168\n",
      "avg train time: 1.4855708122253417\n",
      "tree depth: 9, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5012829952375377\n",
      "tree depth: 9, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5032514908760013\n",
      "tree depth: 9, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5054162200449577\n",
      "tree depth: 9, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.506012509340492\n",
      "tree depth: 9, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5045408637136877\n",
      "avgfold accuracy: 0.5041008158425353\n",
      "standard deviation: 0.0016875999665509063\n",
      "avg train time: 1.5491114616394044\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5362193939994375\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.540091068465648\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5542868360245724\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5165928652478344\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5535544469947065\n",
      "avgfold accuracy: 0.5401489221464398\n",
      "standard deviation: 0.013783476052141471\n",
      "avg train time: 1.3790771484375\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5068657413939934\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.49967411066515727\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.49908818548912337\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.4993846806704859\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.5067847092536225\n",
      "avgfold accuracy: 0.5023594854944765\n",
      "standard deviation: 0.0036510560935226356\n",
      "avg train time: 1.4395870685577392\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5020311550046638\n",
      "avgfold accuracy: 0.500976932932956\n",
      "standard deviation: 0.001504081320064753\n",
      "avg train time: 1.5611696243286133\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5016957953432145\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5028817889769316\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5008412407505006\n",
      "standard deviation: 0.001242185306500152\n",
      "avg train time: 1.5291474342346192\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5053846021057309\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5020253876880784\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.501354201606478\n",
      "standard deviation: 0.0021955533755483063\n",
      "avg train time: 1.5282761573791503\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5035732227329268\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.5009405610429047\n",
      "standard deviation: 0.001585142876361919\n",
      "avg train time: 1.5545347690582276\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49957461892945765\n",
      "avgfold accuracy: 0.500815420004859\n",
      "standard deviation: 0.0015082698545386174\n",
      "avg train time: 1.5258769989013672\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5052359940676873\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5027949494997053\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5047890026715041\n",
      "avgfold accuracy: 0.5028448095686019\n",
      "standard deviation: 0.0020029296858297347\n",
      "avg train time: 1.5358157634735108\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5033420546737478\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.49969126273541215\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.501622247437707\n",
      "tree depth: 9, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5074582292819813\n",
      "avgfold accuracy: 0.5029296450393749\n",
      "standard deviation: 0.0025702435475075695\n",
      "avg train time: 1.595318365097046\n",
      "tree depth: 9, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5069483014151288\n",
      "tree depth: 9, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 9, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5022522027670405\n",
      "tree depth: 9, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 9, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5020079174678246\n",
      "standard deviation: 0.0026972834782741997\n",
      "avg train time: 1.5715880393981934\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5395003417484511\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5258452811043767\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5427757663453082\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5209406913347909\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5385954107165136\n",
      "avgfold accuracy: 0.5335314982498882\n",
      "standard deviation: 0.008536195259964451\n",
      "avg train time: 1.4713510036468507\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5019482645967484\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5038598142049281\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5121231624745771\n",
      "avgfold accuracy: 0.5038653240426483\n",
      "standard deviation: 0.004316536752448923\n",
      "avg train time: 1.478642177581787\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035732227329268\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5018893613144829\n",
      "avgfold accuracy: 0.5009647884998458\n",
      "standard deviation: 0.0015413850299471885\n",
      "avg train time: 1.6058170318603515\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.5005344083734719\n",
      "standard deviation: 0.0015075680383685095\n",
      "avg train time: 1.545567226409912\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5026429804145591\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49980503367600143\n",
      "avgfold accuracy: 0.5011625749856828\n",
      "standard deviation: 0.0016083690738424394\n",
      "avg train time: 1.6522850513458252\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5015471873051709\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5000895041922597\n",
      "standard deviation: 0.0007396200006665203\n",
      "avg train time: 1.5679913997650146\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034576387033374\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5026864001531723\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.502243845539935\n",
      "avgfold accuracy: 0.5016103149975717\n",
      "standard deviation: 0.0015037517850974061\n",
      "avg train time: 2.0309011936187744\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49943554339802876\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5040083395603281\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.50496624478423\n",
      "avgfold accuracy: 0.502362621981004\n",
      "standard deviation: 0.0022667556909084473\n",
      "avg train time: 2.5129132747650145\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4996367359070044\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4992835743128826\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5018344265168498\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5018361886806652\n",
      "avgfold accuracy: 0.5008313277219022\n",
      "standard deviation: 0.001129412677197689\n",
      "avg train time: 2.3888078212738035\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5053846021057309\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5033887074380404\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5021870731591208\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5039022500207566\n",
      "tree depth: 9, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.49920241049273306\n",
      "avgfold accuracy: 0.5028130086432763\n",
      "standard deviation: 0.0020764962341347244\n",
      "avg train time: 1.5926205158233642\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5285049105699844\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5251077420834168\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5507481273275988\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5185970350279984\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5433738184684301\n",
      "avgfold accuracy: 0.5332663266954858\n",
      "standard deviation: 0.01193479323406463\n",
      "avg train time: 1.3796169757843018\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5068822533982206\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5050688621360523\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.503626417217871\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5007372875817641\n",
      "avgfold accuracy: 0.5032320903403228\n",
      "standard deviation: 0.002628499946452895\n",
      "avg train time: 1.497389316558838\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5001903472235518\n",
      "standard deviation: 0.000793322047047528\n",
      "avg train time: 1.6226463794708252\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.4998513919619563\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.502860079107625\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.49997878209208574\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5024210876526609\n",
      "avgfold accuracy: 0.5009948248504579\n",
      "standard deviation: 0.001351643328824842\n",
      "avg train time: 1.5400790691375732\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.5001550973940937\n",
      "standard deviation: 0.0007158467345579994\n",
      "avg train time: 1.5068321704864502\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5016132353220791\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5007655319420503\n",
      "standard deviation: 0.001112843815450805\n",
      "avg train time: 1.589390516281128\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5054671621268663\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5033887074380404\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5079013345637963\n",
      "avgfold accuracy: 0.5032612432974375\n",
      "standard deviation: 0.003185209505724693\n",
      "avg train time: 1.5878350734710693\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5035236867202456\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5023324665962979\n",
      "avgfold accuracy: 0.5010249016922774\n",
      "standard deviation: 0.0016022874944322803\n",
      "avg train time: 1.578215980529785\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5014316032755813\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5031828825949818\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.4995006730059485\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.4995756418417144\n",
      "tree depth: 9, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.49934420418291386\n",
      "avgfold accuracy: 0.500607000980228\n",
      "standard deviation: 0.0014964928365551423\n",
      "avg train time: 1.5222136974334717\n",
      "tree depth: 9, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5032264706441584\n",
      "tree depth: 9, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5033372512272758\n",
      "tree depth: 9, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.505242541090505\n",
      "tree depth: 9, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5060761630642349\n",
      "tree depth: 9, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5016766707792117\n",
      "avgfold accuracy: 0.5039118193610772\n",
      "standard deviation: 0.0015653066519852982\n",
      "avg train time: 1.552581262588501\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5402433819386694\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5219077074181802\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5579445566556966\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5262317918061975\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5481096485061163\n",
      "avgfold accuracy: 0.538887417264972\n",
      "standard deviation: 0.013405507474564203\n",
      "avg train time: 1.5016398906707764\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.503143910623023\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49897963614259044\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5098511056375059\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5065542945070788\n",
      "avgfold accuracy: 0.5036577635853259\n",
      "standard deviation: 0.004100972198685451\n",
      "avg train time: 1.590287208557129\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5072950535038974\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5018323810371649\n",
      "standard deviation: 0.002890560175987903\n",
      "avg train time: 1.5124249458312988\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5072620294954432\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49955689471818504\n",
      "avgfold accuracy: 0.5012673428692956\n",
      "standard deviation: 0.003000232833577662\n",
      "avg train time: 1.550024652481079\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49961006735200286\n",
      "avgfold accuracy: 0.5001837935674738\n",
      "standard deviation: 0.0007708292801126275\n",
      "avg train time: 1.5322569370269776\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5054341381184121\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49946827366182206\n",
      "avgfold accuracy: 0.5012241083427498\n",
      "standard deviation: 0.002245275094474152\n",
      "avg train time: 1.6444427013397216\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5018344265168498\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5004712128667402\n",
      "standard deviation: 0.0010129106653476408\n",
      "avg train time: 1.563899564743042\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5016297473263063\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4995440927445617\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49953917050691243\n",
      "avgfold accuracy: 0.5004363309250508\n",
      "standard deviation: 0.0009758754913277672\n",
      "avg train time: 1.4894440174102783\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5034576387033374\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49939212365941554\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49930875576036865\n",
      "avgfold accuracy: 0.5007229929315232\n",
      "standard deviation: 0.0016135882468605936\n",
      "avg train time: 1.5618805885314941\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.503375078682202\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5013427362787951\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.499370413790109\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 9, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5018893613144829\n",
      "avgfold accuracy: 0.5011361078709579\n",
      "standard deviation: 0.0014693328724507382\n",
      "avg train time: 1.5359429359436034\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5372596502657432\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5248161568890839\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5517467813157018\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5262857590937186\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5389287051027906\n",
      "avgfold accuracy: 0.5358074105334076\n",
      "standard deviation: 0.009771228415887923\n",
      "avg train time: 1.4282448768615723\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5052029700592331\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5050254423974392\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5013251967269071\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.5039382405304194\n",
      "avgfold accuracy: 0.5034046517531194\n",
      "standard deviation: 0.0016720525979118034\n",
      "avg train time: 1.4333206176757813\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035732227329268\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5021020518497541\n",
      "avgfold accuracy: 0.5010282423493082\n",
      "standard deviation: 0.0015498904152169775\n",
      "avg train time: 1.5050730228424072\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.5001514014241344\n",
      "standard deviation: 0.0007131862460385238\n",
      "avg train time: 1.5633564472198487\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4998046111762407\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5001588686418322\n",
      "standard deviation: 0.0007053124857922583\n",
      "avg train time: 1.5944474697113038\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4998018559492751\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5024033634413884\n",
      "avgfold accuracy: 0.5003196584059021\n",
      "standard deviation: 0.0010455945864379023\n",
      "avg train time: 1.5232439517974854\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5036227587456081\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5009497395778613\n",
      "standard deviation: 0.0016033382598663116\n",
      "avg train time: 1.562617826461792\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5034576387033374\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5058069976924763\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5020138073075857\n",
      "standard deviation: 0.002391385624080328\n",
      "avg train time: 1.5572700500488281\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 9, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4993796526054591\n",
      "avgfold accuracy: 0.5009755533337893\n",
      "standard deviation: 0.001659210716278635\n",
      "avg train time: 1.5577063083648681\n",
      "tree depth: 9, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 9, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.4996569585949024\n",
      "tree depth: 9, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5023607521135734\n",
      "tree depth: 9, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 9, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.4992555831265509\n",
      "avgfold accuracy: 0.5016593145400096\n",
      "standard deviation: 0.0021541966964832497\n",
      "avg train time: 1.530090856552124\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.527309416445756\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5036851674312879\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.522419101964717\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5080051476489635\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.1 valid accuracy for this fold, 0.5242852389996019\n",
      "avgfold accuracy: 0.5171408144980651\n",
      "standard deviation: 0.009453236825265731\n",
      "avg train time: 1.9719597816467285\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5014316032755813\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5027081100224788\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.2 valid accuracy for this fold, 0.5048953479391396\n",
      "avgfold accuracy: 0.5021872637191669\n",
      "standard deviation: 0.0016403934812143286\n",
      "avg train time: 1.9141627311706544\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5024210876526609\n",
      "avgfold accuracy: 0.5007723907867037\n",
      "standard deviation: 0.0010915682818640887\n",
      "avg train time: 1.8090949535369873\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5055166981395475\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5017200818244025\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.5021102593197354\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.4 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.5017448130620974\n",
      "standard deviation: 0.0021362646553564975\n",
      "avg train time: 1.7229662895202638\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5036888067625164\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.5028383692383184\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.5 valid accuracy for this fold, 0.4995214462956398\n",
      "avgfold accuracy: 0.5011798416442914\n",
      "standard deviation: 0.001728810227789646\n",
      "avg train time: 1.6395441532135009\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5016957953432145\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.6 valid accuracy for this fold, 0.4993796526054591\n",
      "avgfold accuracy: 0.5008898709865255\n",
      "standard deviation: 0.0010980467857136234\n",
      "avg train time: 1.6681862831115724\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49946827366182206\n",
      "avgfold accuracy: 0.5000576652710412\n",
      "standard deviation: 0.0007837224173475979\n",
      "avg train time: 1.6177908897399902\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5090899208724743\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5026646902838657\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5040507753761566\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.8 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5035890350737733\n",
      "standard deviation: 0.0030532565580144587\n",
      "avg train time: 1.5094717025756836\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5032925186610666\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.49908818548912337\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.5018344265168498\n",
      "tree depth: 10, lambda: 0.0, learning rate: 0.9 valid accuracy for this fold, 0.50496624478423\n",
      "avgfold accuracy: 0.5021631393848797\n",
      "standard deviation: 0.0019481241964398538\n",
      "avg train time: 1.5248965263366698\n",
      "tree depth: 10, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5052690180761413\n",
      "tree depth: 10, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 10, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5024258817214933\n",
      "tree depth: 10, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.4995756418417144\n",
      "tree depth: 10, lambda: 0.0, learning rate: 1.0 valid accuracy for this fold, 0.5015703255115762\n",
      "avgfold accuracy: 0.5020778856545558\n",
      "standard deviation: 0.0018495109220149606\n",
      "avg train time: 1.524052667617798\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5338003228347008\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5090169336070443\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5112610138352027\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5101790606924418\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.1 valid accuracy for this fold, 0.5124385346137014\n",
      "avgfold accuracy: 0.5153391731166181\n",
      "standard deviation: 0.009300071057892167\n",
      "avg train time: 1.7788560390472412\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.509255040914745\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5027298198917854\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.2 valid accuracy for this fold, 0.5020134307933911\n",
      "avgfold accuracy: 0.5027382663549137\n",
      "standard deviation: 0.0034559725271780003\n",
      "avg train time: 1.7741726398468018\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5024565360752061\n",
      "avgfold accuracy: 0.5007604661309129\n",
      "standard deviation: 0.001107570380291136\n",
      "avg train time: 1.7469388961791992\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5036557827540622\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.5017200818244025\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.4 valid accuracy for this fold, 0.49966323998582063\n",
      "avgfold accuracy: 0.5009564059724705\n",
      "standard deviation: 0.0015425379836899854\n",
      "avg train time: 1.6899940967559814\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5036392707498352\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5017200818244025\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.5 valid accuracy for this fold, 0.5024565360752061\n",
      "avgfold accuracy: 0.5015067320696917\n",
      "standard deviation: 0.0014809448150910844\n",
      "avg train time: 1.6112488746643066\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.5028383692383184\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.6 valid accuracy for this fold, 0.49953917050691243\n",
      "avgfold accuracy: 0.5007557642859319\n",
      "standard deviation: 0.0012958150777396572\n",
      "avg train time: 1.5734671115875245\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015306753009439\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5000769777745241\n",
      "standard deviation: 0.0007413075117744881\n",
      "avg train time: 1.543794584274292\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5014316032755813\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.8 valid accuracy for this fold, 0.5022261213286623\n",
      "avgfold accuracy: 0.5011251252789337\n",
      "standard deviation: 0.001205933831998877\n",
      "avg train time: 1.549542236328125\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5052690180761413\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.4994789631366419\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.4995119881179716\n",
      "tree depth: 10, lambda: 0.1, learning rate: 0.9 valid accuracy for this fold, 0.5019602581595733\n",
      "avgfold accuracy: 0.5015571881364873\n",
      "standard deviation: 0.002118588298217233\n",
      "avg train time: 1.5216829776763916\n",
      "tree depth: 10, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5086936327710245\n",
      "tree depth: 10, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 10, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.49904476575051016\n",
      "tree depth: 10, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5058852018930063\n",
      "tree depth: 10, lambda: 0.1, learning rate: 1.0 valid accuracy for this fold, 0.5046472089813232\n",
      "avgfold accuracy: 0.5035958448403062\n",
      "standard deviation: 0.003692110432100733\n",
      "avg train time: 1.502430009841919\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5219908623569335\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5052680326936514\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5133612475170971\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5118649618538916\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.1 valid accuracy for this fold, 0.5179542299473818\n",
      "avgfold accuracy: 0.5140878668737912\n",
      "standard deviation: 0.00566859521231799\n",
      "avg train time: 1.6716294288635254\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.4996862719196856\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.2 valid accuracy for this fold, 0.5019248097370281\n",
      "avgfold accuracy: 0.5006178259739444\n",
      "standard deviation: 0.0010526813987210425\n",
      "avg train time: 1.7161867141723632\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036392707498352\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5015311133358386\n",
      "standard deviation: 0.0014741411417920523\n",
      "avg train time: 1.7097425937652588\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.5055332101437745\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.4 valid accuracy for this fold, 0.4997695852534562\n",
      "avgfold accuracy: 0.5009763489007015\n",
      "standard deviation: 0.0022787718893596993\n",
      "avg train time: 1.681225633621216\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.5 valid accuracy for this fold, 0.5023147423850253\n",
      "avgfold accuracy: 0.5014701804403833\n",
      "standard deviation: 0.0022067568120968523\n",
      "avg train time: 1.5999958038330078\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5034741507075644\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.5034058595082953\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.6 valid accuracy for this fold, 0.4995214462956398\n",
      "avgfold accuracy: 0.5012114100741896\n",
      "standard deviation: 0.0018234244139282876\n",
      "avg train time: 1.6083065032958985\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034741507075644\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4993796526054591\n",
      "avgfold accuracy: 0.5008689082000805\n",
      "standard deviation: 0.0015798664325195413\n",
      "avg train time: 1.5756127834320068\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5015141632967167\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.5027732396303987\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.8 valid accuracy for this fold, 0.49936192839418647\n",
      "avgfold accuracy: 0.5006514999728583\n",
      "standard deviation: 0.0012924457410158849\n",
      "avg train time: 1.5411449432373048\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.502339042244267\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 10, lambda: 0.2, learning rate: 0.9 valid accuracy for this fold, 0.49918468628146045\n",
      "avgfold accuracy: 0.5020206617550966\n",
      "standard deviation: 0.0019542732754149286\n",
      "avg train time: 1.4914488792419434\n",
      "tree depth: 10, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5032429826483854\n",
      "tree depth: 10, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 10, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.49887108679605746\n",
      "tree depth: 10, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 10, lambda: 0.2, learning rate: 1.0 valid accuracy for this fold, 0.5070328482114389\n",
      "avgfold accuracy: 0.5020445683053455\n",
      "standard deviation: 0.0029119102074202413\n",
      "avg train time: 1.436621856689453\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5298308120003242\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5075198286959551\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5342936157813487\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5080263655568779\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.1 valid accuracy for this fold, 0.5126406302293849\n",
      "avgfold accuracy: 0.5184622504527782\n",
      "standard deviation: 0.01133500656653629\n",
      "avg train time: 1.6949988842010497\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.503375078682202\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.4995006730059485\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.2 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5008374944192224\n",
      "standard deviation: 0.0015769247903305052\n",
      "avg train time: 1.6637043952941895\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036722947582893\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5006512037135713\n",
      "standard deviation: 0.0015126710352163168\n",
      "avg train time: 1.731179428100586\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.4999009279746376\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.4 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.4998431612417913\n",
      "standard deviation: 0.0001002964518935298\n",
      "avg train time: 1.6481265068054198\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.5035732227329268\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.5 valid accuracy for this fold, 0.49961006735200286\n",
      "avgfold accuracy: 0.5005704130760282\n",
      "standard deviation: 0.0015058007765844641\n",
      "avg train time: 1.600657320022583\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.4998018559492751\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5026864001531723\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.6 valid accuracy for this fold, 0.49966323998582063\n",
      "avgfold accuracy: 0.5008343858438139\n",
      "standard deviation: 0.0012850849458066861\n",
      "avg train time: 1.6034549713134765\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5035071747160186\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5019829518722497\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5024033634413884\n",
      "avgfold accuracy: 0.501500758301618\n",
      "standard deviation: 0.0014728804634168541\n",
      "avg train time: 1.626232671737671\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4998018559492751\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.8 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.49975056834947384\n",
      "standard deviation: 8.828767231576699e-05\n",
      "avg train time: 1.5784212589263915\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5015828652623635\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5053728003063446\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5015373758060498\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 0.9 valid accuracy for this fold, 0.5020488792159363\n",
      "avgfold accuracy: 0.5024211239800185\n",
      "standard deviation: 0.0014879398427548533\n",
      "avg train time: 1.630105447769165\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5014646272840355\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.49939212365941554\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5039659037444995\n",
      "tree depth: 10, lambda: 0.30000000000000004, learning rate: 1.0 valid accuracy for this fold, 0.5074050566481635\n",
      "avgfold accuracy: 0.5027381024213387\n",
      "standard deviation: 0.002747058970806161\n",
      "avg train time: 1.4886744022369385\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.523240833586017\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5094628874336712\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.525018662640663\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5120559230251202\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.1 valid accuracy for this fold, 0.5269190171875341\n",
      "avgfold accuracy: 0.519339464774601\n",
      "standard deviation: 0.007148710963142536\n",
      "avg train time: 1.6880258083343507\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5053515780972767\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5056984483459434\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5016434653456213\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.2 valid accuracy for this fold, 0.5046826574038684\n",
      "avgfold accuracy: 0.50346836901044\n",
      "standard deviation: 0.00226244200801043\n",
      "avg train time: 1.66756329536438\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5054671621268663\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49997878209208574\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4996277915632754\n",
      "avgfold accuracy: 0.5009583014962484\n",
      "standard deviation: 0.0022590615337676877\n",
      "avg train time: 1.6767126083374024\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4998679039661834\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.4 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.4998662615443097\n",
      "standard deviation: 8.570371302377498e-05\n",
      "avg train time: 1.6752968311309815\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.4999782901306934\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.5 valid accuracy for this fold, 0.49982275788727404\n",
      "avgfold accuracy: 0.5006576237974363\n",
      "standard deviation: 0.0014753261549781704\n",
      "avg train time: 1.5883202075958252\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5055827461564558\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.6 valid accuracy for this fold, 0.4996277915632754\n",
      "avgfold accuracy: 0.5013903388635802\n",
      "standard deviation: 0.0022590522276289006\n",
      "avg train time: 1.592247200012207\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5014481152798085\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.5000860295178906\n",
      "standard deviation: 0.0006862979824117066\n",
      "avg train time: 1.5501901149749755\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4997523199365939\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.5014113445598146\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.8 valid accuracy for this fold, 0.4992733073378235\n",
      "avgfold accuracy: 0.49997982894309806\n",
      "standard deviation: 0.0007389659471485891\n",
      "avg train time: 1.472837257385254\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5051864580550061\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.4992835743128826\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5014525041743928\n",
      "tree depth: 10, lambda: 0.4, learning rate: 0.9 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.5015501454817951\n",
      "standard deviation: 0.0021017182826885065\n",
      "avg train time: 1.5306210041046142\n",
      "tree depth: 10, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5031769346314771\n",
      "tree depth: 10, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 10, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 10, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5015798116218785\n",
      "tree depth: 10, lambda: 0.4, learning rate: 1.0 valid accuracy for this fold, 0.5018716371032104\n",
      "avgfold accuracy: 0.5012088750274384\n",
      "standard deviation: 0.0013404786460464626\n",
      "avg train time: 1.4409698009490968\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5197006348615444\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5072968517826417\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5188208873006681\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.508111237188535\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.1 valid accuracy for this fold, 0.5192835457928268\n",
      "avgfold accuracy: 0.5146426313852432\n",
      "standard deviation: 0.005678008388904759\n",
      "avg train time: 1.600656223297119\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.503391590686429\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5027298198917854\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.501855644424764\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.2 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.5020229110550556\n",
      "standard deviation: 0.0011407262588164625\n",
      "avg train time: 1.6509507656097413\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016297473263063\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997873094647288\n",
      "avgfold accuracy: 0.5002300939465548\n",
      "standard deviation: 0.0007019139390250312\n",
      "avg train time: 1.640190601348877\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5036557827540622\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.5028383692383184\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.49997878209208574\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.4 valid accuracy for this fold, 0.499734136830911\n",
      "avgfold accuracy: 0.5012276925268716\n",
      "standard deviation: 0.0016709747631707657\n",
      "avg train time: 1.5900964260101318\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.5036227587456081\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.49993487039208023\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.5 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5006239979407264\n",
      "standard deviation: 0.0015021581311761407\n",
      "avg train time: 1.5776007175445557\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4998046111762407\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.6 valid accuracy for this fold, 0.5025097087090239\n",
      "avgfold accuracy: 0.5007775027369361\n",
      "standard deviation: 0.0011086486900629949\n",
      "avg train time: 1.5523107528686524\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034081026906562\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5062246884196349\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49957461892945765\n",
      "avgfold accuracy: 0.5023277857372491\n",
      "standard deviation: 0.0024447436895981205\n",
      "avg train time: 1.6115285396575927\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5033544032975307\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.4998046111762407\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5041144290998995\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.8 valid accuracy for this fold, 0.5023501908075705\n",
      "avgfold accuracy: 0.5029752280906311\n",
      "standard deviation: 0.0018477570730005043\n",
      "avg train time: 1.571756362915039\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5015306753009439\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5053945101756512\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 10, lambda: 0.5, learning rate: 0.9 valid accuracy for this fold, 0.5049485205729575\n",
      "avgfold accuracy: 0.5026792436367544\n",
      "standard deviation: 0.0021479917607248058\n",
      "avg train time: 1.5897194385528564\n",
      "tree depth: 10, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.5033420546737478\n",
      "tree depth: 10, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 10, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.4994572532673353\n",
      "tree depth: 10, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.499469552302143\n",
      "tree depth: 10, lambda: 0.5, learning rate: 1.0 valid accuracy for this fold, 0.4988833746898263\n",
      "avgfold accuracy: 0.500172129947744\n",
      "standard deviation: 0.001608042824841599\n",
      "avg train time: 1.4961359024047851\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5221394703949771\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5034107343072098\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5361984606394838\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5134872092915986\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5282589279525665\n",
      "avgfold accuracy: 0.5206989605171672\n",
      "standard deviation: 0.011404865642349367\n",
      "avg train time: 1.6421411037445068\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5058287075617829\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4991669620701879\n",
      "avgfold accuracy: 0.5016167949836535\n",
      "standard deviation: 0.002584209932104469\n",
      "avg train time: 1.7067264556884765\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.501794867368577\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5002531661316486\n",
      "standard deviation: 0.0007761488289645368\n",
      "avg train time: 1.7085309028625488\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4998679039661834\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5025983297653869\n",
      "avgfold accuracy: 0.5004400277269403\n",
      "standard deviation: 0.0010794749299664049\n",
      "avg train time: 1.638939094543457\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5053680901015039\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5026869508217499\n",
      "avgfold accuracy: 0.5015696083502152\n",
      "standard deviation: 0.002178884541542862\n",
      "avg train time: 1.5718742847442626\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5054176261141851\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5009318776801949\n",
      "standard deviation: 0.0022447759903667136\n",
      "avg train time: 1.6282735824584962\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016462593305333\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5028166593690118\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.500706411419624\n",
      "standard deviation: 0.0013032095744047012\n",
      "avg train time: 1.5740911483764648\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5014976512924897\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5020678235039069\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49941510102800424\n",
      "avgfold accuracy: 0.5004820482004363\n",
      "standard deviation: 0.0010834651935043883\n",
      "avg train time: 1.4913845539093018\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5034246146948832\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5054162200449577\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5040295574682423\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.5033180371203457\n",
      "standard deviation: 0.0013643616182890232\n",
      "avg train time: 1.5323341846466065\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5070969094531724\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5015314090515989\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4994572532673353\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5059488556167492\n",
      "tree depth: 10, lambda: 0.6000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49895427153491667\n",
      "avgfold accuracy: 0.5025977397847545\n",
      "standard deviation: 0.0033390742043505877\n",
      "avg train time: 1.4973487854003906\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5168704523188358\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5070738748693283\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.535612294168206\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5013136652552145\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5250296558730511\n",
      "avgfold accuracy: 0.5171799884969271\n",
      "standard deviation: 0.01229094568196638\n",
      "avg train time: 1.5691386222839356\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5053680901015039\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49922013470400567\n",
      "avgfold accuracy: 0.501249918895164\n",
      "standard deviation: 0.0022687499792727966\n",
      "avg train time: 1.6246078491210938\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5018113793728041\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.5002484512522496\n",
      "standard deviation: 0.0007900825432360472\n",
      "avg train time: 1.7174501419067383\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5054836741310933\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5009771365028113\n",
      "standard deviation: 0.0022548531489624203\n",
      "avg train time: 1.649247646331787\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5018609153854853\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999131605227736\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.5002431088557966\n",
      "standard deviation: 0.0008161723106670693\n",
      "avg train time: 1.5699731349945067\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5036227587456081\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5009174606405276\n",
      "standard deviation: 0.0015242328242114915\n",
      "avg train time: 1.577624464035034\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5027732396303987\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5024033634413884\n",
      "avgfold accuracy: 0.5019994872698108\n",
      "standard deviation: 0.0012692088519969226\n",
      "avg train time: 1.5560094833374023\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5054011141099579\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5051080384744108\n",
      "avgfold accuracy: 0.5023756509385238\n",
      "standard deviation: 0.0024907212648769676\n",
      "avg train time: 1.556261920928955\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5032429826483854\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5052208312211984\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49974538510502864\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5021020518497541\n",
      "avgfold accuracy: 0.50239254487355\n",
      "standard deviation: 0.0018090591934212161\n",
      "avg train time: 1.5454411029815673\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5032264706441584\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.508471687976342\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5015585937139642\n",
      "tree depth: 10, lambda: 0.7000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4990960652250975\n",
      "avgfold accuracy: 0.5024259681292498\n",
      "standard deviation: 0.0033481049658514008\n",
      "avg train time: 1.4730116367340087\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5232573455902441\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.511942535148203\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5407245760428715\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5059373241450567\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.1 valid accuracy for this fold, 0.5158521780976277\n",
      "avgfold accuracy: 0.5195427918048006\n",
      "standard deviation: 0.011991323316091582\n",
      "avg train time: 1.620626449584961\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5025995606759459\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.2 valid accuracy for this fold, 0.502066603427209\n",
      "avgfold accuracy: 0.5023723943874188\n",
      "standard deviation: 0.0017131938538838922\n",
      "avg train time: 1.752871036529541\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016957953432145\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5007692148092504\n",
      "standard deviation: 0.0011363726899992828\n",
      "avg train time: 1.6505122661590577\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5055001861353204\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.4 valid accuracy for this fold, 0.5025983297653869\n",
      "avgfold accuracy: 0.501577490178174\n",
      "standard deviation: 0.0022173092808654687\n",
      "avg train time: 1.706725263595581\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.5016297473263063\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.5 valid accuracy for this fold, 0.499734136830911\n",
      "avgfold accuracy: 0.5002169405655505\n",
      "standard deviation: 0.0007096174349206841\n",
      "avg train time: 1.6863429546356201\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5034230115785502\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.6 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5014675091602373\n",
      "standard deviation: 0.0014435009274831824\n",
      "avg train time: 1.636460304260254\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5054011141099579\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5024210876526609\n",
      "avgfold accuracy: 0.5017988904301118\n",
      "standard deviation: 0.002087601245568821\n",
      "avg train time: 1.5819897651672363\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.5054176261141851\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.8 valid accuracy for this fold, 0.49961006735200286\n",
      "avgfold accuracy: 0.5009282877348735\n",
      "standard deviation: 0.00224707092951596\n",
      "avg train time: 1.5911095142364502\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5015306753009439\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.501770772793107\n",
      "tree depth: 10, lambda: 0.8, learning rate: 0.9 valid accuracy for this fold, 0.5051966595307739\n",
      "avgfold accuracy: 0.502167861624199\n",
      "standard deviation: 0.0017509392054570051\n",
      "avg train time: 1.5276854515075684\n",
      "tree depth: 10, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5051699460507789\n",
      "tree depth: 10, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5013941924895597\n",
      "tree depth: 10, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.4993487039208024\n",
      "tree depth: 10, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5039022500207566\n",
      "tree depth: 10, lambda: 0.8, learning rate: 1.0 valid accuracy for this fold, 0.5044522426573247\n",
      "avgfold accuracy: 0.5028534670278445\n",
      "standard deviation: 0.002164945771048613\n",
      "avg train time: 1.6146849632263183\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5110565381032011\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5125600096773786\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5136217659487762\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5099032278895562\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.1 valid accuracy for this fold, 0.5264759119057191\n",
      "avgfold accuracy: 0.5147234907049262\n",
      "standard deviation: 0.0060115776077208785\n",
      "avg train time: 1.5937097549438477\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.2 valid accuracy for this fold, 0.5049130721504123\n",
      "avgfold accuracy: 0.5015299715515408\n",
      "standard deviation: 0.0019035702699587407\n",
      "avg train time: 1.672376823425293\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036557827540622\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999131605227736\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5006326517137366\n",
      "standard deviation: 0.0015130448347390575\n",
      "avg train time: 1.7444072723388673\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.4 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5007865139333443\n",
      "standard deviation: 0.0011170629296334922\n",
      "avg train time: 1.6216130256652832\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.5 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.5002271444631932\n",
      "standard deviation: 0.0007755218895336323\n",
      "avg train time: 1.6117231845855713\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5016957953432145\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5017372338946574\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5020466055959926\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.6 valid accuracy for this fold, 0.5025274329202964\n",
      "avgfold accuracy: 0.5015666777599417\n",
      "standard deviation: 0.0009195633142640878\n",
      "avg train time: 1.675340461730957\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49978534394504803\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.4997773065395538\n",
      "standard deviation: 0.00017657971395001348\n",
      "avg train time: 1.6024928092956543\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.8 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5004727845141566\n",
      "standard deviation: 0.0015383200633862523\n",
      "avg train time: 1.56335129737854\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.49975987101643166\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 10, lambda: 0.9, learning rate: 0.9 valid accuracy for this fold, 0.50496624478423\n",
      "avgfold accuracy: 0.5018518359752369\n",
      "standard deviation: 0.0026758705986649083\n",
      "avg train time: 1.537806749343872\n",
      "tree depth: 10, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5014150912713543\n",
      "tree depth: 10, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.4997427189461768\n",
      "tree depth: 10, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.49924015457426946\n",
      "tree depth: 10, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 10, lambda: 0.9, learning rate: 1.0 valid accuracy for this fold, 0.5018361886806652\n",
      "avgfold accuracy: 0.5003789333891675\n",
      "standard deviation: 0.001040666067819896\n",
      "avg train time: 1.5564536571502685\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5196510988488632\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.508914021185515\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5228098796122356\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.5125439349071486\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.1 valid accuracy for this fold, 0.520853871304403\n",
      "avgfold accuracy: 0.5169545611716331\n",
      "standard deviation: 0.00530781680750999\n",
      "avg train time: 1.5566094398498536\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5092880649231992\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5019617339643355\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.2 valid accuracy for this fold, 0.5049307963616848\n",
      "avgfold accuracy: 0.503180584949457\n",
      "standard deviation: 0.0035753842833460086\n",
      "avg train time: 1.6196975708007812\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999131605227736\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5011771601474947\n",
      "standard deviation: 0.0015862349348357027\n",
      "avg train time: 1.7088128089904786\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.4 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.5002313573878208\n",
      "standard deviation: 0.0007605132069358149\n",
      "avg train time: 1.6129308223724366\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.5036227587456081\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.5 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.5005776176709836\n",
      "standard deviation: 0.0015275281018882543\n",
      "avg train time: 1.6139698505401612\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.503589734737154\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.6 valid accuracy for this fold, 0.49961006735200286\n",
      "avgfold accuracy: 0.501354239177792\n",
      "standard deviation: 0.0014782841066886766\n",
      "avg train time: 1.6488028049468995\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017200818244025\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996277915632754\n",
      "avgfold accuracy: 0.5008993048504167\n",
      "standard deviation: 0.001535399866626943\n",
      "avg train time: 1.6168894290924072\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5019405160564212\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.8 valid accuracy for this fold, 0.5021197760610268\n",
      "avgfold accuracy: 0.501094259779113\n",
      "standard deviation: 0.0010450342318716304\n",
      "avg train time: 1.5744956016540528\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 10, lambda: 1.0, learning rate: 0.9 valid accuracy for this fold, 0.502155224483572\n",
      "avgfold accuracy: 0.5008907353593514\n",
      "standard deviation: 0.0010532356627893627\n",
      "avg train time: 1.5438799381256103\n",
      "tree depth: 10, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5051038980338707\n",
      "tree depth: 10, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 10, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5053728003063446\n",
      "tree depth: 10, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5040083395603281\n",
      "tree depth: 10, lambda: 1.0, learning rate: 1.0 valid accuracy for this fold, 0.5018007402581199\n",
      "avgfold accuracy: 0.503202269006917\n",
      "standard deviation: 0.002146107609435954\n",
      "avg train time: 1.5338914394378662\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5093772547642136\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5071596352206027\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5169811720504527\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5097971383499847\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.1 valid accuracy for this fold, 0.5207298018254948\n",
      "avgfold accuracy: 0.5128090004421497\n",
      "standard deviation: 0.005155766277678014\n",
      "avg train time: 1.6668324947357178\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.2 valid accuracy for this fold, 0.5021020518497541\n",
      "avgfold accuracy: 0.5006149687681667\n",
      "standard deviation: 0.001019263238053882\n",
      "avg train time: 1.7341790199279785\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999131605227736\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5008245686432482\n",
      "standard deviation: 0.0011078200407985714\n",
      "avg train time: 1.6553174018859864\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5054011141099579\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.5027949494997053\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.4 valid accuracy for this fold, 0.49982275788727404\n",
      "avgfold accuracy: 0.5015910335546389\n",
      "standard deviation: 0.0022073651278311443\n",
      "avg train time: 1.6070855617523194\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.49993487039208023\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.5 valid accuracy for this fold, 0.5024565360752061\n",
      "avgfold accuracy: 0.5011659162979694\n",
      "standard deviation: 0.0015350903348921147\n",
      "avg train time: 1.6242751598358154\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.503589734737154\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.6 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5005681034837759\n",
      "standard deviation: 0.0015142841943030008\n",
      "avg train time: 1.6657344341278075\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5004322129057505\n",
      "standard deviation: 0.0009928360471279326\n",
      "avg train time: 1.5828795909881592\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5072620294954432\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.5018768623326784\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.8 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5019656854610458\n",
      "standard deviation: 0.0028333547138331343\n",
      "avg train time: 1.5377545833587647\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5025344310680262\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 10, lambda: 1.1, learning rate: 0.9 valid accuracy for this fold, 0.4992555831265509\n",
      "avgfold accuracy: 0.5009785475284844\n",
      "standard deviation: 0.0012422802712185382\n",
      "avg train time: 1.6289572715759277\n",
      "tree depth: 10, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5070969094531724\n",
      "tree depth: 10, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.4997255668759219\n",
      "tree depth: 10, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5024041718521867\n",
      "tree depth: 10, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.5016646832535355\n",
      "tree depth: 10, lambda: 1.1, learning rate: 1.0 valid accuracy for this fold, 0.49932647997164126\n",
      "avgfold accuracy: 0.5020435622812915\n",
      "standard deviation: 0.0027772484655349205\n",
      "avg train time: 1.530052661895752\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5248590100002702\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5055081616772197\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5255179896347144\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5098395741658134\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5221831871498478\n",
      "avgfold accuracy: 0.5175815845255731\n",
      "standard deviation: 0.0082804403022264\n",
      "avg train time: 1.5395193576812745\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5039446858365852\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.49948599787309467\n",
      "avgfold accuracy: 0.500906655579712\n",
      "standard deviation: 0.001699400408865616\n",
      "avg train time: 1.63310809135437\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49980503367600143\n",
      "avgfold accuracy: 0.5006280271120138\n",
      "standard deviation: 0.0014901282985601177\n",
      "avg train time: 1.7233957767486572\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5036888067625164\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49997878209208574\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49980503367600143\n",
      "avgfold accuracy: 0.5006607002750405\n",
      "standard deviation: 0.0015156861104765322\n",
      "avg train time: 1.6320836067199707\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5016627713347604\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4997695852534562\n",
      "avgfold accuracy: 0.5002323407384591\n",
      "standard deviation: 0.0007179676317440486\n",
      "avg train time: 1.625935697555542\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49978534394504803\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.5016857776838928\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49961006735200286\n",
      "avgfold accuracy: 0.5001732116266894\n",
      "standard deviation: 0.0007634304697196475\n",
      "avg train time: 1.6052472114562988\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998018559492751\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5027081100224788\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5041144290998995\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995214462956398\n",
      "avgfold accuracy: 0.5012051553751018\n",
      "standard deviation: 0.0018591976503990974\n",
      "avg train time: 1.6193679809570312\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5072950535038974\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.502155224483572\n",
      "avgfold accuracy: 0.5020494714054523\n",
      "standard deviation: 0.0028121873834257554\n",
      "avg train time: 1.5660275936126709\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5022739126363471\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.4996392955654572\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5021729486948445\n",
      "avgfold accuracy: 0.5014532771085785\n",
      "standard deviation: 0.0009507411725660663\n",
      "avg train time: 1.5594907760620118\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5071134214573996\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.499708414805667\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5026864001531723\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5017495548851927\n",
      "tree depth: 10, lambda: 1.2000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.4992378589152783\n",
      "avgfold accuracy: 0.502099130043342\n",
      "standard deviation: 0.0028111420872709053\n",
      "avg train time: 1.5239773750305177\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5167053322765651\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5022223666607085\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5255614093733276\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.5096273950866705\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.1 valid accuracy for this fold, 0.520588008135314\n",
      "avgfold accuracy: 0.5149409023065171\n",
      "standard deviation: 0.008219400168699334\n",
      "avg train time: 1.625288486480713\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.49966051347337154\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.2 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5006253392585887\n",
      "standard deviation: 0.0010677629733941295\n",
      "avg train time: 1.6375609397888184\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036557827540622\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999131605227736\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998936547323644\n",
      "avgfold accuracy: 0.5006806020246234\n",
      "standard deviation: 0.0014879245523899411\n",
      "avg train time: 1.6790262699127196\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.49993487039208023\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.4 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5002281077224017\n",
      "standard deviation: 0.0007634875623770785\n",
      "avg train time: 1.7067179679870605\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5054341381184121\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.5016686256136379\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.5 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5013107276744591\n",
      "standard deviation: 0.0021832122138397345\n",
      "avg train time: 1.61849102973938\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.5037053187667434\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49969606182970777\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.6 valid accuracy for this fold, 0.49955689471818504\n",
      "avgfold accuracy: 0.5005516587496943\n",
      "standard deviation: 0.0015824777096417597\n",
      "avg train time: 1.6287795543670653\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5015636993093979\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5026646902838657\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5011189486660065\n",
      "standard deviation: 0.0012670567040867203\n",
      "avg train time: 1.5543078422546386\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5035071747160186\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.4997177716990144\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.4996817313812858\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.8 valid accuracy for this fold, 0.5023501908075705\n",
      "avgfold accuracy: 0.501030791236472\n",
      "standard deviation: 0.001593898186759241\n",
      "avg train time: 1.621821403503418\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5052029700592331\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.5033715553677856\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.4994572532673353\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 10, lambda: 1.3, learning rate: 0.9 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5014200811604892\n",
      "standard deviation: 0.0024123625399684163\n",
      "avg train time: 1.6149221420288087\n",
      "tree depth: 10, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5013325312502189\n",
      "tree depth: 10, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5015142569813439\n",
      "tree depth: 10, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5024258817214933\n",
      "tree depth: 10, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.5037961604811853\n",
      "tree depth: 10, lambda: 1.3, learning rate: 1.0 valid accuracy for this fold, 0.49941510102800424\n",
      "avgfold accuracy: 0.5016967862924491\n",
      "standard deviation: 0.0014360388894575345\n",
      "avg train time: 1.5505653381347657\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5209060111701207\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5021366063094341\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5148158087606385\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5034663603907785\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5296414164318292\n",
      "avgfold accuracy: 0.5141932406125602\n",
      "standard deviation: 0.01043574056607643\n",
      "avg train time: 1.5976125717163085\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5034741507075644\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5025806055541143\n",
      "avgfold accuracy: 0.5011000728474484\n",
      "standard deviation: 0.0015995858092016561\n",
      "avg train time: 1.607024335861206\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036227587456081\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.499734136830911\n",
      "avgfold accuracy: 0.5006324981351089\n",
      "standard deviation: 0.0014979779771600314\n",
      "avg train time: 1.7466209411621094\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5036888067625164\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.5006224729311071\n",
      "standard deviation: 0.0015355063484608484\n",
      "avg train time: 1.6123728275299072\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5036392707498352\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5026160539766595\n",
      "avgfold accuracy: 0.501205519921559\n",
      "standard deviation: 0.0016026104655575588\n",
      "avg train time: 1.6156337261199951\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.4996277915632754\n",
      "avgfold accuracy: 0.5001735384710366\n",
      "standard deviation: 0.0007907119445395674\n",
      "avg train time: 1.597991132736206\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49978534394504803\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5016343214731281\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.499645515774548\n",
      "avgfold accuracy: 0.5001657664874626\n",
      "standard deviation: 0.0007394014846399289\n",
      "avg train time: 1.6110958099365233\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5035401987244728\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5016000173326184\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.5025127211987196\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.506309560051292\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.49955689471818504\n",
      "avgfold accuracy: 0.5027038784050576\n",
      "standard deviation: 0.0022297501420958364\n",
      "avg train time: 1.569444513320923\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5033255426695208\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4995440927445617\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5020890414118212\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5022083971173897\n",
      "avgfold accuracy: 0.5013922498200469\n",
      "standard deviation: 0.001473686763167794\n",
      "avg train time: 1.5855295181274414\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5011839232121753\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014284966300695\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49978290130693415\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5040295574682423\n",
      "tree depth: 10, lambda: 1.4000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5021375002722993\n",
      "avgfold accuracy: 0.5017124757779442\n",
      "standard deviation: 0.0013880045382273604\n",
      "avg train time: 1.568678379058838\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5291373078227871\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5043026419604636\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5297892102472676\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5157247760588197\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.1 valid accuracy for this fold, 0.5220768418822123\n",
      "avgfold accuracy: 0.52020615559431\n",
      "standard deviation: 0.009466830433049894\n",
      "avg train time: 1.5613435745239257\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.5053846021057309\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.2 valid accuracy for this fold, 0.4989897199574619\n",
      "avgfold accuracy: 0.5007891262911088\n",
      "standard deviation: 0.0023224090808642997\n",
      "avg train time: 1.6463678359985352\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5016957953432145\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025097087090239\n",
      "avgfold accuracy: 0.5007819134821487\n",
      "standard deviation: 0.001109660995116063\n",
      "avg train time: 1.6285162448883057\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.5055001861353204\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.4 valid accuracy for this fold, 0.49980503367600143\n",
      "avgfold accuracy: 0.5010042961365612\n",
      "standard deviation: 0.0022483720107892386\n",
      "avg train time: 1.6332317352294923\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5073115655081244\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.5 valid accuracy for this fold, 0.5025451571315691\n",
      "avgfold accuracy: 0.5019342866673645\n",
      "standard deviation: 0.0028720811610888014\n",
      "avg train time: 1.6138738632202148\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.5015802113136251\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.6 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.5002076337100643\n",
      "standard deviation: 0.0006910939024272659\n",
      "avg train time: 1.6347358226776123\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998046111762407\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5001899244455069\n",
      "standard deviation: 0.0007633110379591606\n",
      "avg train time: 1.5611591815948487\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5034411266991102\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5034401636488051\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.5028383692383184\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.8 valid accuracy for this fold, 0.49941510102800424\n",
      "avgfold accuracy: 0.5017930034701847\n",
      "standard deviation: 0.0017904765390996287\n",
      "avg train time: 1.5075772285461426\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5016792833389875\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5034058595082953\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49958751248317484\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.5040295574682423\n",
      "tree depth: 10, lambda: 1.5, learning rate: 0.9 valid accuracy for this fold, 0.49939737681673163\n",
      "avgfold accuracy: 0.5016199179230864\n",
      "standard deviation: 0.0019010436700708103\n",
      "avg train time: 1.6577627658843994\n",
      "tree depth: 10, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5052855300803685\n",
      "tree depth: 10, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5033200991570209\n",
      "tree depth: 10, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.49941383352872215\n",
      "tree depth: 10, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.5059912914325778\n",
      "tree depth: 10, lambda: 1.5, learning rate: 1.0 valid accuracy for this fold, 0.4989365473236441\n",
      "avgfold accuracy: 0.5025894603044667\n",
      "standard deviation: 0.002925865993440792\n",
      "avg train time: 1.554422378540039\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5164246282047048\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5072110914313673\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5108702361876841\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5073261745957066\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.1 valid accuracy for this fold, 0.5357348814458204\n",
      "avgfold accuracy: 0.5155134023730568\n",
      "standard deviation: 0.010651353024683551\n",
      "avg train time: 1.59360933303833\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.507212493482762\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.2 valid accuracy for this fold, 0.49929103154909604\n",
      "avgfold accuracy: 0.5011916306804716\n",
      "standard deviation: 0.0030182038320330576\n",
      "avg train time: 1.6552110195159913\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5036888067625164\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49971641261963845\n",
      "avgfold accuracy: 0.5006336947737038\n",
      "standard deviation: 0.0015297588306225172\n",
      "avg train time: 1.782448148727417\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4998018559492751\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.49993487039208023\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.4 valid accuracy for this fold, 0.5025628813428417\n",
      "avgfold accuracy: 0.5004292255543041\n",
      "standard deviation: 0.0010679514142410869\n",
      "avg train time: 1.7154627799987794\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5036557827540622\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5017372338946574\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.5021314772276497\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.5 valid accuracy for this fold, 0.4998936547323644\n",
      "avgfold accuracy: 0.501431526035411\n",
      "standard deviation: 0.0014668857856377745\n",
      "avg train time: 1.6407577991485596\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.5036227587456081\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.6 valid accuracy for this fold, 0.499911378943637\n",
      "avgfold accuracy: 0.5006534117339183\n",
      "standard deviation: 0.0014850787380750639\n",
      "avg train time: 1.706716823577881\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998348799577293\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5055898989994104\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5080254040427045\n",
      "avgfold accuracy: 0.502646609872685\n",
      "standard deviation: 0.003483831031697155\n",
      "avg train time: 1.5787292003631592\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.5055001861353204\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.499630932221788\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.4997666030129429\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.8 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.5008699974487307\n",
      "standard deviation: 0.002319792853201921\n",
      "avg train time: 1.643950891494751\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.5013820672629001\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.4995440927445617\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 1.6, learning rate: 0.9 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5000061615026943\n",
      "standard deviation: 0.0007063643942301243\n",
      "avg train time: 1.6417786121368407\n",
      "tree depth: 10, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5070143494320372\n",
      "tree depth: 10, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5014628007705793\n",
      "tree depth: 10, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5022304928977339\n",
      "tree depth: 10, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 10, lambda: 1.6, learning rate: 1.0 valid accuracy for this fold, 0.5020843276384815\n",
      "avgfold accuracy: 0.5029167922879706\n",
      "standard deviation: 0.002065587022481936\n",
      "avg train time: 1.6282641410827636\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5206418191024875\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5088282608342406\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5257133784584738\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.5073261745957066\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.1 valid accuracy for this fold, 0.519230373159009\n",
      "avgfold accuracy: 0.5163480012299835\n",
      "standard deviation: 0.007104858646443413\n",
      "avg train time: 1.6639918327331542\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5090899208724743\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.502004169780164\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.2 valid accuracy for this fold, 0.5021729486948445\n",
      "avgfold accuracy: 0.5025990011541104\n",
      "standard deviation: 0.00339495328673846\n",
      "avg train time: 1.6696771144866944\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5054671621268663\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025274329202964\n",
      "avgfold accuracy: 0.501537829209868\n",
      "standard deviation: 0.0022130607938937235\n",
      "avg train time: 1.6597710132598877\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4999009279746376\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.5017543859649123\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49997878209208574\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.4 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5002448470186937\n",
      "standard deviation: 0.0007603889666186296\n",
      "avg train time: 1.678208827972412\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49993487039208023\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.5 valid accuracy for this fold, 0.49980503367600143\n",
      "avgfold accuracy: 0.5002757011636756\n",
      "standard deviation: 0.0007453642444590818\n",
      "avg train time: 1.6750423908233643\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.503606246741381\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.6 valid accuracy for this fold, 0.49953917050691243\n",
      "avgfold accuracy: 0.5009249375442801\n",
      "standard deviation: 0.0015285961990662313\n",
      "avg train time: 1.627453327178955\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49958719989432315\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4995223828752551\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.49973734919823415\n",
      "standard deviation: 0.00016194434585524748\n",
      "avg train time: 1.6619874477386474\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5055166981395475\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49981132722719634\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.8 valid accuracy for this fold, 0.5024033634413884\n",
      "avgfold accuracy: 0.5014341735794599\n",
      "standard deviation: 0.00228661156625357\n",
      "avg train time: 1.6670967102050782\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5091559688893826\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.49956580261386824\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.5041993007315566\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 0.9 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5024503114387147\n",
      "standard deviation: 0.003793431189899722\n",
      "avg train time: 1.605862522125244\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5032760066568396\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.4998284792974512\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5052208312211984\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.5017919907010212\n",
      "tree depth: 10, lambda: 1.7000000000000002, learning rate: 1.0 valid accuracy for this fold, 0.49936192839418647\n",
      "avgfold accuracy: 0.5018958472541393\n",
      "standard deviation: 0.0021755872585972327\n",
      "avg train time: 1.5549976348876953\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5139577097549919\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5038223839933269\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5224625217033302\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.512268102104263\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.1 valid accuracy for this fold, 0.5277875035398913\n",
      "avgfold accuracy: 0.5160596442191606\n",
      "standard deviation: 0.008331674288636235\n",
      "avg train time: 1.6667123317718506\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.5015471873051709\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.49987993550821586\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.2 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5001110935908166\n",
      "standard deviation: 0.0007349306329953283\n",
      "avg train time: 1.669488000869751\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5035071747160186\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998404820985466\n",
      "avgfold accuracy: 0.5006220648272739\n",
      "standard deviation: 0.0014439607597627792\n",
      "avg train time: 1.6048126220703125\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.5037053187667434\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.4 valid accuracy for this fold, 0.4997518610421836\n",
      "avgfold accuracy: 0.5006448809859566\n",
      "standard deviation: 0.001532256066604111\n",
      "avg train time: 1.6641685485839843\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.4998914506534671\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.5 valid accuracy for this fold, 0.5026160539766595\n",
      "avgfold accuracy: 0.5008120692058299\n",
      "standard deviation: 0.0011562160158607518\n",
      "avg train time: 1.629537296295166\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.5017453313558957\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.49986974078416047\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.6 valid accuracy for this fold, 0.49955689471818504\n",
      "avgfold accuracy: 0.5001917798118962\n",
      "standard deviation: 0.0007872935715141447\n",
      "avg train time: 1.6572347640991212\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5028166593690118\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999575641841714\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4996277915632754\n",
      "avgfold accuracy: 0.5008160036646782\n",
      "standard deviation: 0.0012391749870130607\n",
      "avg train time: 1.6299230575561523\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.4998513919619563\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.499862783437961\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.5027298198917854\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.8 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.5003860497785724\n",
      "standard deviation: 0.0011768684143177582\n",
      "avg train time: 1.5821776390075684\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5015967233178521\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5015657131921086\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.49932699405149583\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.5019192981485069\n",
      "tree depth: 10, lambda: 1.8, learning rate: 0.9 valid accuracy for this fold, 0.49943282523927685\n",
      "avgfold accuracy: 0.5007683107898481\n",
      "standard deviation: 0.001140859798881195\n",
      "avg train time: 1.5302356243133546\n",
      "tree depth: 10, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5052029700592331\n",
      "tree depth: 10, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.501651473543383\n",
      "tree depth: 10, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5054813496528775\n",
      "tree depth: 10, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5017283369772784\n",
      "tree depth: 10, lambda: 1.8, learning rate: 1.0 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5032722296813049\n",
      "standard deviation: 0.0017070127170225316\n",
      "avg train time: 1.6001144886016845\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5203941390390814\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5120282954994774\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5225927809191697\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5087150250463566\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.1 valid accuracy for this fold, 0.5261746003140849\n",
      "avgfold accuracy: 0.517980968163634\n",
      "standard deviation: 0.006565361103280624\n",
      "avg train time: 1.6288655281066895\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.5053680901015039\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.4997241671971144\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.2 valid accuracy for this fold, 0.49946827366182206\n",
      "avgfold accuracy: 0.500835773782205\n",
      "standard deviation: 0.0022717593705123018\n",
      "avg train time: 1.7003350257873535\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017618433601229\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999782901306934\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5025097087090239\n",
      "avgfold accuracy: 0.5008303768671174\n",
      "standard deviation: 0.0010918608896345443\n",
      "avg train time: 1.6866253852844237\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.5055001861353204\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.4 valid accuracy for this fold, 0.4997695852534562\n",
      "avgfold accuracy: 0.5009996269140142\n",
      "standard deviation: 0.0022514111837827295\n",
      "avg train time: 1.6204728126525878\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.4999151283683429\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.5 valid accuracy for this fold, 0.49968096419709324\n",
      "avgfold accuracy: 0.5002027154681378\n",
      "standard deviation: 0.0007621761360326354\n",
      "avg train time: 1.6803070068359376\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5016627713347604\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.5016171694028733\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.6 valid accuracy for this fold, 0.49969868840836584\n",
      "avgfold accuracy: 0.5005096734902013\n",
      "standard deviation: 0.000923978501931633\n",
      "avg train time: 1.6666772365570068\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5052525060719143\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4999142396487256\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5022970181737527\n",
      "avgfold accuracy: 0.5014326538907693\n",
      "standard deviation: 0.002128620957063611\n",
      "avg train time: 1.6536340713500977\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4997358079323668\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4998456313677061\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4996526420910946\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.8 valid accuracy for this fold, 0.4992378589152783\n",
      "avgfold accuracy: 0.49965619582704346\n",
      "standard deviation: 0.00021936421556025134\n",
      "avg train time: 1.6535279273986816\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.503391590686429\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5015485611218538\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.49960922235248145\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.5040083395603281\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 0.9 valid accuracy for this fold, 0.4995037220843672\n",
      "avgfold accuracy: 0.501612287161092\n",
      "standard deviation: 0.0018638328494538436\n",
      "avg train time: 1.5872475624084472\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.5014811392882625\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.4999313917189805\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49939212365941554\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49961807765754296\n",
      "tree depth: 10, lambda: 1.9000000000000001, learning rate: 1.0 valid accuracy for this fold, 0.49934420418291386\n",
      "avgfold accuracy: 0.49995338730142314\n",
      "standard deviation: 0.0007915887118869386\n",
      "avg train time: 1.5507563591003417\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5134458376239526\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5074512204149356\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5223322624874907\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.513826695818227\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.1 valid accuracy for this fold, 0.5171743646513876\n",
      "avgfold accuracy: 0.5148460761991986\n",
      "standard deviation: 0.004881549113926289\n",
      "avg train time: 1.6719451427459717\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.503358566677975\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.49996569585949024\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.4998263210455473\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.49970294928920006\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.2 valid accuracy for this fold, 0.49955689471818504\n",
      "avgfold accuracy: 0.5004820855180795\n",
      "standard deviation: 0.0014445664031486208\n",
      "avg train time: 1.6419734001159667\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5017123073474417\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5028817889769316\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.4998939104604286\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.30000000000000004 valid accuracy for this fold, 0.5026160539766595\n",
      "avgfold accuracy: 0.5014173817382414\n",
      "standard deviation: 0.0012686320816441756\n",
      "avg train time: 1.624265718460083\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.5035567107286998\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.49994854378923537\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.4998090388287715\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.4 valid accuracy for this fold, 0.4998759305210918\n",
      "avgfold accuracy: 0.5006076509565304\n",
      "standard deviation: 0.0014752352037398033\n",
      "avg train time: 1.678354024887085\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.5016957953432145\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4999828479297451\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4998480309148539\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.49993634627625716\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.5 valid accuracy for this fold, 0.4997873094647288\n",
      "avgfold accuracy: 0.5002500659857599\n",
      "standard deviation: 0.0007260488573907534\n",
      "avg train time: 1.7257113933563233\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5054506501226392\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.5017543859649123\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.49973948156832093\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4998726925525143\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.6 valid accuracy for this fold, 0.4996277915632754\n",
      "avgfold accuracy: 0.5012890003543324\n",
      "standard deviation: 0.0022227219108904004\n",
      "avg train time: 1.7809314250946044\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.507229005486989\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.5017372338946574\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49976119143762754\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.4997878209208572\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.7000000000000001 valid accuracy for this fold, 0.49959234314073026\n",
      "avgfold accuracy: 0.5016215189761722\n",
      "standard deviation: 0.0029119792784711804\n",
      "avg train time: 1.7106163024902343\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.5035732227329268\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.49989708757847073\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.49985147464460006\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.8 valid accuracy for this fold, 0.49941510102800424\n",
      "avgfold accuracy: 0.5004822475888806\n",
      "standard deviation: 0.0015547289934285714\n",
      "avg train time: 1.6326878070831299\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.5052359940676873\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4997770230866866\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.4996743519604012\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.49983025673668574\n",
      "tree depth: 10, lambda: 2.0, learning rate: 0.9 valid accuracy for this fold, 0.49945054945054945\n",
      "avgfold accuracy: 0.5007936350604021\n",
      "standard deviation: 0.002224986510640313\n",
      "avg train time: 1.5820134162902832\n",
      "tree depth: 10, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5015141632967167\n",
      "tree depth: 10, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.49979417515694147\n",
      "tree depth: 10, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.5021002336818945\n",
      "tree depth: 10, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.4995968597496287\n",
      "tree depth: 10, lambda: 2.0, learning rate: 1.0 valid accuracy for this fold, 0.49929103154909604\n",
      "avgfold accuracy: 0.5004592926868555\n",
      "standard deviation: 0.001127514553215182\n",
      "avg train time: 1.5609673976898193\n"
     ]
    }
   ],
   "source": [
    "## parameters for leaderboard submissions\n",
    "# max_depths = range(3, 15)\n",
    "# eta_val = np.arange(0.1, 1.1, 0.1)\n",
    "# lambda_val = np.arange(0, 2.1, 0.1)\n",
    "\n",
    "# parameters for revised submissions\n",
    "max_depths = range(1, 11)\n",
    "eta_val = np.arange(0.1, 1.1, 0.1)\n",
    "lambda_val = np.arange(0, 2.1, 0.1)\n",
    "\n",
    "accuracies = {}\n",
    "stdevs = {}\n",
    "times = {}\n",
    "for depth in max_depths:\n",
    "    for lam in lambda_val:\n",
    "        for etav in eta_val:\n",
    "            fold_accuracy = []\n",
    "            avg_train_time = 0\n",
    "            for fold in [1,2,3,4,5]:      \n",
    "                # subset of data\n",
    "                random.seed(fold) # set seed for reproducibility across models\n",
    "                X_train_fold, X_val_fold, y_train_fold, y_val_fold = splitter(X_cv)\n",
    "                ros = RandomOverSampler(random_state=0, sampling_strategy = 'minority')\n",
    "                X_train_fold, y_train_fold = ros.fit_resample(\n",
    "                    X_train_fold, y_train_fold)  # oversample minority class\n",
    "                \n",
    "                X_train_fold = X_train_fold.select_dtypes('number') # select numeric columns\n",
    "                X_val_fold = X_val_fold.select_dtypes('number')     # select numeric columns\n",
    "                \n",
    "                XGB = XGBClassifier(max_depth=depth, reg_lambda=lam, learning_rate=etav) # fit XGBoost\n",
    "\n",
    "                startt = time.time()\n",
    "                XGB.fit(X_train_fold.drop(columns = [\"ID\",\"pre_nucleus_id\",\"post_nucleus_id\"])\n",
    "                        ,y_train_fold) # fit model\n",
    "                endt = time.time(); elapsed = endt-startt # training time\n",
    "                \n",
    "                y_hat_valid = XGB.predict(\n",
    "                    X_val_fold.drop(columns = [\"ID\",\"pre_nucleus_id\",\"post_nucleus_id\"])) # predict\n",
    "                \n",
    "                valid_acc = balanced_accuracy_score(y_val_fold, y_hat_valid) # validation accuracy\n",
    "                fold_accuracy.append(valid_acc)\n",
    "                avg_train_time += elapsed\n",
    "                print(f\"tree depth: {depth}, lambda: {lam}, learning rate: {etav} valid accuracy for this fold, {valid_acc}\")\n",
    "            avg_fold_accuracy = sum(fold_accuracy)/len(fold_accuracy) # calculate average accuracy\n",
    "            fold_std = np.std(fold_accuracy) # calculate standard deviation\n",
    "            print(f\"avgfold accuracy: {avg_fold_accuracy}\")\n",
    "            print(f\"standard deviation: {fold_std}\")\n",
    "            print(f\"avg train time: {avg_train_time/5}\")\n",
    "            accuracies[(depth, lam, etav)] = avg_fold_accuracy # store values\n",
    "            stdevs[(depth, lam, etav)] = fold_std\n",
    "            times[(depth, lam, etav)] = avg_train_time/5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame({'Param Pair':accuracies.keys(), 'Avg Accuracy':accuracies.values()})\n",
    "std_df = pd.DataFrame({'Param Pair':stdevs.keys(), 'Standard Dev':stdevs.values()})\n",
    "time_df = pd.DataFrame({'Param Pair':times.keys(), 'Training Time':times.values()})\n",
    "xg_hyperparams = acc_df.merge(std_df, on='Param Pair').merge(time_df, on='Param Pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the 5 best parameters\n",
    "xg_hyperparams = xg_hyperparams.sort_values(by='Avg Accuracy', ascending=False).reset_index()\n",
    "xg_hyperparams.head(5)\n",
    "xg_hyperparams.to_csv('final_xg_hyperparams.csv')\n",
    "# hyperparameters for submitted models were saved to xgb_accuracy_est_leaf_eta_cv.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0.6000000000000001, 0.1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best parameters\n",
    "xg_best_param = xg_hyperparams['Param Pair'][0]\n",
    "xg_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0045757650884054755"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Standard Deviation\n",
    "xg_hyperparams[\"Standard Dev\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param0(row):\n",
    "    \"\"\" \n",
    "    Helper function to create a column in the hyperparams dataframe \n",
    "    for the first hyperparameter\n",
    "\n",
    "    Inputs: \n",
    "        - row: a row of the hyperparameter dataframe\n",
    "    \"\"\"\n",
    "    param_pair = row[\"Param Pair\"]\n",
    "    return param_pair[0]\n",
    "\n",
    "def get_param1(row):\n",
    "    \"\"\" \n",
    "    Helper function to create a column in the hyperparams dataframe \n",
    "    for the second hyperparameter\n",
    "\n",
    "    Inputs: \n",
    "        - row: a row of the hyperparameter dataframe\n",
    "    \"\"\"\n",
    "    param_pair = row[\"Param Pair\"]\n",
    "    return param_pair[1]\n",
    "\n",
    "def get_param2(row):\n",
    "    \"\"\" \n",
    "    Helper function to create a column in the hyperparams dataframe \n",
    "    for the third hyperparameter\n",
    "\n",
    "    Inputs: \n",
    "        - row: a row of the hyperparameter dataframe\n",
    "    \"\"\"\n",
    "    param_pair = row[\"Param Pair\"]\n",
    "    return param_pair[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_idx = 0; xg_hyperparams[\"Tree Depth\"] = xg_hyperparams.apply(get_param0, axis=1)\n",
    "hyp_idx = 1; xg_hyperparams[\"Lambda\"] = xg_hyperparams.apply(get_param1, axis=1)\n",
    "hyp_idx = 2; xg_hyperparams[\"Learning Rate\"] = xg_hyperparams.apply(get_param2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'XGBoost: Tuning Learning Rate')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create plots to visualize hyperparameter tuning\n",
    "plt.figure()\n",
    "sns.scatterplot(xg_hyperparams, x='Tree Depth', y='Avg Accuracy')\n",
    "plt.title('XGBoost: Tuning Tree Depth')\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(xg_hyperparams, x='Lambda', y='Avg Accuracy', hue=\"Tree Depth\")\n",
    "plt.title('XGBoost: Tuning Lambda')\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(xg_hyperparams, x='Learning Rate', y='Avg Accuracy', hue='Tree Depth')\n",
    "plt.title('XGBoost: Tuning Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on Query Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X_train=X_train.select_dtypes('number')\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy = 'minority')\n",
    "X_train_use, y_train_use = ros.fit_resample(\n",
    "                    X_train, y_train) \n",
    "X_query = X_query.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Balanced Accuracy on Query Set: 0.7515812554256209\n"
     ]
    }
   ],
   "source": [
    "xg_model = XGBClassifier(max_depth=xg_best_param[0], reg_lambda=xg_best_param[1], learning_rate=xg_best_param[2]) # fit model\n",
    "xg_model.fit(X_train_use.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1), y_train_use)\n",
    "\n",
    "# make predictions on query set\n",
    "xg_preds = xg_model.predict(X_query.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "xg_accuracy = balanced_accuracy_score(y_query, xg_preds)\n",
    "print(f\"XGBoost Balanced Accuracy on Query Set: {xg_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adp_dist</td>\n",
       "      <td>0.205895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>post_test_score</td>\n",
       "      <td>0.051042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RL_pre</td>\n",
       "      <td>0.047841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>post_skeletal_distance_to_soma</td>\n",
       "      <td>0.045103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pre_skeletal_distance_to_soma</td>\n",
       "      <td>0.044928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pre_nucleus_y</td>\n",
       "      <td>0.037207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pre_nucleus_z</td>\n",
       "      <td>0.033861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RL_post</td>\n",
       "      <td>0.032576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pre_rf_x</td>\n",
       "      <td>0.031916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>area1</td>\n",
       "      <td>0.031404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>area2</td>\n",
       "      <td>0.028719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>V1_pre</td>\n",
       "      <td>0.027423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pre_rf_y</td>\n",
       "      <td>0.026560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>axonal_coor_x</td>\n",
       "      <td>0.025944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>minicol_dist</td>\n",
       "      <td>0.024952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>post_oracle</td>\n",
       "      <td>0.024424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>post_nucleus_y</td>\n",
       "      <td>0.024253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>area3</td>\n",
       "      <td>0.023268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>V1_post</td>\n",
       "      <td>0.022468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>post_rf_x</td>\n",
       "      <td>0.022177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>post_nucleus_x</td>\n",
       "      <td>0.020883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_distance</td>\n",
       "      <td>0.019208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>post_rf_y</td>\n",
       "      <td>0.018122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>post_nucleus_z</td>\n",
       "      <td>0.017164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AL_post</td>\n",
       "      <td>0.017145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pre_test_score</td>\n",
       "      <td>0.016101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>nuclei_adp_dist</td>\n",
       "      <td>0.015626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fw_similarity</td>\n",
       "      <td>0.015023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AL_pre</td>\n",
       "      <td>0.014232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pre_nucleus_x</td>\n",
       "      <td>0.012528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pre_oracle</td>\n",
       "      <td>0.011671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>me_similarity</td>\n",
       "      <td>0.010333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dendritic_coor_z</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dendritic_coor_y</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>axonal_coor_y</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dendritic_coor_x</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>axonal_coor_z</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Features  Importances\n",
       "6                         adp_dist     0.205895\n",
       "14                 post_test_score     0.051042\n",
       "26                          RL_pre     0.047841\n",
       "7   post_skeletal_distance_to_soma     0.045103\n",
       "8    pre_skeletal_distance_to_soma     0.044928\n",
       "18                   pre_nucleus_y     0.037207\n",
       "19                   pre_nucleus_z     0.033861\n",
       "29                         RL_post     0.032576\n",
       "11                        pre_rf_x     0.031916\n",
       "32                           area1     0.031404\n",
       "33                           area2     0.028719\n",
       "28                          V1_pre     0.027423\n",
       "12                        pre_rf_y     0.026560\n",
       "0                    axonal_coor_x     0.025944\n",
       "35                    minicol_dist     0.024952\n",
       "13                     post_oracle     0.024424\n",
       "21                  post_nucleus_y     0.024253\n",
       "34                           area3     0.023268\n",
       "30                         V1_post     0.022468\n",
       "15                       post_rf_x     0.022177\n",
       "20                  post_nucleus_x     0.020883\n",
       "25                     rf_distance     0.019208\n",
       "16                       post_rf_y     0.018122\n",
       "22                  post_nucleus_z     0.017164\n",
       "31                         AL_post     0.017145\n",
       "10                  pre_test_score     0.016101\n",
       "36                 nuclei_adp_dist     0.015626\n",
       "24                   fw_similarity     0.015023\n",
       "27                          AL_pre     0.014232\n",
       "17                   pre_nucleus_x     0.012528\n",
       "9                       pre_oracle     0.011671\n",
       "23                   me_similarity     0.010333\n",
       "5                 dendritic_coor_z     0.000000\n",
       "4                 dendritic_coor_y     0.000000\n",
       "1                    axonal_coor_y     0.000000\n",
       "3                 dendritic_coor_x     0.000000\n",
       "2                    axonal_coor_z     0.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract feature importances\n",
    "xg_features = pd.DataFrame({\"Features\":X_train_use.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).columns, \n",
    "              \"Importances\":abs(xg_model.feature_importances_)}).sort_values(by='Importances', ascending=False)\n",
    "xg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'XGBoost Feature Importance')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create plot of feature importances\n",
    "sns.set_theme(palette='deep')\n",
    "plt.figure(dpi=250)\n",
    "g = sns.barplot(xg_features, y='Features', x='Importances', palette='mako')\n",
    "g.set_yticklabels(g.get_yticklabels(), fontsize=5)\n",
    "plt.title(\"XGBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_path = \"Data/leaderboard_data.csv\"\n",
    "sub_data = cleaner(leaderboard_path, feature_path, morph_path)\n",
    "sub_data = sub_data.select_dtypes('number')\n",
    "preds = xg_model.predict(sub_data.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "sub_data['connected_pred'] = preds==1\n",
    "submission_data = sub_data.filter(['ID','connected_pred'])\n",
    "submission_data.to_csv('final_submission_data_xg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Accuracy Score: 0.7550977356024662\n",
      "Private Accuracy Score: 0.7513936646868813\n"
     ]
    }
   ],
   "source": [
    "# evaluate against real leaderbaord data\n",
    "solutions = pd.read_csv('Data/solution_data.csv')\n",
    "leaderboard = pd.concat([solutions,submission_data], axis=1)\n",
    "public = leaderboard[leaderboard[\"Usage\"]==\"Public\"]\n",
    "print(\"Public Accuracy Score:\", balanced_accuracy_score(public[\"connected\"], public[\"connected_pred\"]))\n",
    "private = leaderboard[leaderboard[\"Usage\"]==\"Private\"]\n",
    "print(\"Private Accuracy Score:\", balanced_accuracy_score(private[\"connected\"], private[\"connected_pred\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Tune maximum tree depth and number of features.\n",
    "For our submission to the leaderboard and for this report, we tested tree depths in [1, 10] and number of features in [1, 20]. For this report, we decided to test number of features in [1, 20] with a step size of 2. We also decided to test a varying number of estimators. We tested $10^2$ and $10^3$ trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num estimators: 100, depth: 1, num features 1, valid accuracy for this fold, 0.7330495520243216\n",
      "num estimators: 100, depth: 1, num features 1, valid accuracy for this fold, 0.6160765451758449\n",
      "num estimators: 100, depth: 1, num features 1, valid accuracy for this fold, 0.7288354930729823\n",
      "num estimators: 100, depth: 1, num features 1, valid accuracy for this fold, 0.6255316008450262\n",
      "num estimators: 100, depth: 1, num features 1, valid accuracy for this fold, 0.6125559698906248\n",
      "avgfold accuracy: 0.6632098322017599\n",
      "standard deviation: 0.055482111460817524\n",
      "avg train time: 1.9123390674591065\n",
      "num estimators: 1000, depth: 1, num features 1, valid accuracy for this fold, 0.7017855731116522\n",
      "num estimators: 1000, depth: 1, num features 1, valid accuracy for this fold, 0.6846692990580805\n",
      "num estimators: 1000, depth: 1, num features 1, valid accuracy for this fold, 0.6853923444200143\n",
      "num estimators: 1000, depth: 1, num features 1, valid accuracy for this fold, 0.728269172224836\n",
      "num estimators: 1000, depth: 1, num features 1, valid accuracy for this fold, 0.5944736107287918\n",
      "avgfold accuracy: 0.678917999908675\n",
      "standard deviation: 0.04508727881763098\n",
      "avg train time: 18.85610270500183\n",
      "num estimators: 100, depth: 1, num features 3, valid accuracy for this fold, 0.7023851965378831\n",
      "num estimators: 100, depth: 1, num features 3, valid accuracy for this fold, 0.7042640046653631\n",
      "num estimators: 100, depth: 1, num features 3, valid accuracy for this fold, 0.7311472018070981\n",
      "num estimators: 100, depth: 1, num features 3, valid accuracy for this fold, 0.7270620577680607\n",
      "num estimators: 100, depth: 1, num features 3, valid accuracy for this fold, 0.5818293168356243\n",
      "avgfold accuracy: 0.6893375555228058\n",
      "standard deviation: 0.05499499213156328\n",
      "avg train time: 2.8375168323516844\n",
      "num estimators: 1000, depth: 1, num features 3, valid accuracy for this fold, 0.7528114189015415\n",
      "num estimators: 1000, depth: 1, num features 3, valid accuracy for this fold, 0.726551043658338\n",
      "num estimators: 1000, depth: 1, num features 3, valid accuracy for this fold, 0.7250250186626406\n",
      "num estimators: 1000, depth: 1, num features 3, valid accuracy for this fold, 0.7466697109751934\n",
      "num estimators: 1000, depth: 1, num features 3, valid accuracy for this fold, 0.6222295275458007\n",
      "avgfold accuracy: 0.7146573439487028\n",
      "standard deviation: 0.04748131231318516\n",
      "avg train time: 27.554013347625734\n",
      "num estimators: 100, depth: 1, num features 5, valid accuracy for this fold, 0.7455824134145523\n",
      "num estimators: 100, depth: 1, num features 5, valid accuracy for this fold, 0.729113743502525\n",
      "num estimators: 100, depth: 1, num features 5, valid accuracy for this fold, 0.7169223984645106\n",
      "num estimators: 100, depth: 1, num features 5, valid accuracy for this fold, 0.7386728659858486\n",
      "num estimators: 100, depth: 1, num features 5, valid accuracy for this fold, 0.5824922419443955\n",
      "avgfold accuracy: 0.7025567326623664\n",
      "standard deviation: 0.0607991996666033\n",
      "avg train time: 3.766058588027954\n",
      "num estimators: 1000, depth: 1, num features 5, valid accuracy for this fold, 0.7511717893908871\n",
      "num estimators: 1000, depth: 1, num features 5, valid accuracy for this fold, 0.7295941820177696\n",
      "num estimators: 1000, depth: 1, num features 5, valid accuracy for this fold, 0.7176436630984011\n",
      "num estimators: 1000, depth: 1, num features 5, valid accuracy for this fold, 0.7450516148672959\n",
      "num estimators: 1000, depth: 1, num features 5, valid accuracy for this fold, 0.7144539457658938\n",
      "avgfold accuracy: 0.7315830390280496\n",
      "standard deviation: 0.014538151639146773\n",
      "avg train time: 39.902663707733154\n",
      "num estimators: 100, depth: 1, num features 7, valid accuracy for this fold, 0.7438075605965537\n",
      "num estimators: 100, depth: 1, num features 7, valid accuracy for this fold, 0.7448396642527385\n",
      "num estimators: 100, depth: 1, num features 7, valid accuracy for this fold, 0.740327337747499\n",
      "num estimators: 100, depth: 1, num features 7, valid accuracy for this fold, 0.733950958957186\n",
      "num estimators: 100, depth: 1, num features 7, valid accuracy for this fold, 0.7342019847155907\n",
      "avgfold accuracy: 0.7394255012539135\n",
      "standard deviation: 0.004617026153406955\n",
      "avg train time: 4.816853094100952\n",
      "num estimators: 1000, depth: 1, num features 7, valid accuracy for this fold, 0.7441427417732691\n",
      "num estimators: 1000, depth: 1, num features 7, valid accuracy for this fold, 0.7392379786556027\n",
      "num estimators: 1000, depth: 1, num features 7, valid accuracy for this fold, 0.73562832808373\n",
      "num estimators: 1000, depth: 1, num features 7, valid accuracy for this fold, 0.7368347493980572\n",
      "num estimators: 1000, depth: 1, num features 7, valid accuracy for this fold, 0.7313449210728\n",
      "avgfold accuracy: 0.7374377437966919\n",
      "standard deviation: 0.004218007103479403\n",
      "avg train time: 47.93491082191467\n",
      "num estimators: 100, depth: 1, num features 9, valid accuracy for this fold, 0.744791651030299\n",
      "num estimators: 100, depth: 1, num features 9, valid accuracy for this fold, 0.7393089340620255\n",
      "num estimators: 100, depth: 1, num features 9, valid accuracy for this fold, 0.7343885114510406\n",
      "num estimators: 100, depth: 1, num features 9, valid accuracy for this fold, 0.7165472006199319\n",
      "num estimators: 100, depth: 1, num features 9, valid accuracy for this fold, 0.7409155990882428\n",
      "avgfold accuracy: 0.7351903792503081\n",
      "standard deviation: 0.009900870235839547\n",
      "avg train time: 5.828245162963867\n",
      "num estimators: 1000, depth: 1, num features 9, valid accuracy for this fold, 0.7431074265991375\n",
      "num estimators: 1000, depth: 1, num features 9, valid accuracy for this fold, 0.7388140516981452\n",
      "num estimators: 1000, depth: 1, num features 9, valid accuracy for this fold, 0.7327513780535716\n",
      "num estimators: 1000, depth: 1, num features 9, valid accuracy for this fold, 0.7362194300685431\n",
      "num estimators: 1000, depth: 1, num features 9, valid accuracy for this fold, 0.740412191880925\n",
      "avgfold accuracy: 0.7382608956600645\n",
      "standard deviation: 0.0035481061199847757\n",
      "avg train time: 57.57833294868469\n",
      "num estimators: 100, depth: 1, num features 11, valid accuracy for this fold, 0.7416725709590872\n",
      "num estimators: 100, depth: 1, num features 11, valid accuracy for this fold, 0.7381794250987146\n",
      "num estimators: 100, depth: 1, num features 11, valid accuracy for this fold, 0.740202702172504\n",
      "num estimators: 100, depth: 1, num features 11, valid accuracy for this fold, 0.7358665670347513\n",
      "num estimators: 100, depth: 1, num features 11, valid accuracy for this fold, 0.7452614964779318\n",
      "avgfold accuracy: 0.7402365523485978\n",
      "standard deviation: 0.00318251167955338\n",
      "avg train time: 6.856682825088501\n",
      "num estimators: 1000, depth: 1, num features 11, valid accuracy for this fold, 0.7381885380671749\n",
      "num estimators: 1000, depth: 1, num features 11, valid accuracy for this fold, 0.7373512509275659\n",
      "num estimators: 1000, depth: 1, num features 11, valid accuracy for this fold, 0.7367074132141436\n",
      "num estimators: 1000, depth: 1, num features 11, valid accuracy for this fold, 0.7353707137519718\n",
      "num estimators: 1000, depth: 1, num features 11, valid accuracy for this fold, 0.7381577514214026\n",
      "avgfold accuracy: 0.7371551334764518\n",
      "standard deviation: 0.0010484500019257912\n",
      "avg train time: 67.65082874298096\n",
      "num estimators: 100, depth: 1, num features 13, valid accuracy for this fold, 0.7408205765591585\n",
      "num estimators: 100, depth: 1, num features 13, valid accuracy for this fold, 0.7361040245978743\n",
      "num estimators: 100, depth: 1, num features 13, valid accuracy for this fold, 0.7395176642482395\n",
      "num estimators: 100, depth: 1, num features 13, valid accuracy for this fold, 0.7363467375160287\n",
      "num estimators: 100, depth: 1, num features 13, valid accuracy for this fold, 0.7439959481858923\n",
      "avgfold accuracy: 0.7393569902214387\n",
      "standard deviation: 0.002943849140337724\n",
      "avg train time: 7.858706331253051\n",
      "num estimators: 1000, depth: 1, num features 13, valid accuracy for this fold, 0.7394054852878043\n",
      "num estimators: 1000, depth: 1, num features 13, valid accuracy for this fold, 0.7365156742839913\n",
      "num estimators: 1000, depth: 1, num features 13, valid accuracy for this fold, 0.7410703122506961\n",
      "num estimators: 1000, depth: 1, num features 13, valid accuracy for this fold, 0.7371511729813006\n",
      "num estimators: 1000, depth: 1, num features 13, valid accuracy for this fold, 0.7379982335199491\n",
      "avgfold accuracy: 0.7384281756647482\n",
      "standard deviation: 0.0016376365574159604\n",
      "avg train time: 79.33072390556336\n",
      "num estimators: 100, depth: 1, num features 15, valid accuracy for this fold, 0.7399057865067904\n",
      "num estimators: 100, depth: 1, num features 15, valid accuracy for this fold, 0.7356311691031634\n",
      "num estimators: 100, depth: 1, num features 15, valid accuracy for this fold, 0.7336953650213724\n",
      "num estimators: 100, depth: 1, num features 15, valid accuracy for this fold, 0.7395372651039216\n",
      "num estimators: 100, depth: 1, num features 15, valid accuracy for this fold, 0.7467466665610475\n",
      "avgfold accuracy: 0.739103250459259\n",
      "standard deviation: 0.00448452486618372\n",
      "avg train time: 8.800177621841431\n",
      "num estimators: 1000, depth: 1, num features 15, valid accuracy for this fold, 0.7407875525507044\n",
      "num estimators: 1000, depth: 1, num features 15, valid accuracy for this fold, 0.7398406482399268\n",
      "num estimators: 1000, depth: 1, num features 15, valid accuracy for this fold, 0.7400226148831355\n",
      "num estimators: 1000, depth: 1, num features 15, valid accuracy for this fold, 0.7417305510198433\n",
      "num estimators: 1000, depth: 1, num features 15, valid accuracy for this fold, 0.7404830887260154\n",
      "avgfold accuracy: 0.7405728910839251\n",
      "standard deviation: 0.0006683268684974919\n",
      "avg train time: 86.12907056808471\n",
      "num estimators: 100, depth: 1, num features 17, valid accuracy for this fold, 0.7399553225194717\n",
      "num estimators: 100, depth: 1, num features 17, valid accuracy for this fold, 0.7213602855548875\n",
      "num estimators: 100, depth: 1, num features 17, valid accuracy for this fold, 0.7141653758449193\n",
      "num estimators: 100, depth: 1, num features 17, valid accuracy for this fold, 0.738472679637266\n",
      "num estimators: 100, depth: 1, num features 17, valid accuracy for this fold, 0.7404830887260154\n",
      "avgfold accuracy: 0.7308873504565121\n",
      "standard deviation: 0.010974815985722435\n",
      "avg train time: 9.652313852310181\n",
      "num estimators: 1000, depth: 1, num features 17, valid accuracy for this fold, 0.7407875525507044\n",
      "num estimators: 1000, depth: 1, num features 17, valid accuracy for this fold, 0.7363784577219523\n",
      "num estimators: 1000, depth: 1, num features 17, valid accuracy for this fold, 0.7141653758449193\n",
      "num estimators: 1000, depth: 1, num features 17, valid accuracy for this fold, 0.7417942047435861\n",
      "num estimators: 1000, depth: 1, num features 17, valid accuracy for this fold, 0.7428118916585307\n",
      "avgfold accuracy: 0.7351874965039386\n",
      "standard deviation: 0.010737677296224624\n",
      "avg train time: 96.46350040435792\n",
      "num estimators: 100, depth: 1, num features 20, valid accuracy for this fold, 0.725794777803463\n",
      "num estimators: 100, depth: 1, num features 20, valid accuracy for this fold, 0.7213431334846326\n",
      "num estimators: 100, depth: 1, num features 20, valid accuracy for this fold, 0.7141436659756127\n",
      "num estimators: 100, depth: 1, num features 20, valid accuracy for this fold, 0.7168770007103387\n",
      "num estimators: 100, depth: 1, num features 20, valid accuracy for this fold, 0.7220531765945354\n",
      "avgfold accuracy: 0.7200423509137165\n",
      "standard deviation: 0.004089982793273219\n",
      "avg train time: 11.528039073944091\n",
      "num estimators: 1000, depth: 1, num features 20, valid accuracy for this fold, 0.725794777803463\n",
      "num estimators: 1000, depth: 1, num features 20, valid accuracy for this fold, 0.740107678891579\n",
      "num estimators: 1000, depth: 1, num features 20, valid accuracy for this fold, 0.7141653758449193\n",
      "num estimators: 1000, depth: 1, num features 20, valid accuracy for this fold, 0.7168770007103387\n",
      "num estimators: 1000, depth: 1, num features 20, valid accuracy for this fold, 0.7220531765945354\n",
      "avgfold accuracy: 0.7237996019689671\n",
      "standard deviation: 0.009096425916898255\n",
      "avg train time: 112.0312479019165\n",
      "num estimators: 100, depth: 2, num features 1, valid accuracy for this fold, 0.6960046829044715\n",
      "num estimators: 100, depth: 2, num features 1, valid accuracy for this fold, 0.7322142961602833\n",
      "num estimators: 100, depth: 2, num features 1, valid accuracy for this fold, 0.7025801525759152\n",
      "num estimators: 100, depth: 2, num features 1, valid accuracy for this fold, 0.6884168673142741\n",
      "num estimators: 100, depth: 2, num features 1, valid accuracy for this fold, 0.6609528694408853\n",
      "avgfold accuracy: 0.696033773679166\n",
      "standard deviation: 0.022980745134510178\n",
      "avg train time: 2.4954059600830076\n",
      "num estimators: 1000, depth: 2, num features 1, valid accuracy for this fold, 0.7271001643194603\n",
      "num estimators: 1000, depth: 2, num features 1, valid accuracy for this fold, 0.7097528476950326\n",
      "num estimators: 1000, depth: 2, num features 1, valid accuracy for this fold, 0.7010178266799123\n",
      "num estimators: 1000, depth: 2, num features 1, valid accuracy for this fold, 0.7312858052196054\n",
      "num estimators: 1000, depth: 2, num features 1, valid accuracy for this fold, 0.5962177127251915\n",
      "avgfold accuracy: 0.6930748713278404\n",
      "standard deviation: 0.04967967520033665\n",
      "avg train time: 22.973120355606078\n",
      "num estimators: 100, depth: 2, num features 3, valid accuracy for this fold, 0.7473558276867283\n",
      "num estimators: 100, depth: 2, num features 3, valid accuracy for this fold, 0.7265096981416184\n",
      "num estimators: 100, depth: 2, num features 3, valid accuracy for this fold, 0.7246133158398869\n",
      "num estimators: 100, depth: 2, num features 3, valid accuracy for this fold, 0.7583806123672727\n",
      "num estimators: 100, depth: 2, num features 3, valid accuracy for this fold, 0.6286528213181664\n",
      "avgfold accuracy: 0.7171024550707346\n",
      "standard deviation: 0.04601608368188592\n",
      "avg train time: 4.295293760299683\n",
      "num estimators: 1000, depth: 2, num features 3, valid accuracy for this fold, 0.7563778867236481\n",
      "num estimators: 1000, depth: 2, num features 3, valid accuracy for this fold, 0.7521466267294252\n",
      "num estimators: 1000, depth: 2, num features 3, valid accuracy for this fold, 0.7497349041862382\n",
      "num estimators: 1000, depth: 2, num features 3, valid accuracy for this fold, 0.7438514192935359\n",
      "num estimators: 1000, depth: 2, num features 3, valid accuracy for this fold, 0.6599711065650875\n",
      "avgfold accuracy: 0.732416388699587\n",
      "standard deviation: 0.03644844437603426\n",
      "avg train time: 43.25153455734253\n",
      "num estimators: 100, depth: 2, num features 5, valid accuracy for this fold, 0.7619922183427352\n",
      "num estimators: 100, depth: 2, num features 5, valid accuracy for this fold, 0.7534082969077525\n",
      "num estimators: 100, depth: 2, num features 5, valid accuracy for this fold, 0.717752212444934\n",
      "num estimators: 100, depth: 2, num features 5, valid accuracy for this fold, 0.7418514008431812\n",
      "num estimators: 100, depth: 2, num features 5, valid accuracy for this fold, 0.7250426272232002\n",
      "avgfold accuracy: 0.7400093511523607\n",
      "standard deviation: 0.016646647668178496\n",
      "avg train time: 6.532483673095703\n",
      "num estimators: 1000, depth: 2, num features 5, valid accuracy for this fold, 0.7582241039235524\n",
      "num estimators: 1000, depth: 2, num features 5, valid accuracy for this fold, 0.747728614527984\n",
      "num estimators: 1000, depth: 2, num features 5, valid accuracy for this fold, 0.7397483643052083\n",
      "num estimators: 1000, depth: 2, num features 5, valid accuracy for this fold, 0.7496153101043368\n",
      "num estimators: 1000, depth: 2, num features 5, valid accuracy for this fold, 0.7476511954435905\n",
      "avgfold accuracy: 0.7485935176609344\n",
      "standard deviation: 0.005893471269737754\n",
      "avg train time: 63.27287321090698\n",
      "num estimators: 100, depth: 2, num features 7, valid accuracy for this fold, 0.7546147299086436\n",
      "num estimators: 100, depth: 2, num features 7, valid accuracy for this fold, 0.7502181923884529\n",
      "num estimators: 100, depth: 2, num features 7, valid accuracy for this fold, 0.7485449156270778\n",
      "num estimators: 100, depth: 2, num features 7, valid accuracy for this fold, 0.7501286912240888\n",
      "num estimators: 100, depth: 2, num features 7, valid accuracy for this fold, 0.7382431048857432\n",
      "avgfold accuracy: 0.7483499268068012\n",
      "standard deviation: 0.005442206299372693\n",
      "avg train time: 8.494547462463379\n",
      "num estimators: 1000, depth: 2, num features 7, valid accuracy for this fold, 0.7502521833372862\n",
      "num estimators: 1000, depth: 2, num features 7, valid accuracy for this fold, 0.7526680496651735\n",
      "num estimators: 1000, depth: 2, num features 7, valid accuracy for this fold, 0.74731791566422\n",
      "num estimators: 1000, depth: 2, num features 7, valid accuracy for this fold, 0.7439003127335123\n",
      "num estimators: 1000, depth: 2, num features 7, valid accuracy for this fold, 0.7473071081018459\n",
      "avgfold accuracy: 0.7482891139004075\n",
      "standard deviation: 0.002973141676102136\n",
      "avg train time: 82.2952287197113\n",
      "num estimators: 100, depth: 2, num features 9, valid accuracy for this fold, 0.7541952999830877\n",
      "num estimators: 100, depth: 2, num features 9, valid accuracy for this fold, 0.7503499925072535\n",
      "num estimators: 100, depth: 2, num features 9, valid accuracy for this fold, 0.7464109401001164\n",
      "num estimators: 100, depth: 2, num features 9, valid accuracy for this fold, 0.743356488528492\n",
      "num estimators: 100, depth: 2, num features 9, valid accuracy for this fold, 0.7472857202267906\n",
      "avgfold accuracy: 0.7483196882691481\n",
      "standard deviation: 0.0036875658492642413\n",
      "avg train time: 10.337958145141602\n",
      "num estimators: 1000, depth: 2, num features 9, valid accuracy for this fold, 0.7550275300143203\n",
      "num estimators: 1000, depth: 2, num features 9, valid accuracy for this fold, 0.7525014037615393\n",
      "num estimators: 1000, depth: 2, num features 9, valid accuracy for this fold, 0.7453584037857828\n",
      "num estimators: 1000, depth: 2, num features 9, valid accuracy for this fold, 0.7479589295104199\n",
      "num estimators: 1000, depth: 2, num features 9, valid accuracy for this fold, 0.7476118853213826\n",
      "avgfold accuracy: 0.7496916304786889\n",
      "standard deviation: 0.0035347162797674516\n",
      "avg train time: 100.91191039085388\n",
      "num estimators: 100, depth: 2, num features 11, valid accuracy for this fold, 0.7542861785518071\n",
      "num estimators: 100, depth: 2, num features 11, valid accuracy for this fold, 0.743320171376264\n",
      "num estimators: 100, depth: 2, num features 11, valid accuracy for this fold, 0.748266741578794\n",
      "num estimators: 100, depth: 2, num features 11, valid accuracy for this fold, 0.7447241210712277\n",
      "num estimators: 100, depth: 2, num features 11, valid accuracy for this fold, 0.7514189270812086\n",
      "avgfold accuracy: 0.7484032279318603\n",
      "standard deviation: 0.004076652281468962\n",
      "avg train time: 12.004352617263795\n",
      "num estimators: 1000, depth: 2, num features 11, valid accuracy for this fold, 0.7527505621586894\n",
      "num estimators: 1000, depth: 2, num features 11, valid accuracy for this fold, 0.7493894765729803\n",
      "num estimators: 1000, depth: 2, num features 11, valid accuracy for this fold, 0.7459606564855226\n",
      "num estimators: 1000, depth: 2, num features 11, valid accuracy for this fold, 0.7459741326026992\n",
      "num estimators: 1000, depth: 2, num features 11, valid accuracy for this fold, 0.7541873696676364\n",
      "avgfold accuracy: 0.7496524394975055\n",
      "standard deviation: 0.003387987856865059\n",
      "avg train time: 120.21223382949829\n",
      "num estimators: 100, depth: 2, num features 13, valid accuracy for this fold, 0.7534044125078933\n",
      "num estimators: 100, depth: 2, num features 13, valid accuracy for this fold, 0.7429966291668246\n",
      "num estimators: 100, depth: 2, num features 13, valid accuracy for this fold, 0.7444031695362929\n",
      "num estimators: 100, depth: 2, num features 13, valid accuracy for this fold, 0.7414025959649074\n",
      "num estimators: 100, depth: 2, num features 13, valid accuracy for this fold, 0.7496676957931238\n",
      "avgfold accuracy: 0.7463749005938085\n",
      "standard deviation: 0.004478410264704009\n",
      "avg train time: 13.830558776855469\n",
      "num estimators: 1000, depth: 2, num features 13, valid accuracy for this fold, 0.7516607698797025\n",
      "num estimators: 1000, depth: 2, num features 13, valid accuracy for this fold, 0.7460791270137884\n",
      "num estimators: 1000, depth: 2, num features 13, valid accuracy for this fold, 0.7438218420117881\n",
      "num estimators: 1000, depth: 2, num features 13, valid accuracy for this fold, 0.7458583566269061\n",
      "num estimators: 1000, depth: 2, num features 13, valid accuracy for this fold, 0.7501639737087565\n",
      "avgfold accuracy: 0.7475168138481882\n",
      "standard deviation: 0.0029206186871602044\n",
      "avg train time: 138.44544854164124\n",
      "num estimators: 100, depth: 2, num features 15, valid accuracy for this fold, 0.7490898383124531\n",
      "num estimators: 100, depth: 2, num features 15, valid accuracy for this fold, 0.7392085493140075\n",
      "num estimators: 100, depth: 2, num features 15, valid accuracy for this fold, 0.7383565785873728\n",
      "num estimators: 100, depth: 2, num features 15, valid accuracy for this fold, 0.7410958588178858\n",
      "num estimators: 100, depth: 2, num features 15, valid accuracy for this fold, 0.7472677979796379\n",
      "avgfold accuracy: 0.7430037246022714\n",
      "standard deviation: 0.004355740471251577\n",
      "avg train time: 15.717182922363282\n",
      "num estimators: 1000, depth: 2, num features 15, valid accuracy for this fold, 0.750174689499266\n",
      "num estimators: 1000, depth: 2, num features 15, valid accuracy for this fold, 0.7435797995554905\n",
      "num estimators: 1000, depth: 2, num features 15, valid accuracy for this fold, 0.7384651279339057\n",
      "num estimators: 1000, depth: 2, num features 15, valid accuracy for this fold, 0.7451775385381785\n",
      "num estimators: 1000, depth: 2, num features 15, valid accuracy for this fold, 0.7461477070415619\n",
      "avgfold accuracy: 0.7447089725136805\n",
      "standard deviation: 0.003805895427713177\n",
      "avg train time: 158.72806406021118\n",
      "num estimators: 100, depth: 2, num features 17, valid accuracy for this fold, 0.7462315728534645\n",
      "num estimators: 100, depth: 2, num features 17, valid accuracy for this fold, 0.7371233992153379\n",
      "num estimators: 100, depth: 2, num features 17, valid accuracy for this fold, 0.738921035189344\n",
      "num estimators: 100, depth: 2, num features 17, valid accuracy for this fold, 0.7453048459856642\n",
      "num estimators: 100, depth: 2, num features 17, valid accuracy for this fold, 0.7473918674585461\n",
      "avgfold accuracy: 0.7429945441404714\n",
      "standard deviation: 0.004152496111380784\n",
      "avg train time: 17.706076765060423\n",
      "num estimators: 1000, depth: 2, num features 17, valid accuracy for this fold, 0.7475542219193354\n",
      "num estimators: 1000, depth: 2, num features 17, valid accuracy for this fold, 0.7427565001832563\n",
      "num estimators: 1000, depth: 2, num features 17, valid accuracy for this fold, 0.7382263193715333\n",
      "num estimators: 1000, depth: 2, num features 17, valid accuracy for this fold, 0.7448071476674138\n",
      "num estimators: 1000, depth: 2, num features 17, valid accuracy for this fold, 0.7476825841305927\n",
      "avgfold accuracy: 0.7442053546544264\n",
      "standard deviation: 0.003507598432292104\n",
      "avg train time: 178.99315843582153\n",
      "num estimators: 100, depth: 2, num features 20, valid accuracy for this fold, 0.7412218057527823\n",
      "num estimators: 100, depth: 2, num features 20, valid accuracy for this fold, 0.7385444933729817\n",
      "num estimators: 100, depth: 2, num features 20, valid accuracy for this fold, 0.7399848187853668\n",
      "num estimators: 100, depth: 2, num features 20, valid accuracy for this fold, 0.7422725301893929\n",
      "num estimators: 100, depth: 2, num features 20, valid accuracy for this fold, 0.7470976851585971\n",
      "avgfold accuracy: 0.7418242666518242\n",
      "standard deviation: 0.002915932896881782\n",
      "avg train time: 20.727979040145875\n",
      "num estimators: 1000, depth: 2, num features 20, valid accuracy for this fold, 0.7477688779742873\n",
      "num estimators: 1000, depth: 2, num features 20, valid accuracy for this fold, 0.7405047041809525\n",
      "num estimators: 1000, depth: 2, num features 20, valid accuracy for this fold, 0.7387039364962782\n",
      "num estimators: 1000, depth: 2, num features 20, valid accuracy for this fold, 0.7417633003994502\n",
      "num estimators: 1000, depth: 2, num features 20, valid accuracy for this fold, 0.7495613505254881\n",
      "avgfold accuracy: 0.7436604339152912\n",
      "standard deviation: 0.004238507799313276\n",
      "avg train time: 208.17321376800538\n",
      "num estimators: 100, depth: 3, num features 1, valid accuracy for this fold, 0.6985656071964318\n",
      "num estimators: 100, depth: 3, num features 1, valid accuracy for this fold, 0.6930620778559551\n",
      "num estimators: 100, depth: 3, num features 1, valid accuracy for this fold, 0.7192750420726804\n",
      "num estimators: 100, depth: 3, num features 1, valid accuracy for this fold, 0.7012075757156431\n",
      "num estimators: 100, depth: 3, num features 1, valid accuracy for this fold, 0.5970931303333538\n",
      "avgfold accuracy: 0.6818406866348129\n",
      "standard deviation: 0.0432763174207722\n",
      "avg train time: 2.9014320373535156\n",
      "num estimators: 1000, depth: 3, num features 1, valid accuracy for this fold, 0.7287496760144625\n",
      "num estimators: 1000, depth: 3, num features 1, valid accuracy for this fold, 0.7244814206969518\n",
      "num estimators: 1000, depth: 3, num features 1, valid accuracy for this fold, 0.7011915056343649\n",
      "num estimators: 1000, depth: 3, num features 1, valid accuracy for this fold, 0.7519792618013081\n",
      "num estimators: 1000, depth: 3, num features 1, valid accuracy for this fold, 0.6068237223220103\n",
      "avgfold accuracy: 0.7026451172938195\n",
      "standard deviation: 0.05054895185421529\n",
      "avg train time: 28.81676378250122\n",
      "num estimators: 100, depth: 3, num features 3, valid accuracy for this fold, 0.7509701427938111\n",
      "num estimators: 100, depth: 3, num features 3, valid accuracy for this fold, 0.7480106306725959\n",
      "num estimators: 100, depth: 3, num features 3, valid accuracy for this fold, 0.7606501295006782\n",
      "num estimators: 100, depth: 3, num features 3, valid accuracy for this fold, 0.7645527172759896\n",
      "num estimators: 100, depth: 3, num features 3, valid accuracy for this fold, 0.6879040674589422\n",
      "avgfold accuracy: 0.7424175375404033\n",
      "standard deviation: 0.027923258259898505\n",
      "avg train time: 5.92877721786499\n",
      "num estimators: 1000, depth: 3, num features 3, valid accuracy for this fold, 0.7591188794253423\n",
      "num estimators: 1000, depth: 3, num features 3, valid accuracy for this fold, 0.757383063504186\n",
      "num estimators: 1000, depth: 3, num features 3, valid accuracy for this fold, 0.7602754380816218\n",
      "num estimators: 1000, depth: 3, num features 3, valid accuracy for this fold, 0.7544469967435123\n",
      "num estimators: 1000, depth: 3, num features 3, valid accuracy for this fold, 0.6830158488114877\n",
      "avgfold accuracy: 0.74284804531323\n",
      "standard deviation: 0.029980487746947174\n",
      "avg train time: 58.178554964065555\n",
      "num estimators: 100, depth: 3, num features 5, valid accuracy for this fold, 0.7662159139694558\n",
      "num estimators: 100, depth: 3, num features 5, valid accuracy for this fold, 0.7521392242569994\n",
      "num estimators: 100, depth: 3, num features 5, valid accuracy for this fold, 0.7432919119489552\n",
      "num estimators: 100, depth: 3, num features 5, valid accuracy for this fold, 0.7458731169106727\n",
      "num estimators: 100, depth: 3, num features 5, valid accuracy for this fold, 0.7413342469388604\n",
      "avgfold accuracy: 0.7497708828049887\n",
      "standard deviation: 0.008993270353897657\n",
      "avg train time: 8.741530656814575\n",
      "num estimators: 1000, depth: 3, num features 5, valid accuracy for this fold, 0.766917736694577\n",
      "num estimators: 1000, depth: 3, num features 5, valid accuracy for this fold, 0.7664315930301209\n",
      "num estimators: 1000, depth: 3, num features 5, valid accuracy for this fold, 0.754032673614871\n",
      "num estimators: 1000, depth: 3, num features 5, valid accuracy for this fold, 0.7558727479035785\n",
      "num estimators: 1000, depth: 3, num features 5, valid accuracy for this fold, 0.7488849589768674\n",
      "avgfold accuracy: 0.7584279420440028\n",
      "standard deviation: 0.007114084196796121\n",
      "avg train time: 86.4766921043396\n",
      "num estimators: 100, depth: 3, num features 7, valid accuracy for this fold, 0.7598951938058969\n",
      "num estimators: 100, depth: 3, num features 7, valid accuracy for this fold, 0.761952194471978\n",
      "num estimators: 100, depth: 3, num features 7, valid accuracy for this fold, 0.7576678996826174\n",
      "num estimators: 100, depth: 3, num features 7, valid accuracy for this fold, 0.7483173276506241\n",
      "num estimators: 100, depth: 3, num features 7, valid accuracy for this fold, 0.7538332815139447\n",
      "avgfold accuracy: 0.7563331794250122\n",
      "standard deviation: 0.004827962799120146\n",
      "avg train time: 11.820778274536133\n",
      "num estimators: 1000, depth: 3, num features 7, valid accuracy for this fold, 0.7624611842809723\n",
      "num estimators: 1000, depth: 3, num features 7, valid accuracy for this fold, 0.7595631819076352\n",
      "num estimators: 1000, depth: 3, num features 7, valid accuracy for this fold, 0.7517033092641505\n",
      "num estimators: 1000, depth: 3, num features 7, valid accuracy for this fold, 0.7518454967296746\n",
      "num estimators: 1000, depth: 3, num features 7, valid accuracy for this fold, 0.7464457510411737\n",
      "avgfold accuracy: 0.7544037846447212\n",
      "standard deviation: 0.005808765310930653\n",
      "avg train time: 116.47012577056884\n",
      "num estimators: 100, depth: 3, num features 9, valid accuracy for this fold, 0.7585891818351942\n",
      "num estimators: 100, depth: 3, num features 9, valid accuracy for this fold, 0.7579508873036764\n",
      "num estimators: 100, depth: 3, num features 9, valid accuracy for this fold, 0.7489244459929074\n",
      "num estimators: 100, depth: 3, num features 9, valid accuracy for this fold, 0.7523298185407614\n",
      "num estimators: 100, depth: 3, num features 9, valid accuracy for this fold, 0.7434218421693642\n",
      "avgfold accuracy: 0.7522432351683807\n",
      "standard deviation: 0.005686601176630821\n",
      "avg train time: 15.03521237373352\n",
      "num estimators: 1000, depth: 3, num features 9, valid accuracy for this fold, 0.75855615782674\n",
      "num estimators: 1000, depth: 3, num features 9, valid accuracy for this fold, 0.7592127380301118\n",
      "num estimators: 1000, depth: 3, num features 9, valid accuracy for this fold, 0.7461672925909709\n",
      "num estimators: 1000, depth: 3, num features 9, valid accuracy for this fold, 0.7505069234956042\n",
      "num estimators: 1000, depth: 3, num features 9, valid accuracy for this fold, 0.7529892525927848\n",
      "avgfold accuracy: 0.7534864729072424\n",
      "standard deviation: 0.004923152662267761\n",
      "avg train time: 146.74740133285522\n",
      "num estimators: 100, depth: 3, num features 11, valid accuracy for this fold, 0.7622994416941116\n",
      "num estimators: 100, depth: 3, num features 11, valid accuracy for this fold, 0.7653531791813588\n",
      "num estimators: 100, depth: 3, num features 11, valid accuracy for this fold, 0.7477625756379693\n",
      "num estimators: 100, depth: 3, num features 11, valid accuracy for this fold, 0.753876880783033\n",
      "num estimators: 100, depth: 3, num features 11, valid accuracy for this fold, 0.7456196443671664\n",
      "avgfold accuracy: 0.7549823443327279\n",
      "standard deviation: 0.007773048173840037\n",
      "avg train time: 17.731105184555055\n",
      "num estimators: 1000, depth: 3, num features 11, valid accuracy for this fold, 0.7566903013490808\n",
      "num estimators: 1000, depth: 3, num features 11, valid accuracy for this fold, 0.7549124431950516\n",
      "num estimators: 1000, depth: 3, num features 11, valid accuracy for this fold, 0.7498467230914017\n",
      "num estimators: 1000, depth: 3, num features 11, valid accuracy for this fold, 0.752403158700726\n",
      "num estimators: 1000, depth: 3, num features 11, valid accuracy for this fold, 0.7511459346204346\n",
      "avgfold accuracy: 0.752999712191339\n",
      "standard deviation: 0.0024904897445152847\n",
      "avg train time: 185.0422975540161\n",
      "num estimators: 100, depth: 3, num features 13, valid accuracy for this fold, 0.7601875938807513\n",
      "num estimators: 100, depth: 3, num features 13, valid accuracy for this fold, 0.7483309230160923\n",
      "num estimators: 100, depth: 3, num features 13, valid accuracy for this fold, 0.748056051220644\n",
      "num estimators: 100, depth: 3, num features 13, valid accuracy for this fold, 0.7510239946862978\n",
      "num estimators: 100, depth: 3, num features 13, valid accuracy for this fold, 0.7526736824177804\n",
      "avgfold accuracy: 0.7520544490443133\n",
      "standard deviation: 0.004414638429688429\n",
      "avg train time: 21.643911361694336\n",
      "num estimators: 1000, depth: 3, num features 13, valid accuracy for this fold, 0.7588368618986003\n",
      "num estimators: 1000, depth: 3, num features 13, valid accuracy for this fold, 0.7460693774159594\n",
      "num estimators: 1000, depth: 3, num features 13, valid accuracy for this fold, 0.7505301916275235\n",
      "num estimators: 1000, depth: 3, num features 13, valid accuracy for this fold, 0.7508966872388121\n",
      "num estimators: 1000, depth: 3, num features 13, valid accuracy for this fold, 0.7538080319392267\n",
      "avgfold accuracy: 0.7520282300240244\n",
      "standard deviation: 0.0042085055632053\n",
      "avg train time: 224.26600108146667\n",
      "num estimators: 100, depth: 3, num features 15, valid accuracy for this fold, 0.7562725601512299\n",
      "num estimators: 100, depth: 3, num features 15, valid accuracy for this fold, 0.7501515701366208\n",
      "num estimators: 100, depth: 3, num features 15, valid accuracy for this fold, 0.7488368218216097\n",
      "num estimators: 100, depth: 3, num features 15, valid accuracy for this fold, 0.7489949169272779\n",
      "num estimators: 100, depth: 3, num features 15, valid accuracy for this fold, 0.7450736594456184\n",
      "avgfold accuracy: 0.7498659056964714\n",
      "standard deviation: 0.0036307047431984514\n",
      "avg train time: 27.58899164199829\n",
      "num estimators: 1000, depth: 3, num features 15, valid accuracy for this fold, 0.7600818920355098\n",
      "num estimators: 1000, depth: 3, num features 15, valid accuracy for this fold, 0.7487425727022093\n",
      "num estimators: 1000, depth: 3, num features 15, valid accuracy for this fold, 0.7499601113847078\n",
      "num estimators: 1000, depth: 3, num features 15, valid accuracy for this fold, 0.7505977914925415\n",
      "num estimators: 1000, depth: 3, num features 15, valid accuracy for this fold, 0.7565233018918367\n",
      "avgfold accuracy: 0.753181133901361\n",
      "standard deviation: 0.0043712702793396\n",
      "avg train time: 237.33378505706787\n",
      "num estimators: 100, depth: 3, num features 17, valid accuracy for this fold, 0.7590581477734313\n",
      "num estimators: 100, depth: 3, num features 17, valid accuracy for this fold, 0.7532118605663072\n",
      "num estimators: 100, depth: 3, num features 17, valid accuracy for this fold, 0.7493851921951189\n",
      "num estimators: 100, depth: 3, num features 17, valid accuracy for this fold, 0.7469386248950636\n",
      "num estimators: 100, depth: 3, num features 17, valid accuracy for this fold, 0.750358940032755\n",
      "avgfold accuracy: 0.7517905530925353\n",
      "standard deviation: 0.0041524578652639395\n",
      "avg train time: 26.411019802093506\n",
      "num estimators: 1000, depth: 3, num features 17, valid accuracy for this fold, 0.7584306916128025\n",
      "num estimators: 1000, depth: 3, num features 17, valid accuracy for this fold, 0.7487008660892738\n",
      "num estimators: 1000, depth: 3, num features 17, valid accuracy for this fold, 0.7510456048500371\n",
      "num estimators: 1000, depth: 3, num features 17, valid accuracy for this fold, 0.7531536268784768\n",
      "num estimators: 1000, depth: 3, num features 17, valid accuracy for this fold, 0.7543858016195374\n",
      "avgfold accuracy: 0.7531433182100256\n",
      "standard deviation: 0.003275239563079213\n",
      "avg train time: 260.84715938568115\n",
      "num estimators: 100, depth: 3, num features 20, valid accuracy for this fold, 0.7571262432788637\n",
      "num estimators: 100, depth: 3, num features 20, valid accuracy for this fold, 0.7511559592611249\n",
      "num estimators: 100, depth: 3, num features 20, valid accuracy for this fold, 0.7460635821912112\n",
      "num estimators: 100, depth: 3, num features 20, valid accuracy for this fold, 0.7509681823633059\n",
      "num estimators: 100, depth: 3, num features 20, valid accuracy for this fold, 0.7534924617642225\n",
      "avgfold accuracy: 0.7517612857717457\n",
      "standard deviation: 0.0036121738236880273\n",
      "avg train time: 30.38500919342041\n",
      "num estimators: 1000, depth: 3, num features 20, valid accuracy for this fold, 0.7558927840540073\n",
      "num estimators: 1000, depth: 3, num features 20, valid accuracy for this fold, 0.746767556949387\n",
      "num estimators: 1000, depth: 3, num features 20, valid accuracy for this fold, 0.7475342296632146\n",
      "num estimators: 1000, depth: 3, num features 20, valid accuracy for this fold, 0.7535567671288481\n",
      "num estimators: 1000, depth: 3, num features 20, valid accuracy for this fold, 0.7550344681449386\n",
      "avgfold accuracy: 0.7517571611880791\n",
      "standard deviation: 0.003842186878411492\n",
      "avg train time: 304.25318365097047\n",
      "num estimators: 100, depth: 4, num features 1, valid accuracy for this fold, 0.7350027845243492\n",
      "num estimators: 100, depth: 4, num features 1, valid accuracy for this fold, 0.6540613394141936\n",
      "num estimators: 100, depth: 4, num features 1, valid accuracy for this fold, 0.7163723279205136\n",
      "num estimators: 100, depth: 4, num features 1, valid accuracy for this fold, 0.7336502181754444\n",
      "num estimators: 100, depth: 4, num features 1, valid accuracy for this fold, 0.6390286142043216\n",
      "avgfold accuracy: 0.6956230568477645\n",
      "standard deviation: 0.0408843918744641\n",
      "avg train time: 3.5552420616149902\n",
      "num estimators: 1000, depth: 4, num features 1, valid accuracy for this fold, 0.7270009672031567\n",
      "num estimators: 1000, depth: 4, num features 1, valid accuracy for this fold, 0.7252631940043585\n",
      "num estimators: 1000, depth: 4, num features 1, valid accuracy for this fold, 0.7157966240368534\n",
      "num estimators: 1000, depth: 4, num features 1, valid accuracy for this fold, 0.7538058469174069\n",
      "num estimators: 1000, depth: 4, num features 1, valid accuracy for this fold, 0.613349895734109\n",
      "avgfold accuracy: 0.7070433055791769\n",
      "standard deviation: 0.04852233196488061\n",
      "avg train time: 35.029012966156\n",
      "num estimators: 100, depth: 4, num features 3, valid accuracy for this fold, 0.7603505248315525\n",
      "num estimators: 100, depth: 4, num features 3, valid accuracy for this fold, 0.7600735914087988\n",
      "num estimators: 100, depth: 4, num features 3, valid accuracy for this fold, 0.7673794042916489\n",
      "num estimators: 100, depth: 4, num features 3, valid accuracy for this fold, 0.7693479644646168\n",
      "num estimators: 100, depth: 4, num features 3, valid accuracy for this fold, 0.6750116346079582\n",
      "avgfold accuracy: 0.746432623920915\n",
      "standard deviation: 0.03590159359788616\n",
      "avg train time: 7.195971059799194\n",
      "num estimators: 1000, depth: 4, num features 3, valid accuracy for this fold, 0.7507141441828209\n",
      "num estimators: 1000, depth: 4, num features 3, valid accuracy for this fold, 0.7591130754745256\n",
      "num estimators: 1000, depth: 4, num features 3, valid accuracy for this fold, 0.7607047965209803\n",
      "num estimators: 1000, depth: 4, num features 3, valid accuracy for this fold, 0.7611942914602533\n",
      "num estimators: 1000, depth: 4, num features 3, valid accuracy for this fold, 0.6950540538934844\n",
      "avgfold accuracy: 0.7453560723064129\n",
      "standard deviation: 0.025434974534269694\n",
      "avg train time: 72.38737788200379\n",
      "num estimators: 100, depth: 4, num features 5, valid accuracy for this fold, 0.7653011239170877\n",
      "num estimators: 100, depth: 4, num features 5, valid accuracy for this fold, 0.7690020564429495\n",
      "num estimators: 100, depth: 4, num features 5, valid accuracy for this fold, 0.761788720598167\n",
      "num estimators: 100, depth: 4, num features 5, valid accuracy for this fold, 0.753903172538492\n",
      "num estimators: 100, depth: 4, num features 5, valid accuracy for this fold, 0.7327172107042355\n",
      "avgfold accuracy: 0.7565424568401864\n",
      "standard deviation: 0.012915783597655129\n",
      "avg train time: 11.057421016693116\n",
      "num estimators: 1000, depth: 4, num features 5, valid accuracy for this fold, 0.771681387368617\n",
      "num estimators: 1000, depth: 4, num features 5, valid accuracy for this fold, 0.7641211188927346\n",
      "num estimators: 1000, depth: 4, num features 5, valid accuracy for this fold, 0.770629476352721\n",
      "num estimators: 1000, depth: 4, num features 5, valid accuracy for this fold, 0.7566250611168\n",
      "num estimators: 1000, depth: 4, num features 5, valid accuracy for this fold, 0.7380379397139174\n",
      "avgfold accuracy: 0.7602189966889581\n",
      "standard deviation: 0.012328071268797765\n",
      "avg train time: 110.63726019859314\n",
      "num estimators: 100, depth: 4, num features 7, valid accuracy for this fold, 0.7617610502835561\n",
      "num estimators: 100, depth: 4, num features 7, valid accuracy for this fold, 0.757563972708348\n",
      "num estimators: 100, depth: 4, num features 7, valid accuracy for this fold, 0.7596708312996678\n",
      "num estimators: 100, depth: 4, num features 7, valid accuracy for this fold, 0.7608631075932435\n",
      "num estimators: 100, depth: 4, num features 7, valid accuracy for this fold, 0.7563641800621437\n",
      "avgfold accuracy: 0.7592446283893917\n",
      "standard deviation: 0.0020127140426612587\n",
      "avg train time: 14.796634006500245\n",
      "num estimators: 1000, depth: 4, num features 7, valid accuracy for this fold, 0.767386702632814\n",
      "num estimators: 1000, depth: 4, num features 7, valid accuracy for this fold, 0.7645006310156373\n",
      "num estimators: 1000, depth: 4, num features 7, valid accuracy for this fold, 0.7608270780137614\n",
      "num estimators: 1000, depth: 4, num features 7, valid accuracy for this fold, 0.7606739914574858\n",
      "num estimators: 1000, depth: 4, num features 7, valid accuracy for this fold, 0.7611425878140601\n",
      "avgfold accuracy: 0.7629061981867518\n",
      "standard deviation: 0.0026469962319426604\n",
      "avg train time: 148.29131903648377\n",
      "num estimators: 100, depth: 4, num features 9, valid accuracy for this fold, 0.7658626571517493\n",
      "num estimators: 100, depth: 4, num features 9, valid accuracy for this fold, 0.7677790235597226\n",
      "num estimators: 100, depth: 4, num features 9, valid accuracy for this fold, 0.7623950275506088\n",
      "num estimators: 100, depth: 4, num features 9, valid accuracy for this fold, 0.7650430354523565\n",
      "num estimators: 100, depth: 4, num features 9, valid accuracy for this fold, 0.752656156242388\n",
      "avgfold accuracy: 0.7627471799913651\n",
      "standard deviation: 0.005333842359406781\n",
      "avg train time: 18.611734867095947\n",
      "num estimators: 1000, depth: 4, num features 9, valid accuracy for this fold, 0.7653788679369902\n",
      "num estimators: 1000, depth: 4, num features 9, valid accuracy for this fold, 0.7628735314668269\n",
      "num estimators: 1000, depth: 4, num features 9, valid accuracy for this fold, 0.7593395596192246\n",
      "num estimators: 1000, depth: 4, num features 9, valid accuracy for this fold, 0.7618797221376581\n",
      "num estimators: 1000, depth: 4, num features 9, valid accuracy for this fold, 0.7586929829946589\n",
      "avgfold accuracy: 0.7616329328310718\n",
      "standard deviation: 0.002430427348598464\n",
      "avg train time: 185.21580510139466\n",
      "num estimators: 100, depth: 4, num features 11, valid accuracy for this fold, 0.7652319486266516\n",
      "num estimators: 100, depth: 4, num features 11, valid accuracy for this fold, 0.7579337352334217\n",
      "num estimators: 100, depth: 4, num features 11, valid accuracy for this fold, 0.7548834127705298\n",
      "num estimators: 100, depth: 4, num features 11, valid accuracy for this fold, 0.7586758180426019\n",
      "num estimators: 100, depth: 4, num features 11, valid accuracy for this fold, 0.7535812808564656\n",
      "avgfold accuracy: 0.758061239105934\n",
      "standard deviation: 0.004049238093083172\n",
      "avg train time: 22.4202778339386\n",
      "num estimators: 1000, depth: 4, num features 11, valid accuracy for this fold, 0.7644938495286073\n",
      "num estimators: 1000, depth: 4, num features 11, valid accuracy for this fold, 0.7624887834487939\n",
      "num estimators: 1000, depth: 4, num features 11, valid accuracy for this fold, 0.7564955667400617\n",
      "num estimators: 1000, depth: 4, num features 11, valid accuracy for this fold, 0.7611352503251875\n",
      "num estimators: 1000, depth: 4, num features 11, valid accuracy for this fold, 0.7546979061666392\n",
      "avgfold accuracy: 0.7598622712418579\n",
      "standard deviation: 0.0036871649102968086\n",
      "avg train time: 227.76879105567932\n",
      "num estimators: 100, depth: 4, num features 13, valid accuracy for this fold, 0.7591043063307024\n",
      "num estimators: 100, depth: 4, num features 13, valid accuracy for this fold, 0.7603888284052727\n",
      "num estimators: 100, depth: 4, num features 13, valid accuracy for this fold, 0.7557735174120999\n",
      "num estimators: 100, depth: 4, num features 13, valid accuracy for this fold, 0.759883393758245\n",
      "num estimators: 100, depth: 4, num features 13, valid accuracy for this fold, 0.7598413930635952\n",
      "avgfold accuracy: 0.758998287793983\n",
      "standard deviation: 0.001663624128297297\n",
      "avg train time: 28.168640613555908\n",
      "num estimators: 1000, depth: 4, num features 13, valid accuracy for this fold, 0.7610147577288688\n",
      "num estimators: 1000, depth: 4, num features 13, valid accuracy for this fold, 0.7586467197117008\n",
      "num estimators: 1000, depth: 4, num features 13, valid accuracy for this fold, 0.7555347088497274\n",
      "num estimators: 1000, depth: 4, num features 13, valid accuracy for this fold, 0.7613358979326377\n",
      "num estimators: 1000, depth: 4, num features 13, valid accuracy for this fold, 0.757820832978519\n",
      "avgfold accuracy: 0.7588705834402907\n",
      "standard deviation: 0.0021426809940310546\n",
      "avg train time: 269.9513917922974\n",
      "num estimators: 100, depth: 4, num features 15, valid accuracy for this fold, 0.7611204595741103\n",
      "num estimators: 100, depth: 4, num features 15, valid accuracy for this fold, 0.7587373548618896\n",
      "num estimators: 100, depth: 4, num features 15, valid accuracy for this fold, 0.7573800477407873\n",
      "num estimators: 100, depth: 4, num features 15, valid accuracy for this fold, 0.7607727008551739\n",
      "num estimators: 100, depth: 4, num features 15, valid accuracy for this fold, 0.7537868421000518\n",
      "avgfold accuracy: 0.7583594810264026\n",
      "standard deviation: 0.0026629959605742274\n",
      "avg train time: 30.32822756767273\n",
      "num estimators: 1000, depth: 4, num features 15, valid accuracy for this fold, 0.7605474805183368\n",
      "num estimators: 1000, depth: 4, num features 15, valid accuracy for this fold, 0.7586687465808701\n",
      "num estimators: 1000, depth: 4, num features 15, valid accuracy for this fold, 0.754188696952719\n",
      "num estimators: 1000, depth: 4, num features 15, valid accuracy for this fold, 0.7531900663290252\n",
      "num estimators: 1000, depth: 4, num features 15, valid accuracy for this fold, 0.7591784679548241\n",
      "avgfold accuracy: 0.757154691667155\n",
      "standard deviation: 0.0029125393611343195\n",
      "avg train time: 305.4799514293671\n",
      "num estimators: 100, depth: 4, num features 17, valid accuracy for this fold, 0.7630028280559967\n",
      "num estimators: 100, depth: 4, num features 17, valid accuracy for this fold, 0.7594528670136802\n",
      "num estimators: 100, depth: 4, num features 17, valid accuracy for this fold, 0.7467912551600174\n",
      "num estimators: 100, depth: 4, num features 17, valid accuracy for this fold, 0.7593626324966097\n",
      "num estimators: 100, depth: 4, num features 17, valid accuracy for this fold, 0.7577322119221561\n",
      "avgfold accuracy: 0.757268358929692\n",
      "standard deviation: 0.005514110200237011\n",
      "avg train time: 35.813889503479004\n",
      "num estimators: 1000, depth: 4, num features 17, valid accuracy for this fold, 0.758995352120992\n",
      "num estimators: 1000, depth: 4, num features 17, valid accuracy for this fold, 0.7566448022907943\n",
      "num estimators: 1000, depth: 4, num features 17, valid accuracy for this fold, 0.7517354817210747\n",
      "num estimators: 1000, depth: 4, num features 17, valid accuracy for this fold, 0.7563921253886106\n",
      "num estimators: 1000, depth: 4, num features 17, valid accuracy for this fold, 0.757998075091245\n",
      "avgfold accuracy: 0.7563531673225433\n",
      "standard deviation: 0.0024938504797327775\n",
      "avg train time: 358.32796573638916\n",
      "num estimators: 100, depth: 4, num features 20, valid accuracy for this fold, 0.7593322220254124\n",
      "num estimators: 100, depth: 4, num features 20, valid accuracy for this fold, 0.7547017435530784\n",
      "num estimators: 100, depth: 4, num features 20, valid accuracy for this fold, 0.749303976358737\n",
      "num estimators: 100, depth: 4, num features 20, valid accuracy for this fold, 0.7536974510834971\n",
      "num estimators: 100, depth: 4, num features 20, valid accuracy for this fold, 0.7634711927106953\n",
      "avgfold accuracy: 0.756101317146284\n",
      "standard deviation: 0.004872269127327624\n",
      "avg train time: 40.26550917625427\n",
      "num estimators: 1000, depth: 4, num features 20, valid accuracy for this fold, 0.759893630169133\n",
      "num estimators: 1000, depth: 4, num features 20, valid accuracy for this fold, 0.7527978637547867\n",
      "num estimators: 1000, depth: 4, num features 20, valid accuracy for this fold, 0.7467912551600174\n",
      "num estimators: 1000, depth: 4, num features 20, valid accuracy for this fold, 0.7552251404533252\n",
      "num estimators: 1000, depth: 4, num features 20, valid accuracy for this fold, 0.7638150820165597\n",
      "avgfold accuracy: 0.7557045943107644\n",
      "standard deviation: 0.005855845038784081\n",
      "avg train time: 406.8420236110687\n",
      "num estimators: 100, depth: 5, num features 1, valid accuracy for this fold, 0.7294398027365895\n",
      "num estimators: 100, depth: 5, num features 1, valid accuracy for this fold, 0.6854443920854931\n",
      "num estimators: 100, depth: 5, num features 1, valid accuracy for this fold, 0.7106304598359571\n",
      "num estimators: 100, depth: 5, num features 1, valid accuracy for this fold, 0.7241713484441739\n",
      "num estimators: 100, depth: 5, num features 1, valid accuracy for this fold, 0.6257393174495355\n",
      "avgfold accuracy: 0.6950850641103499\n",
      "standard deviation: 0.037867385618042676\n",
      "avg train time: 3.9608248233795167\n",
      "num estimators: 1000, depth: 5, num features 1, valid accuracy for this fold, 0.7335759972500007\n",
      "num estimators: 1000, depth: 5, num features 1, valid accuracy for this fold, 0.6827442951311592\n",
      "num estimators: 1000, depth: 5, num features 1, valid accuracy for this fold, 0.7200356721924823\n",
      "num estimators: 1000, depth: 5, num features 1, valid accuracy for this fold, 0.7545000415132981\n",
      "num estimators: 1000, depth: 5, num features 1, valid accuracy for this fold, 0.6274974799934252\n",
      "avgfold accuracy: 0.7036706972160731\n",
      "standard deviation: 0.04469012182586161\n",
      "avg train time: 39.672771310806276\n",
      "num estimators: 100, depth: 5, num features 3, valid accuracy for this fold, 0.7589519455644254\n",
      "num estimators: 100, depth: 5, num features 3, valid accuracy for this fold, 0.7517600732303127\n",
      "num estimators: 100, depth: 5, num features 3, valid accuracy for this fold, 0.765525172201114\n",
      "num estimators: 100, depth: 5, num features 3, valid accuracy for this fold, 0.7689194549765219\n",
      "num estimators: 100, depth: 5, num features 3, valid accuracy for this fold, 0.6935688838103687\n",
      "avgfold accuracy: 0.7477451059565485\n",
      "standard deviation: 0.027717359449698963\n",
      "avg train time: 8.970330572128296\n",
      "num estimators: 1000, depth: 5, num features 3, valid accuracy for this fold, 0.7576144732220325\n",
      "num estimators: 1000, depth: 5, num features 3, valid accuracy for this fold, 0.7523923527043399\n",
      "num estimators: 1000, depth: 5, num features 3, valid accuracy for this fold, 0.7712284594938305\n",
      "num estimators: 1000, depth: 5, num features 3, valid accuracy for this fold, 0.7689927951364866\n",
      "num estimators: 1000, depth: 5, num features 3, valid accuracy for this fold, 0.7013212953922991\n",
      "avgfold accuracy: 0.7503098751897976\n",
      "standard deviation: 0.02547239086778583\n",
      "avg train time: 86.84211926460266\n",
      "num estimators: 100, depth: 5, num features 5, valid accuracy for this fold, 0.7657963589529588\n",
      "num estimators: 100, depth: 5, num features 5, valid accuracy for this fold, 0.7687743852788296\n",
      "num estimators: 100, depth: 5, num features 5, valid accuracy for this fold, 0.7706841433730232\n",
      "num estimators: 100, depth: 5, num features 5, valid accuracy for this fold, 0.7698322862757037\n",
      "num estimators: 100, depth: 5, num features 5, valid accuracy for this fold, 0.7273612313078883\n",
      "avgfold accuracy: 0.7604896810376808\n",
      "standard deviation: 0.0166463369622235\n",
      "avg train time: 13.797703266143799\n",
      "num estimators: 1000, depth: 5, num features 5, valid accuracy for this fold, 0.7684449094491695\n",
      "num estimators: 1000, depth: 5, num features 5, valid accuracy for this fold, 0.7735793120755992\n",
      "num estimators: 1000, depth: 5, num features 5, valid accuracy for this fold, 0.7807953188290482\n",
      "num estimators: 1000, depth: 5, num features 5, valid accuracy for this fold, 0.7642344486572754\n",
      "num estimators: 1000, depth: 5, num features 5, valid accuracy for this fold, 0.7301331395222186\n",
      "avgfold accuracy: 0.7634374257066622\n",
      "standard deviation: 0.017544457415601115\n",
      "avg train time: 132.81128897666932\n",
      "num estimators: 100, depth: 5, num features 7, valid accuracy for this fold, 0.7686794549637587\n",
      "num estimators: 100, depth: 5, num features 7, valid accuracy for this fold, 0.7632831951237566\n",
      "num estimators: 100, depth: 5, num features 7, valid accuracy for this fold, 0.7693542177044769\n",
      "num estimators: 100, depth: 5, num features 7, valid accuracy for this fold, 0.7637736510484414\n",
      "num estimators: 100, depth: 5, num features 7, valid accuracy for this fold, 0.7361237248964767\n",
      "avgfold accuracy: 0.7602428487473821\n",
      "standard deviation: 0.012309636672423171\n",
      "avg train time: 17.822316026687623\n",
      "num estimators: 1000, depth: 5, num features 7, valid accuracy for this fold, 0.7766036533559898\n",
      "num estimators: 1000, depth: 5, num features 7, valid accuracy for this fold, 0.7686198360984275\n",
      "num estimators: 1000, depth: 5, num features 7, valid accuracy for this fold, 0.7797307505389541\n",
      "num estimators: 1000, depth: 5, num features 7, valid accuracy for this fold, 0.7666477550530909\n",
      "num estimators: 1000, depth: 5, num features 7, valid accuracy for this fold, 0.7568003540881536\n",
      "avgfold accuracy: 0.7696804698269233\n",
      "standard deviation: 0.008064220939648212\n",
      "avg train time: 182.52087807655334\n",
      "num estimators: 100, depth: 5, num features 9, valid accuracy for this fold, 0.780239671741356\n",
      "num estimators: 100, depth: 5, num features 9, valid accuracy for this fold, 0.7711727863447855\n",
      "num estimators: 100, depth: 5, num features 9, valid accuracy for this fold, 0.7739993450420152\n",
      "num estimators: 100, depth: 5, num features 9, valid accuracy for this fold, 0.7590535890552496\n",
      "num estimators: 100, depth: 5, num features 9, valid accuracy for this fold, 0.7399413615758903\n",
      "avgfold accuracy: 0.7648813507518593\n",
      "standard deviation: 0.014244745500019232\n",
      "avg train time: 23.68873863220215\n",
      "num estimators: 1000, depth: 5, num features 9, valid accuracy for this fold, 0.7807234609561151\n",
      "num estimators: 1000, depth: 5, num features 9, valid accuracy for this fold, 0.7770019625579334\n",
      "num estimators: 1000, depth: 5, num features 9, valid accuracy for this fold, 0.7735378141457323\n",
      "num estimators: 1000, depth: 5, num features 9, valid accuracy for this fold, 0.7700813660642626\n",
      "num estimators: 1000, depth: 5, num features 9, valid accuracy for this fold, 0.7655027437871194\n",
      "avgfold accuracy: 0.7733694695022326\n",
      "standard deviation: 0.005291531606597487\n",
      "avg train time: 229.49364428520204\n",
      "num estimators: 100, depth: 5, num features 11, valid accuracy for this fold, 0.7757979426042734\n",
      "num estimators: 100, depth: 5, num features 11, valid accuracy for this fold, 0.7716676687086658\n",
      "num estimators: 100, depth: 5, num features 11, valid accuracy for this fold, 0.76210796030285\n",
      "num estimators: 100, depth: 5, num features 11, valid accuracy for this fold, 0.7653728355427634\n",
      "num estimators: 100, depth: 5, num features 11, valid accuracy for this fold, 0.7646094039318043\n",
      "avgfold accuracy: 0.7679111622180714\n",
      "standard deviation: 0.005046474183712838\n",
      "avg train time: 27.877997207641602\n",
      "num estimators: 1000, depth: 5, num features 11, valid accuracy for this fold, 0.7744934942703345\n",
      "num estimators: 1000, depth: 5, num features 11, valid accuracy for this fold, 0.7736695861295722\n",
      "num estimators: 1000, depth: 5, num features 11, valid accuracy for this fold, 0.7673778349035062\n",
      "num estimators: 1000, depth: 5, num features 11, valid accuracy for this fold, 0.7592948274430575\n",
      "num estimators: 1000, depth: 5, num features 11, valid accuracy for this fold, 0.7640776775936264\n",
      "avgfold accuracy: 0.7677826840680194\n",
      "standard deviation: 0.005755437000976046\n",
      "avg train time: 280.7422772407532\n",
      "num estimators: 100, depth: 5, num features 13, valid accuracy for this fold, 0.7765244707902645\n",
      "num estimators: 100, depth: 5, num features 13, valid accuracy for this fold, 0.7698741038043292\n",
      "num estimators: 100, depth: 5, num features 13, valid accuracy for this fold, 0.7692392600230282\n",
      "num estimators: 100, depth: 5, num features 13, valid accuracy for this fold, 0.7560369560604803\n",
      "num estimators: 100, depth: 5, num features 13, valid accuracy for this fold, 0.7630779924706759\n",
      "avgfold accuracy: 0.7669505566297556\n",
      "standard deviation: 0.006921684104450364\n",
      "avg train time: 33.01304054260254\n",
      "num estimators: 1000, depth: 5, num features 13, valid accuracy for this fold, 0.7693466900436617\n",
      "num estimators: 1000, depth: 5, num features 13, valid accuracy for this fold, 0.7651524096853227\n",
      "num estimators: 1000, depth: 5, num features 13, valid accuracy for this fold, 0.7654022367966068\n",
      "num estimators: 1000, depth: 5, num features 13, valid accuracy for this fold, 0.7564091919667155\n",
      "num estimators: 1000, depth: 5, num features 13, valid accuracy for this fold, 0.7629007503579499\n",
      "avgfold accuracy: 0.7638422557700514\n",
      "standard deviation: 0.004256423632649556\n",
      "avg train time: 550.3621336460113\n",
      "num estimators: 100, depth: 5, num features 15, valid accuracy for this fold, 0.7718350615897758\n",
      "num estimators: 100, depth: 5, num features 15, valid accuracy for this fold, 0.7647579120694605\n",
      "num estimators: 100, depth: 5, num features 15, valid accuracy for this fold, 0.7650548788877014\n",
      "num estimators: 100, depth: 5, num features 15, valid accuracy for this fold, 0.7634498473233148\n",
      "num estimators: 100, depth: 5, num features 15, valid accuracy for this fold, 0.7649673537851589\n",
      "avgfold accuracy: 0.7660130107310823\n",
      "standard deviation: 0.002968257757823431\n",
      "avg train time: 37.93938188552856\n",
      "num estimators: 1000, depth: 5, num features 15, valid accuracy for this fold, 0.771433832396152\n",
      "num estimators: 1000, depth: 5, num features 15, valid accuracy for this fold, 0.7655909610395237\n",
      "num estimators: 1000, depth: 5, num features 15, valid accuracy for this fold, 0.7629273116956559\n",
      "num estimators: 1000, depth: 5, num features 15, valid accuracy for this fold, 0.7553695144789159\n",
      "num estimators: 1000, depth: 5, num features 15, valid accuracy for this fold, 0.7623512998084994\n",
      "avgfold accuracy: 0.7635345838837493\n",
      "standard deviation: 0.005197386134710537\n",
      "avg train time: 375.84480662345885\n",
      "num estimators: 100, depth: 5, num features 17, valid accuracy for this fold, 0.7730966037309124\n",
      "num estimators: 100, depth: 5, num features 17, valid accuracy for this fold, 0.7612466124661247\n",
      "num estimators: 100, depth: 5, num features 17, valid accuracy for this fold, 0.761451040582808\n",
      "num estimators: 100, depth: 5, num features 17, valid accuracy for this fold, 0.7577228572219301\n",
      "num estimators: 100, depth: 5, num features 17, valid accuracy for this fold, 0.7610219839630544\n",
      "avgfold accuracy: 0.7629078195929659\n",
      "standard deviation: 0.005275098998587138\n",
      "avg train time: 42.842512941360475\n",
      "num estimators: 1000, depth: 5, num features 17, valid accuracy for this fold, 0.7693698318677679\n",
      "num estimators: 1000, depth: 5, num features 17, valid accuracy for this fold, 0.7636527771007224\n",
      "num estimators: 1000, depth: 5, num features 17, valid accuracy for this fold, 0.7605826458105445\n",
      "num estimators: 1000, depth: 5, num features 17, valid accuracy for this fold, 0.7544147086227733\n",
      "num estimators: 1000, depth: 5, num features 17, valid accuracy for this fold, 0.7648787327287958\n",
      "avgfold accuracy: 0.7625797392261208\n",
      "standard deviation: 0.0049638341300241124\n",
      "avg train time: 428.08636498451233\n",
      "num estimators: 100, depth: 5, num features 20, valid accuracy for this fold, 0.7631877124669635\n",
      "num estimators: 100, depth: 5, num features 20, valid accuracy for this fold, 0.7598401427052246\n",
      "num estimators: 100, depth: 5, num features 20, valid accuracy for this fold, 0.7543470743727809\n",
      "num estimators: 100, depth: 5, num features 20, valid accuracy for this fold, 0.7635365639904427\n",
      "num estimators: 100, depth: 5, num features 20, valid accuracy for this fold, 0.7627872757986291\n",
      "avgfold accuracy: 0.7607397538668081\n",
      "standard deviation: 0.0034549282363727594\n",
      "avg train time: 54.71277899742127\n",
      "num estimators: 1000, depth: 5, num features 20, valid accuracy for this fold, 0.7660063240976189\n",
      "num estimators: 1000, depth: 5, num features 20, valid accuracy for this fold, 0.7630524546418016\n",
      "num estimators: 1000, depth: 5, num features 20, valid accuracy for this fold, 0.7576791469643063\n",
      "num estimators: 1000, depth: 5, num features 20, valid accuracy for this fold, 0.7575646454303084\n",
      "num estimators: 1000, depth: 5, num features 20, valid accuracy for this fold, 0.7647546632498876\n",
      "avgfold accuracy: 0.7618114468767846\n",
      "standard deviation: 0.0035471352633377074\n",
      "avg train time: 559.3279873371124\n",
      "num estimators: 100, depth: 6, num features 1, valid accuracy for this fold, 0.7167269980275661\n",
      "num estimators: 100, depth: 6, num features 1, valid accuracy for this fold, 0.6486761309984852\n",
      "num estimators: 100, depth: 6, num features 1, valid accuracy for this fold, 0.6788334790248659\n",
      "num estimators: 100, depth: 6, num features 1, valid accuracy for this fold, 0.7195781326396\n",
      "num estimators: 100, depth: 6, num features 1, valid accuracy for this fold, 0.639964729809747\n",
      "avgfold accuracy: 0.6807558941000529\n",
      "standard deviation: 0.033159651649310784\n",
      "avg train time: 4.693207502365112\n",
      "num estimators: 1000, depth: 6, num features 1, valid accuracy for this fold, 0.7220537455730316\n",
      "num estimators: 1000, depth: 6, num features 1, valid accuracy for this fold, 0.6731761481505554\n",
      "num estimators: 1000, depth: 6, num features 1, valid accuracy for this fold, 0.728361799418594\n",
      "num estimators: 1000, depth: 6, num features 1, valid accuracy for this fold, 0.7210615411581288\n",
      "num estimators: 1000, depth: 6, num features 1, valid accuracy for this fold, 0.6247043819399198\n",
      "avgfold accuracy: 0.693871523248046\n",
      "standard deviation: 0.039838438389601424\n",
      "avg train time: 45.92046809196472\n",
      "num estimators: 100, depth: 6, num features 3, valid accuracy for this fold, 0.7587834480667446\n",
      "num estimators: 100, depth: 6, num features 3, valid accuracy for this fold, 0.7230630347609273\n",
      "num estimators: 100, depth: 6, num features 3, valid accuracy for this fold, 0.7705015712190955\n",
      "num estimators: 100, depth: 6, num features 3, valid accuracy for this fold, 0.7431138663640808\n",
      "num estimators: 100, depth: 6, num features 3, valid accuracy for this fold, 0.7343977431831099\n",
      "avgfold accuracy: 0.7459719327187917\n",
      "standard deviation: 0.016928001079903547\n",
      "avg train time: 9.85195426940918\n",
      "num estimators: 1000, depth: 6, num features 3, valid accuracy for this fold, 0.7585902451081936\n",
      "num estimators: 1000, depth: 6, num features 3, valid accuracy for this fold, 0.7418613426640595\n",
      "num estimators: 1000, depth: 6, num features 3, valid accuracy for this fold, 0.7713217073059726\n",
      "num estimators: 1000, depth: 6, num features 3, valid accuracy for this fold, 0.7642570503417928\n",
      "num estimators: 1000, depth: 6, num features 3, valid accuracy for this fold, 0.7076488388166168\n",
      "avgfold accuracy: 0.748735836847327\n",
      "standard deviation: 0.02273120785325192\n",
      "avg train time: 98.33408579826354\n",
      "num estimators: 100, depth: 6, num features 5, valid accuracy for this fold, 0.7739235173971477\n",
      "num estimators: 100, depth: 6, num features 5, valid accuracy for this fold, 0.7543525635123107\n",
      "num estimators: 100, depth: 6, num features 5, valid accuracy for this fold, 0.7815671962305389\n",
      "num estimators: 100, depth: 6, num features 5, valid accuracy for this fold, 0.7585323665347466\n",
      "num estimators: 100, depth: 6, num features 5, valid accuracy for this fold, 0.7216576989418942\n",
      "avgfold accuracy: 0.7580066685233277\n",
      "standard deviation: 0.020704207182971376\n",
      "avg train time: 15.809323644638061\n",
      "num estimators: 1000, depth: 6, num features 5, valid accuracy for this fold, 0.7677017441680101\n",
      "num estimators: 1000, depth: 6, num features 5, valid accuracy for this fold, 0.7558520155488031\n",
      "num estimators: 1000, depth: 6, num features 5, valid accuracy for this fold, 0.7852466267309697\n",
      "num estimators: 1000, depth: 6, num features 5, valid accuracy for this fold, 0.7697972306017583\n",
      "num estimators: 1000, depth: 6, num features 5, valid accuracy for this fold, 0.73306129804598\n",
      "avgfold accuracy: 0.7623317830191042\n",
      "standard deviation: 0.017368984597381617\n",
      "avg train time: 153.78955903053284\n",
      "num estimators: 100, depth: 6, num features 7, valid accuracy for this fold, 0.7747277896030415\n",
      "num estimators: 100, depth: 6, num features 7, valid accuracy for this fold, 0.7732608252131822\n",
      "num estimators: 100, depth: 6, num features 7, valid accuracy for this fold, 0.7874329151960192\n",
      "num estimators: 100, depth: 6, num features 7, valid accuracy for this fold, 0.7720620116421739\n",
      "num estimators: 100, depth: 6, num features 7, valid accuracy for this fold, 0.7621035569224432\n",
      "avgfold accuracy: 0.773917419715372\n",
      "standard deviation: 0.008084136651378148\n",
      "avg train time: 20.64514741897583\n",
      "num estimators: 1000, depth: 6, num features 7, valid accuracy for this fold, 0.771702715374077\n",
      "num estimators: 1000, depth: 6, num features 7, valid accuracy for this fold, 0.7714403586407617\n",
      "num estimators: 1000, depth: 6, num features 7, valid accuracy for this fold, 0.7774415363682161\n",
      "num estimators: 1000, depth: 6, num features 7, valid accuracy for this fold, 0.7724863698004594\n",
      "num estimators: 1000, depth: 6, num features 7, valid accuracy for this fold, 0.7437274115324215\n",
      "avgfold accuracy: 0.7673596783431871\n",
      "standard deviation: 0.012016029156862243\n",
      "avg train time: 207.83856225013733\n",
      "num estimators: 100, depth: 6, num features 9, valid accuracy for this fold, 0.7743248716817126\n",
      "num estimators: 100, depth: 6, num features 9, valid accuracy for this fold, 0.7738952712645049\n",
      "num estimators: 100, depth: 6, num features 9, valid accuracy for this fold, 0.7863852178284585\n",
      "num estimators: 100, depth: 6, num features 9, valid accuracy for this fold, 0.7707737156246829\n",
      "num estimators: 100, depth: 6, num features 9, valid accuracy for this fold, 0.7433941171461445\n",
      "avgfold accuracy: 0.7697546387091005\n",
      "standard deviation: 0.014216354272871755\n",
      "avg train time: 26.339800119400024\n",
      "num estimators: 1000, depth: 6, num features 9, valid accuracy for this fold, 0.7747492426994425\n",
      "num estimators: 1000, depth: 6, num features 9, valid accuracy for this fold, 0.7722683522638025\n",
      "num estimators: 1000, depth: 6, num features 9, valid accuracy for this fold, 0.7817610156661556\n",
      "num estimators: 1000, depth: 6, num features 9, valid accuracy for this fold, 0.7728549156357531\n",
      "num estimators: 1000, depth: 6, num features 9, valid accuracy for this fold, 0.7526070433441131\n",
      "avgfold accuracy: 0.7708481139218534\n",
      "standard deviation: 0.009727190921423146\n",
      "avg train time: 262.26745595932005\n",
      "num estimators: 100, depth: 6, num features 11, valid accuracy for this fold, 0.7710885188532063\n",
      "num estimators: 100, depth: 6, num features 11, valid accuracy for this fold, 0.7742137581269217\n",
      "num estimators: 100, depth: 6, num features 11, valid accuracy for this fold, 0.778634794486007\n",
      "num estimators: 100, depth: 6, num features 11, valid accuracy for this fold, 0.7711459515309182\n",
      "num estimators: 100, depth: 6, num features 11, valid accuracy for this fold, 0.7537271342821892\n",
      "avgfold accuracy: 0.7697620314558484\n",
      "standard deviation: 0.008477458641628991\n",
      "avg train time: 31.622224521636962\n",
      "num estimators: 1000, depth: 6, num features 11, valid accuracy for this fold, 0.7752297795497327\n",
      "num estimators: 1000, depth: 6, num features 11, valid accuracy for this fold, 0.7715405628406717\n",
      "num estimators: 1000, depth: 6, num features 11, valid accuracy for this fold, 0.7760135239407546\n",
      "num estimators: 1000, depth: 6, num features 11, valid accuracy for this fold, 0.7734471720218821\n",
      "num estimators: 1000, depth: 6, num features 11, valid accuracy for this fold, 0.7571585019773883\n",
      "avgfold accuracy: 0.7706779080660858\n",
      "standard deviation: 0.006933623408777761\n",
      "avg train time: 317.40871572494507\n",
      "num estimators: 100, depth: 6, num features 13, valid accuracy for this fold, 0.7713923647491727\n",
      "num estimators: 100, depth: 6, num features 13, valid accuracy for this fold, 0.7687913568009764\n",
      "num estimators: 100, depth: 6, num features 13, valid accuracy for this fold, 0.7766374865098011\n",
      "num estimators: 100, depth: 6, num features 13, valid accuracy for this fold, 0.7727778854048468\n",
      "num estimators: 100, depth: 6, num features 13, valid accuracy for this fold, 0.7629186726051026\n",
      "avgfold accuracy: 0.77050355321398\n",
      "standard deviation: 0.0045617922123813255\n",
      "avg train time: 37.24420666694641\n",
      "num estimators: 1000, depth: 6, num features 13, valid accuracy for this fold, 0.7744983102715675\n",
      "num estimators: 1000, depth: 6, num features 13, valid accuracy for this fold, 0.7700580823263263\n",
      "num estimators: 1000, depth: 6, num features 13, valid accuracy for this fold, 0.7723284697994792\n",
      "num estimators: 1000, depth: 6, num features 13, valid accuracy for this fold, 0.77006383822729\n",
      "num estimators: 1000, depth: 6, num features 13, valid accuracy for this fold, 0.7507743202913504\n",
      "avgfold accuracy: 0.7675446041832027\n",
      "standard deviation: 0.008545711590688322\n",
      "avg train time: 372.9334678173065\n",
      "num estimators: 100, depth: 6, num features 15, valid accuracy for this fold, 0.7717242935614193\n",
      "num estimators: 100, depth: 6, num features 15, valid accuracy for this fold, 0.7713591119921859\n",
      "num estimators: 100, depth: 6, num features 15, valid accuracy for this fold, 0.7703745815618864\n",
      "num estimators: 100, depth: 6, num features 15, valid accuracy for this fold, 0.7683027518704048\n",
      "num estimators: 100, depth: 6, num features 15, valid accuracy for this fold, 0.7444787596616755\n",
      "avgfold accuracy: 0.7652478997295145\n",
      "standard deviation: 0.010452394555851275\n",
      "avg train time: 42.784036254882814\n",
      "num estimators: 1000, depth: 6, num features 15, valid accuracy for this fold, 0.7783209017956054\n",
      "num estimators: 1000, depth: 6, num features 15, valid accuracy for this fold, 0.7667477327671345\n",
      "num estimators: 1000, depth: 6, num features 15, valid accuracy for this fold, 0.7728277967935308\n",
      "num estimators: 1000, depth: 6, num features 15, valid accuracy for this fold, 0.7690878144632332\n",
      "num estimators: 1000, depth: 6, num features 15, valid accuracy for this fold, 0.7537377292017768\n",
      "avgfold accuracy: 0.7681443950042561\n",
      "standard deviation: 0.008196197508828877\n",
      "avg train time: 427.75427508354187\n",
      "num estimators: 100, depth: 6, num features 17, valid accuracy for this fold, 0.76830461995871\n",
      "num estimators: 100, depth: 6, num features 17, valid accuracy for this fold, 0.7669069761983429\n",
      "num estimators: 100, depth: 6, num features 17, valid accuracy for this fold, 0.75980265990365\n",
      "num estimators: 100, depth: 6, num features 17, valid accuracy for this fold, 0.7716012140333398\n",
      "num estimators: 100, depth: 6, num features 17, valid accuracy for this fold, 0.7509267089011187\n",
      "avgfold accuracy: 0.7635084357990323\n",
      "standard deviation: 0.007376708556117025\n",
      "avg train time: 48.36997761726379\n",
      "num estimators: 1000, depth: 6, num features 17, valid accuracy for this fold, 0.7765706293475356\n",
      "num estimators: 1000, depth: 6, num features 17, valid accuracy for this fold, 0.7648953091796075\n",
      "num estimators: 1000, depth: 6, num features 17, valid accuracy for this fold, 0.7547772175062109\n",
      "num estimators: 1000, depth: 6, num features 17, valid accuracy for this fold, 0.768113635734647\n",
      "num estimators: 1000, depth: 6, num features 17, valid accuracy for this fold, 0.7534718660326878\n",
      "avgfold accuracy: 0.7635657315601376\n",
      "standard deviation: 0.008610489835656187\n",
      "avg train time: 485.05170464515686\n",
      "num estimators: 100, depth: 6, num features 20, valid accuracy for this fold, 0.7637489955197428\n",
      "num estimators: 100, depth: 6, num features 20, valid accuracy for this fold, 0.7601907671308559\n",
      "num estimators: 100, depth: 6, num features 20, valid accuracy for this fold, 0.758804005915547\n",
      "num estimators: 100, depth: 6, num features 20, valid accuracy for this fold, 0.7677838356442402\n",
      "num estimators: 100, depth: 6, num features 20, valid accuracy for this fold, 0.7690830344641842\n",
      "avgfold accuracy: 0.763922127734914\n",
      "standard deviation: 0.0040421762436714135\n",
      "avg train time: 57.03758177757263\n",
      "num estimators: 1000, depth: 6, num features 20, valid accuracy for this fold, 0.7712091690659109\n",
      "num estimators: 1000, depth: 6, num features 20, valid accuracy for this fold, 0.7607910895897767\n",
      "num estimators: 1000, depth: 6, num features 20, valid accuracy for this fold, 0.7596072710798907\n",
      "num estimators: 1000, depth: 6, num features 20, valid accuracy for this fold, 0.768495558077104\n",
      "num estimators: 1000, depth: 6, num features 20, valid accuracy for this fold, 0.7517703417505184\n",
      "avgfold accuracy: 0.7623746859126401\n",
      "standard deviation: 0.00690118317273447\n",
      "avg train time: 567.9828123569489\n",
      "num estimators: 100, depth: 7, num features 1, valid accuracy for this fold, 0.6924738660005825\n",
      "num estimators: 100, depth: 7, num features 1, valid accuracy for this fold, 0.6174992642664601\n",
      "num estimators: 100, depth: 7, num features 1, valid accuracy for this fold, 0.6916052905120547\n",
      "num estimators: 100, depth: 7, num features 1, valid accuracy for this fold, 0.6801967730329616\n",
      "num estimators: 100, depth: 7, num features 1, valid accuracy for this fold, 0.6252646254448381\n",
      "avgfold accuracy: 0.6614079638513795\n",
      "standard deviation: 0.03305844211291801\n",
      "avg train time: 5.072019147872925\n",
      "num estimators: 1000, depth: 7, num features 1, valid accuracy for this fold, 0.6976371822314823\n",
      "num estimators: 1000, depth: 7, num features 1, valid accuracy for this fold, 0.6170287558971526\n",
      "num estimators: 1000, depth: 7, num features 1, valid accuracy for this fold, 0.6896731121437685\n",
      "num estimators: 1000, depth: 7, num features 1, valid accuracy for this fold, 0.6890870764490447\n",
      "num estimators: 1000, depth: 7, num features 1, valid accuracy for this fold, 0.6144601838961183\n",
      "avgfold accuracy: 0.6615772621235132\n",
      "standard deviation: 0.037552824070808705\n",
      "avg train time: 50.50661039352417\n",
      "num estimators: 100, depth: 7, num features 3, valid accuracy for this fold, 0.7347466608224179\n",
      "num estimators: 100, depth: 7, num features 3, valid accuracy for this fold, 0.7048478972464608\n",
      "num estimators: 100, depth: 7, num features 3, valid accuracy for this fold, 0.7557685476829814\n",
      "num estimators: 100, depth: 7, num features 3, valid accuracy for this fold, 0.7382738770652866\n",
      "num estimators: 100, depth: 7, num features 3, valid accuracy for this fold, 0.7051073453488303\n",
      "avgfold accuracy: 0.7277488656331954\n",
      "standard deviation: 0.01990949712657046\n",
      "avg train time: 10.993922090530395\n",
      "num estimators: 1000, depth: 7, num features 3, valid accuracy for this fold, 0.7401098098317477\n",
      "num estimators: 1000, depth: 7, num features 3, valid accuracy for this fold, 0.702860784770406\n",
      "num estimators: 1000, depth: 7, num features 3, valid accuracy for this fold, 0.7494565993556093\n",
      "num estimators: 1000, depth: 7, num features 3, valid accuracy for this fold, 0.7472665799499996\n",
      "num estimators: 1000, depth: 7, num features 3, valid accuracy for this fold, 0.6979930053727135\n",
      "avgfold accuracy: 0.7275373558560952\n",
      "standard deviation: 0.022403410115160394\n",
      "avg train time: 110.83083858489991\n",
      "num estimators: 100, depth: 7, num features 5, valid accuracy for this fold, 0.7553886050158465\n",
      "num estimators: 100, depth: 7, num features 5, valid accuracy for this fold, 0.7392150490458935\n",
      "num estimators: 100, depth: 7, num features 5, valid accuracy for this fold, 0.7748491687212573\n",
      "num estimators: 100, depth: 7, num features 5, valid accuracy for this fold, 0.7628239190398436\n",
      "num estimators: 100, depth: 7, num features 5, valid accuracy for this fold, 0.7403813973015632\n",
      "avgfold accuracy: 0.7545316278248808\n",
      "standard deviation: 0.013543452890097635\n",
      "avg train time: 17.215090608596803\n",
      "num estimators: 1000, depth: 7, num features 5, valid accuracy for this fold, 0.7546769001063773\n",
      "num estimators: 1000, depth: 7, num features 5, valid accuracy for this fold, 0.7402147439195911\n",
      "num estimators: 1000, depth: 7, num features 5, valid accuracy for this fold, 0.7779697662605614\n",
      "num estimators: 1000, depth: 7, num features 5, valid accuracy for this fold, 0.7642243009621861\n",
      "num estimators: 1000, depth: 7, num features 5, valid accuracy for this fold, 0.7309523149404209\n",
      "avgfold accuracy: 0.7536076052378273\n",
      "standard deviation: 0.01673322098142741\n",
      "avg train time: 171.8055206298828\n",
      "num estimators: 100, depth: 7, num features 7, valid accuracy for this fold, 0.768571939299871\n",
      "num estimators: 100, depth: 7, num features 7, valid accuracy for this fold, 0.7498612487790435\n",
      "num estimators: 100, depth: 7, num features 7, valid accuracy for this fold, 0.7906740940576165\n",
      "num estimators: 100, depth: 7, num features 7, valid accuracy for this fold, 0.7713286100425281\n",
      "num estimators: 100, depth: 7, num features 7, valid accuracy for this fold, 0.7382473626571662\n",
      "avgfold accuracy: 0.763736650967245\n",
      "standard deviation: 0.01815970429575367\n",
      "avg train time: 23.310250902175902\n",
      "num estimators: 1000, depth: 7, num features 7, valid accuracy for this fold, 0.766341130001511\n",
      "num estimators: 1000, depth: 7, num features 7, valid accuracy for this fold, 0.7465680512901065\n",
      "num estimators: 1000, depth: 7, num features 7, valid accuracy for this fold, 0.7845141148153902\n",
      "num estimators: 1000, depth: 7, num features 7, valid accuracy for this fold, 0.7673299569184218\n",
      "num estimators: 1000, depth: 7, num features 7, valid accuracy for this fold, 0.7370598405019021\n",
      "avgfold accuracy: 0.7603626187054664\n",
      "standard deviation: 0.01673852278798657\n",
      "avg train time: 233.06930141448976\n",
      "num estimators: 100, depth: 7, num features 9, valid accuracy for this fold, 0.7643828313183685\n",
      "num estimators: 100, depth: 7, num features 9, valid accuracy for this fold, 0.7577410904022431\n",
      "num estimators: 100, depth: 7, num features 9, valid accuracy for this fold, 0.7841233371678717\n",
      "num estimators: 100, depth: 7, num features 9, valid accuracy for this fold, 0.7743572357678576\n",
      "num estimators: 100, depth: 7, num features 9, valid accuracy for this fold, 0.7315298865848514\n",
      "avgfold accuracy: 0.7624268762482385\n",
      "standard deviation: 0.01785203749825149\n",
      "avg train time: 29.475261783599855\n",
      "num estimators: 1000, depth: 7, num features 9, valid accuracy for this fold, 0.7652579675424034\n",
      "num estimators: 1000, depth: 7, num features 9, valid accuracy for this fold, 0.7581552677618715\n",
      "num estimators: 1000, depth: 7, num features 9, valid accuracy for this fold, 0.7777679691018862\n",
      "num estimators: 1000, depth: 7, num features 9, valid accuracy for this fold, 0.7714001051670218\n",
      "num estimators: 1000, depth: 7, num features 9, valid accuracy for this fold, 0.7360956038014967\n",
      "avgfold accuracy: 0.7617353826749358\n",
      "standard deviation: 0.014374107109693987\n",
      "avg train time: 295.66606040000914\n",
      "num estimators: 100, depth: 7, num features 11, valid accuracy for this fold, 0.7749440092947573\n",
      "num estimators: 100, depth: 7, num features 11, valid accuracy for this fold, 0.7594340900104537\n",
      "num estimators: 100, depth: 7, num features 11, valid accuracy for this fold, 0.7741263346992242\n",
      "num estimators: 100, depth: 7, num features 11, valid accuracy for this fold, 0.7728198599618078\n",
      "num estimators: 100, depth: 7, num features 11, valid accuracy for this fold, 0.7320438887117569\n",
      "avgfold accuracy: 0.7626736365355999\n",
      "standard deviation: 0.016330023307583427\n",
      "avg train time: 35.77167420387268\n",
      "num estimators: 1000, depth: 7, num features 11, valid accuracy for this fold, 0.7653257042870166\n",
      "num estimators: 1000, depth: 7, num features 11, valid accuracy for this fold, 0.7530022442129818\n",
      "num estimators: 1000, depth: 7, num features 11, valid accuracy for this fold, 0.7760311795573593\n",
      "num estimators: 1000, depth: 7, num features 11, valid accuracy for this fold, 0.7768281995221358\n",
      "num estimators: 1000, depth: 7, num features 11, valid accuracy for this fold, 0.7432525214918438\n",
      "avgfold accuracy: 0.7628879698142674\n",
      "standard deviation: 0.013086595565731775\n",
      "avg train time: 357.70074529647826\n",
      "num estimators: 100, depth: 7, num features 13, valid accuracy for this fold, 0.7746302812144429\n",
      "num estimators: 100, depth: 7, num features 13, valid accuracy for this fold, 0.7659444742348822\n",
      "num estimators: 100, depth: 7, num features 13, valid accuracy for this fold, 0.7776915922122776\n",
      "num estimators: 100, depth: 7, num features 13, valid accuracy for this fold, 0.7741238387808005\n",
      "num estimators: 100, depth: 7, num features 13, valid accuracy for this fold, 0.7432453922001588\n",
      "avgfold accuracy: 0.7671271157285126\n",
      "standard deviation: 0.012558623720319653\n",
      "avg train time: 41.787343692779544\n",
      "num estimators: 1000, depth: 7, num features 13, valid accuracy for this fold, 0.7668398675837333\n",
      "num estimators: 1000, depth: 7, num features 13, valid accuracy for this fold, 0.7658464366122675\n",
      "num estimators: 1000, depth: 7, num features 13, valid accuracy for this fold, 0.763738554583058\n",
      "num estimators: 1000, depth: 7, num features 13, valid accuracy for this fold, 0.7771040323250215\n",
      "num estimators: 1000, depth: 7, num features 13, valid accuracy for this fold, 0.749044674914201\n",
      "avgfold accuracy: 0.7645147132036562\n",
      "standard deviation: 0.009006611937341882\n",
      "avg train time: 420.38674612045287\n",
      "num estimators: 100, depth: 7, num features 15, valid accuracy for this fold, 0.7673566808069466\n",
      "num estimators: 100, depth: 7, num features 15, valid accuracy for this fold, 0.7688969774441249\n",
      "num estimators: 100, depth: 7, num features 15, valid accuracy for this fold, 0.7585120997210151\n",
      "num estimators: 100, depth: 7, num features 15, valid accuracy for this fold, 0.7707386599507375\n",
      "num estimators: 100, depth: 7, num features 15, valid accuracy for this fold, 0.752114429092263\n",
      "avgfold accuracy: 0.7635237694030174\n",
      "standard deviation: 0.007083884519486195\n",
      "avg train time: 48.16272912025452\n",
      "num estimators: 1000, depth: 7, num features 15, valid accuracy for this fold, 0.7675895375938306\n",
      "num estimators: 1000, depth: 7, num features 15, valid accuracy for this fold, 0.7638493939902757\n",
      "num estimators: 1000, depth: 7, num features 15, valid accuracy for this fold, 0.759494667480656\n",
      "num estimators: 1000, depth: 7, num features 15, valid accuracy for this fold, 0.7738904417937436\n",
      "num estimators: 1000, depth: 7, num features 15, valid accuracy for this fold, 0.7501718951439622\n",
      "avgfold accuracy: 0.7629991872004936\n",
      "standard deviation: 0.007965358433922149\n",
      "avg train time: 480.31523818969725\n",
      "num estimators: 100, depth: 7, num features 17, valid accuracy for this fold, 0.7663528260045053\n",
      "num estimators: 100, depth: 7, num features 17, valid accuracy for this fold, 0.763030969416956\n",
      "num estimators: 100, depth: 7, num features 17, valid accuracy for this fold, 0.7637007584852893\n",
      "num estimators: 100, depth: 7, num features 17, valid accuracy for this fold, 0.7655227446747663\n",
      "num estimators: 100, depth: 7, num features 17, valid accuracy for this fold, 0.7512565376594932\n",
      "avgfold accuracy: 0.7619727672482021\n",
      "standard deviation: 0.005490547815675769\n",
      "avg train time: 54.38726491928101\n",
      "num estimators: 1000, depth: 7, num features 17, valid accuracy for this fold, 0.7682285646665126\n",
      "num estimators: 1000, depth: 7, num features 17, valid accuracy for this fold, 0.758076548786807\n",
      "num estimators: 1000, depth: 7, num features 17, valid accuracy for this fold, 0.7566997179809507\n",
      "num estimators: 1000, depth: 7, num features 17, valid accuracy for this fold, 0.7683756307715015\n",
      "num estimators: 1000, depth: 7, num features 17, valid accuracy for this fold, 0.7494629266930583\n",
      "avgfold accuracy: 0.7601686777797659\n",
      "standard deviation: 0.007257267339367156\n",
      "avg train time: 545.4496367931366\n",
      "num estimators: 100, depth: 7, num features 20, valid accuracy for this fold, 0.768340896331633\n",
      "num estimators: 100, depth: 7, num features 20, valid accuracy for this fold, 0.7546853136752554\n",
      "num estimators: 100, depth: 7, num features 20, valid accuracy for this fold, 0.7527268118978454\n",
      "num estimators: 100, depth: 7, num features 20, valid accuracy for this fold, 0.7667533833337945\n",
      "num estimators: 100, depth: 7, num features 20, valid accuracy for this fold, 0.743280840622704\n",
      "avgfold accuracy: 0.7571574491722466\n",
      "standard deviation: 0.009332242047970955\n",
      "avg train time: 63.96162619590759\n",
      "num estimators: 1000, depth: 7, num features 20, valid accuracy for this fold, 0.7644803397069669\n",
      "num estimators: 1000, depth: 7, num features 20, valid accuracy for this fold, 0.7587723811948313\n",
      "num estimators: 1000, depth: 7, num features 20, valid accuracy for this fold, 0.7561569712482861\n",
      "num estimators: 1000, depth: 7, num features 20, valid accuracy for this fold, 0.7680573621527873\n",
      "num estimators: 1000, depth: 7, num features 20, valid accuracy for this fold, 0.7531778817686188\n",
      "avgfold accuracy: 0.7601289872142981\n",
      "standard deviation: 0.00543559666134234\n",
      "avg train time: 643.5008780479432\n",
      "num estimators: 100, depth: 8, num features 1, valid accuracy for this fold, 0.6316942417137259\n",
      "num estimators: 100, depth: 8, num features 1, valid accuracy for this fold, 0.5646302645571426\n",
      "num estimators: 100, depth: 8, num features 1, valid accuracy for this fold, 0.655472744174562\n",
      "num estimators: 100, depth: 8, num features 1, valid accuracy for this fold, 0.6349472781114217\n",
      "num estimators: 100, depth: 8, num features 1, valid accuracy for this fold, 0.6048292039551726\n",
      "avgfold accuracy: 0.6183147465024049\n",
      "standard deviation: 0.03130575745885458\n",
      "avg train time: 5.49551100730896\n",
      "num estimators: 1000, depth: 8, num features 1, valid accuracy for this fold, 0.6483401557932618\n",
      "num estimators: 1000, depth: 8, num features 1, valid accuracy for this fold, 0.5872386791822615\n",
      "num estimators: 1000, depth: 8, num features 1, valid accuracy for this fold, 0.6648940427595493\n",
      "num estimators: 1000, depth: 8, num features 1, valid accuracy for this fold, 0.6202391165970166\n",
      "num estimators: 1000, depth: 8, num features 1, valid accuracy for this fold, 0.5960771072502916\n",
      "avgfold accuracy: 0.6233578203164761\n",
      "standard deviation: 0.02969151675235773\n",
      "avg train time: 55.49204258918762\n",
      "num estimators: 100, depth: 8, num features 3, valid accuracy for this fold, 0.7154785904352465\n",
      "num estimators: 100, depth: 8, num features 3, valid accuracy for this fold, 0.661593806477705\n",
      "num estimators: 100, depth: 8, num features 3, valid accuracy for this fold, 0.7258281530446391\n",
      "num estimators: 100, depth: 8, num features 3, valid accuracy for this fold, 0.7089728687534018\n",
      "num estimators: 100, depth: 8, num features 3, valid accuracy for this fold, 0.6655193788010512\n",
      "avgfold accuracy: 0.6954785595024087\n",
      "standard deviation: 0.026641788391533548\n",
      "avg train time: 12.236269235610962\n",
      "num estimators: 1000, depth: 8, num features 3, valid accuracy for this fold, 0.7267480958656944\n",
      "num estimators: 1000, depth: 8, num features 3, valid accuracy for this fold, 0.6537823925874169\n",
      "num estimators: 1000, depth: 8, num features 3, valid accuracy for this fold, 0.7097258383540676\n",
      "num estimators: 1000, depth: 8, num features 3, valid accuracy for this fold, 0.7028524248378676\n",
      "num estimators: 1000, depth: 8, num features 3, valid accuracy for this fold, 0.6834595482009431\n",
      "avgfold accuracy: 0.6953136599691978\n",
      "standard deviation: 0.02497211948483591\n",
      "avg train time: 121.95696001052856\n",
      "num estimators: 100, depth: 8, num features 5, valid accuracy for this fold, 0.7463926899856195\n",
      "num estimators: 100, depth: 8, num features 5, valid accuracy for this fold, 0.7095549669687236\n",
      "num estimators: 100, depth: 8, num features 5, valid accuracy for this fold, 0.749152661185317\n",
      "num estimators: 100, depth: 8, num features 5, valid accuracy for this fold, 0.7450520761261635\n",
      "num estimators: 100, depth: 8, num features 5, valid accuracy for this fold, 0.7237175691491784\n",
      "avgfold accuracy: 0.7347739926830004\n",
      "standard deviation: 0.01552818723168054\n",
      "avg train time: 18.982699966430665\n",
      "num estimators: 1000, depth: 8, num features 5, valid accuracy for this fold, 0.7445911927971636\n",
      "num estimators: 1000, depth: 8, num features 5, valid accuracy for this fold, 0.7142398292737091\n",
      "num estimators: 1000, depth: 8, num features 5, valid accuracy for this fold, 0.7598877992103885\n",
      "num estimators: 1000, depth: 8, num features 5, valid accuracy for this fold, 0.7437038164558714\n",
      "num estimators: 1000, depth: 8, num features 5, valid accuracy for this fold, 0.7242598904069439\n",
      "avgfold accuracy: 0.7373365056288153\n",
      "standard deviation: 0.01616284813759411\n",
      "avg train time: 190.84012107849122\n",
      "num estimators: 100, depth: 8, num features 7, valid accuracy for this fold, 0.7501558633126283\n",
      "num estimators: 100, depth: 8, num features 7, valid accuracy for this fold, 0.7192700801091955\n",
      "num estimators: 100, depth: 8, num features 7, valid accuracy for this fold, 0.7719191210589393\n",
      "num estimators: 100, depth: 8, num features 7, valid accuracy for this fold, 0.7598492606020351\n",
      "num estimators: 100, depth: 8, num features 7, valid accuracy for this fold, 0.7458121352426632\n",
      "avgfold accuracy: 0.7494012920650922\n",
      "standard deviation: 0.017535986014706226\n",
      "avg train time: 26.00366578102112\n",
      "num estimators: 1000, depth: 8, num features 7, valid accuracy for this fold, 0.752814295993187\n",
      "num estimators: 1000, depth: 8, num features 7, valid accuracy for this fold, 0.7448409280894941\n",
      "num estimators: 1000, depth: 8, num features 7, valid accuracy for this fold, 0.7783774148306133\n",
      "num estimators: 1000, depth: 8, num features 7, valid accuracy for this fold, 0.7513561010710431\n",
      "num estimators: 1000, depth: 8, num features 7, valid accuracy for this fold, 0.7345857782513036\n",
      "avgfold accuracy: 0.7523949036471282\n",
      "standard deviation: 0.014495332919401896\n",
      "avg train time: 260.4145434379578\n",
      "num estimators: 100, depth: 8, num features 9, valid accuracy for this fold, 0.7604858732298381\n",
      "num estimators: 100, depth: 8, num features 9, valid accuracy for this fold, 0.7389749200623252\n",
      "num estimators: 100, depth: 8, num features 9, valid accuracy for this fold, 0.7684889617084987\n",
      "num estimators: 100, depth: 8, num features 9, valid accuracy for this fold, 0.7619170841059419\n",
      "num estimators: 100, depth: 8, num features 9, valid accuracy for this fold, 0.7302610707007895\n",
      "avgfold accuracy: 0.7520255819614787\n",
      "standard deviation: 0.014727355106309275\n",
      "avg train time: 32.682946348190306\n",
      "num estimators: 1000, depth: 8, num features 9, valid accuracy for this fold, 0.7603537771960216\n",
      "num estimators: 1000, depth: 8, num features 9, valid accuracy for this fold, 0.743461359996678\n",
      "num estimators: 1000, depth: 8, num features 9, valid accuracy for this fold, 0.7712404914695907\n",
      "num estimators: 1000, depth: 8, num features 9, valid accuracy for this fold, 0.7637939464386203\n",
      "num estimators: 1000, depth: 8, num features 9, valid accuracy for this fold, 0.7412392887343329\n",
      "avgfold accuracy: 0.7560177727670487\n",
      "standard deviation: 0.011722355292691014\n",
      "avg train time: 361.79108934402467\n",
      "num estimators: 100, depth: 8, num features 11, valid accuracy for this fold, 0.7578769765619606\n",
      "num estimators: 100, depth: 8, num features 11, valid accuracy for this fold, 0.7457963886767449\n",
      "num estimators: 100, depth: 8, num features 11, valid accuracy for this fold, 0.7654769135157277\n",
      "num estimators: 100, depth: 8, num features 11, valid accuracy for this fold, 0.7642916447568704\n",
      "num estimators: 100, depth: 8, num features 11, valid accuracy for this fold, 0.7332138846916284\n",
      "avgfold accuracy: 0.7533311616405864\n",
      "standard deviation: 0.012245762920389614\n",
      "avg train time: 43.619806432724\n",
      "num estimators: 1000, depth: 8, num features 11, valid accuracy for this fold, 0.7646106219221374\n",
      "num estimators: 1000, depth: 8, num features 11, valid accuracy for this fold, 0.7445908689599887\n",
      "num estimators: 1000, depth: 8, num features 11, valid accuracy for this fold, 0.7604619337059061\n",
      "num estimators: 1000, depth: 8, num features 11, valid accuracy for this fold, 0.7721597985221267\n",
      "num estimators: 1000, depth: 8, num features 11, valid accuracy for this fold, 0.7422778879077312\n",
      "avgfold accuracy: 0.7568202222035779\n",
      "standard deviation: 0.011578355320185574\n",
      "avg train time: 401.09492683410645\n",
      "num estimators: 100, depth: 8, num features 13, valid accuracy for this fold, 0.7560986211976106\n",
      "num estimators: 100, depth: 8, num features 13, valid accuracy for this fold, 0.7341894924612138\n",
      "num estimators: 100, depth: 8, num features 13, valid accuracy for this fold, 0.7586165948148462\n",
      "num estimators: 100, depth: 8, num features 13, valid accuracy for this fold, 0.7757110305445623\n",
      "num estimators: 100, depth: 8, num features 13, valid accuracy for this fold, 0.7486266211712238\n",
      "avgfold accuracy: 0.7546484720378913\n",
      "standard deviation: 0.013537450535345653\n",
      "avg train time: 47.26926474571228\n",
      "num estimators: 1000, depth: 8, num features 13, valid accuracy for this fold, 0.7659266411681293\n",
      "num estimators: 1000, depth: 8, num features 13, valid accuracy for this fold, 0.7430569322348787\n",
      "num estimators: 1000, depth: 8, num features 13, valid accuracy for this fold, 0.761433254183858\n",
      "num estimators: 1000, depth: 8, num features 13, valid accuracy for this fold, 0.7696039631361913\n",
      "num estimators: 1000, depth: 8, num features 13, valid accuracy for this fold, 0.7392010044379841\n",
      "avgfold accuracy: 0.7558443590322083\n",
      "standard deviation: 0.012350987032856421\n",
      "avg train time: 487.4724932193756\n",
      "num estimators: 100, depth: 8, num features 15, valid accuracy for this fold, 0.7562588001477074\n",
      "num estimators: 100, depth: 8, num features 15, valid accuracy for this fold, 0.7442942284186332\n",
      "num estimators: 100, depth: 8, num features 15, valid accuracy for this fold, 0.7533250103448834\n",
      "num estimators: 100, depth: 8, num features 15, valid accuracy for this fold, 0.7642897997213997\n",
      "num estimators: 100, depth: 8, num features 15, valid accuracy for this fold, 0.7438659376305798\n",
      "avgfold accuracy: 0.7524067552526407\n",
      "standard deviation: 0.007689562346718287\n",
      "avg train time: 57.432592439651486\n",
      "num estimators: 1000, depth: 8, num features 15, valid accuracy for this fold, 0.7668298603084442\n",
      "num estimators: 1000, depth: 8, num features 15, valid accuracy for this fold, 0.7394770243505233\n",
      "num estimators: 1000, depth: 8, num features 15, valid accuracy for this fold, 0.7573357125257576\n",
      "num estimators: 1000, depth: 8, num features 15, valid accuracy for this fold, 0.7683308886613345\n",
      "num estimators: 1000, depth: 8, num features 15, valid accuracy for this fold, 0.7409308478510137\n",
      "avgfold accuracy: 0.7545808667394146\n",
      "standard deviation: 0.012338064614647704\n",
      "avg train time: 539.3919902324676\n",
      "num estimators: 100, depth: 8, num features 17, valid accuracy for this fold, 0.7645842277335624\n",
      "num estimators: 100, depth: 8, num features 17, valid accuracy for this fold, 0.7414764140979184\n",
      "num estimators: 100, depth: 8, num features 17, valid accuracy for this fold, 0.7592405573838927\n",
      "num estimators: 100, depth: 8, num features 17, valid accuracy for this fold, 0.7665370529248425\n",
      "num estimators: 100, depth: 8, num features 17, valid accuracy for this fold, 0.7412853320764656\n",
      "avgfold accuracy: 0.7546247168433362\n",
      "standard deviation: 0.011074462903734122\n",
      "avg train time: 60.97390160560608\n",
      "num estimators: 1000, depth: 8, num features 17, valid accuracy for this fold, 0.7622940002381732\n",
      "num estimators: 1000, depth: 8, num features 17, valid accuracy for this fold, 0.7420350299439037\n",
      "num estimators: 1000, depth: 8, num features 17, valid accuracy for this fold, 0.7521526774023277\n",
      "num estimators: 1000, depth: 8, num features 17, valid accuracy for this fold, 0.7653294772091994\n",
      "num estimators: 1000, depth: 8, num features 17, valid accuracy for this fold, 0.737481755944542\n",
      "avgfold accuracy: 0.7518585881476292\n",
      "standard deviation: 0.010896281623343258\n",
      "avg train time: 606.9601290702819\n",
      "num estimators: 100, depth: 8, num features 20, valid accuracy for this fold, 0.7613875287333891\n",
      "num estimators: 100, depth: 8, num features 20, valid accuracy for this fold, 0.7451075976449305\n",
      "num estimators: 100, depth: 8, num features 20, valid accuracy for this fold, 0.7535855287765625\n",
      "num estimators: 100, depth: 8, num features 20, valid accuracy for this fold, 0.7687746196920635\n",
      "num estimators: 100, depth: 8, num features 20, valid accuracy for this fold, 0.7411612625975574\n",
      "avgfold accuracy: 0.7540033074889005\n",
      "standard deviation: 0.010168035181515917\n",
      "avg train time: 70.48648915290832\n",
      "num estimators: 1000, depth: 8, num features 20, valid accuracy for this fold, 0.7634680412660004\n",
      "num estimators: 1000, depth: 8, num features 20, valid accuracy for this fold, 0.7349414753308092\n",
      "num estimators: 1000, depth: 8, num features 20, valid accuracy for this fold, 0.7545568492545145\n",
      "num estimators: 1000, depth: 8, num features 20, valid accuracy for this fold, 0.7639078773789426\n",
      "num estimators: 1000, depth: 8, num features 20, valid accuracy for this fold, 0.7383325180856267\n",
      "avgfold accuracy: 0.7510413522631787\n",
      "standard deviation: 0.012272320812409372\n",
      "avg train time: 700.2208304405212\n",
      "num estimators: 100, depth: 9, num features 1, valid accuracy for this fold, 0.6017049394910099\n",
      "num estimators: 100, depth: 9, num features 1, valid accuracy for this fold, 0.5371335099093829\n",
      "num estimators: 100, depth: 9, num features 1, valid accuracy for this fold, 0.6175977846516978\n",
      "num estimators: 100, depth: 9, num features 1, valid accuracy for this fold, 0.5561864039336156\n",
      "num estimators: 100, depth: 9, num features 1, valid accuracy for this fold, 0.5873429327925633\n",
      "avgfold accuracy: 0.5799931141556539\n",
      "standard deviation: 0.02947283347168227\n",
      "avg train time: 5.8426800727844235\n",
      "num estimators: 1000, depth: 9, num features 1, valid accuracy for this fold, 0.5881253171055357\n",
      "num estimators: 1000, depth: 9, num features 1, valid accuracy for this fold, 0.5354257053563207\n",
      "num estimators: 1000, depth: 9, num features 1, valid accuracy for this fold, 0.5972732927280308\n",
      "num estimators: 1000, depth: 9, num features 1, valid accuracy for this fold, 0.5674245149863006\n",
      "num estimators: 1000, depth: 9, num features 1, valid accuracy for this fold, 0.5742272144867206\n",
      "avgfold accuracy: 0.5724952089325817\n",
      "standard deviation: 0.02126552888502604\n",
      "avg train time: 59.4217643737793\n",
      "num estimators: 100, depth: 9, num features 3, valid accuracy for this fold, 0.6863642993456243\n",
      "num estimators: 100, depth: 9, num features 3, valid accuracy for this fold, 0.65091817740296\n",
      "num estimators: 100, depth: 9, num features 3, valid accuracy for this fold, 0.7011576330069529\n",
      "num estimators: 100, depth: 9, num features 3, valid accuracy for this fold, 0.6726256699785054\n",
      "num estimators: 100, depth: 9, num features 3, valid accuracy for this fold, 0.6724743989115948\n",
      "avgfold accuracy: 0.6767080357291274\n",
      "standard deviation: 0.016677724169607427\n",
      "avg train time: 13.153224849700928\n",
      "num estimators: 1000, depth: 9, num features 3, valid accuracy for this fold, 0.6992487288258564\n",
      "num estimators: 1000, depth: 9, num features 3, valid accuracy for this fold, 0.6135665653791781\n",
      "num estimators: 1000, depth: 9, num features 3, valid accuracy for this fold, 0.6854678058332064\n",
      "num estimators: 1000, depth: 9, num features 3, valid accuracy for this fold, 0.6713641269753411\n",
      "num estimators: 1000, depth: 9, num features 3, valid accuracy for this fold, 0.6552077495400617\n",
      "avgfold accuracy: 0.6649709953107287\n",
      "standard deviation: 0.029575972591195304\n",
      "avg train time: 131.4657784461975\n",
      "num estimators: 100, depth: 9, num features 5, valid accuracy for this fold, 0.7234390652004007\n",
      "num estimators: 100, depth: 9, num features 5, valid accuracy for this fold, 0.663463382135487\n",
      "num estimators: 100, depth: 9, num features 5, valid accuracy for this fold, 0.7139970589666207\n",
      "num estimators: 100, depth: 9, num features 5, valid accuracy for this fold, 0.7130296404948385\n",
      "num estimators: 100, depth: 9, num features 5, valid accuracy for this fold, 0.6870433045059103\n",
      "avgfold accuracy: 0.7001944902606515\n",
      "standard deviation: 0.021989498581845852\n",
      "avg train time: 20.321988439559938\n",
      "num estimators: 1000, depth: 9, num features 5, valid accuracy for this fold, 0.7331135360407016\n",
      "num estimators: 1000, depth: 9, num features 5, valid accuracy for this fold, 0.6657029008664503\n",
      "num estimators: 1000, depth: 9, num features 5, valid accuracy for this fold, 0.7268541405428932\n",
      "num estimators: 1000, depth: 9, num features 5, valid accuracy for this fold, 0.701569663926789\n",
      "num estimators: 1000, depth: 9, num features 5, valid accuracy for this fold, 0.6801593792763372\n",
      "avgfold accuracy: 0.7014799241306342\n",
      "standard deviation: 0.02599655877524551\n",
      "avg train time: 202.99533185958862\n",
      "num estimators: 100, depth: 9, num features 7, valid accuracy for this fold, 0.7438910587997474\n",
      "num estimators: 100, depth: 9, num features 7, valid accuracy for this fold, 0.6909281797681401\n",
      "num estimators: 100, depth: 9, num features 7, valid accuracy for this fold, 0.7542769750357428\n",
      "num estimators: 100, depth: 9, num features 7, valid accuracy for this fold, 0.7320856280962001\n",
      "num estimators: 100, depth: 9, num features 7, valid accuracy for this fold, 0.7159368384363879\n",
      "avgfold accuracy: 0.7274237360272436\n",
      "standard deviation: 0.02226152216326725\n",
      "avg train time: 27.402652168273924\n",
      "num estimators: 1000, depth: 9, num features 7, valid accuracy for this fold, 0.7458675582148222\n",
      "num estimators: 1000, depth: 9, num features 7, valid accuracy for this fold, 0.6971567283960647\n",
      "num estimators: 1000, depth: 9, num features 7, valid accuracy for this fold, 0.7485294833103417\n",
      "num estimators: 1000, depth: 9, num features 7, valid accuracy for this fold, 0.7318310132012288\n",
      "num estimators: 1000, depth: 9, num features 7, valid accuracy for this fold, 0.713338508671001\n",
      "avgfold accuracy: 0.7273446583586918\n",
      "standard deviation: 0.019593393537557097\n",
      "avg train time: 273.43157238960265\n",
      "num estimators: 100, depth: 9, num features 9, valid accuracy for this fold, 0.7445911927971636\n",
      "num estimators: 100, depth: 9, num features 9, valid accuracy for this fold, 0.7196916599412497\n",
      "num estimators: 100, depth: 9, num features 9, valid accuracy for this fold, 0.7503201551810995\n",
      "num estimators: 100, depth: 9, num features 9, valid accuracy for this fold, 0.7436632256755136\n",
      "num estimators: 100, depth: 9, num features 9, valid accuracy for this fold, 0.7231929721026855\n",
      "avgfold accuracy: 0.7362918411395424\n",
      "standard deviation: 0.012386739107799439\n",
      "avg train time: 34.494512224197386\n",
      "num estimators: 1000, depth: 9, num features 9, valid accuracy for this fold, 0.752607833394878\n",
      "num estimators: 1000, depth: 9, num features 9, valid accuracy for this fold, 0.7118042352975162\n",
      "num estimators: 1000, depth: 9, num features 9, valid accuracy for this fold, 0.7496905689712086\n",
      "num estimators: 1000, depth: 9, num features 9, valid accuracy for this fold, 0.7451581656657349\n",
      "num estimators: 1000, depth: 9, num features 9, valid accuracy for this fold, 0.7183330725860911\n",
      "avgfold accuracy: 0.7355187751830858\n",
      "standard deviation: 0.016991299387349414\n",
      "avg train time: 345.9646418571472\n",
      "num estimators: 100, depth: 9, num features 11, valid accuracy for this fold, 0.7553736566483834\n",
      "num estimators: 100, depth: 9, num features 11, valid accuracy for this fold, 0.72290631900323\n",
      "num estimators: 100, depth: 9, num features 11, valid accuracy for this fold, 0.7541676409951386\n",
      "num estimators: 100, depth: 9, num features 11, valid accuracy for this fold, 0.7475539442245778\n",
      "num estimators: 100, depth: 9, num features 11, valid accuracy for this fold, 0.7260500357454764\n",
      "avgfold accuracy: 0.7412103193233612\n",
      "standard deviation: 0.01395424316727252\n",
      "avg train time: 41.87105541229248\n",
      "num estimators: 1000, depth: 9, num features 11, valid accuracy for this fold, 0.7518549110203118\n",
      "num estimators: 1000, depth: 9, num features 11, valid accuracy for this fold, 0.7182434835674139\n",
      "num estimators: 1000, depth: 9, num features 11, valid accuracy for this fold, 0.7551228752446284\n",
      "num estimators: 1000, depth: 9, num features 11, valid accuracy for this fold, 0.7533870238655338\n",
      "num estimators: 1000, depth: 9, num features 11, valid accuracy for this fold, 0.7212610330739724\n",
      "avgfold accuracy: 0.7399738653543719\n",
      "standard deviation: 0.01657072208793638\n",
      "avg train time: 418.60604152679446\n",
      "num estimators: 100, depth: 9, num features 13, valid accuracy for this fold, 0.7590129273982185\n",
      "num estimators: 100, depth: 9, num features 13, valid accuracy for this fold, 0.7161509309963185\n",
      "num estimators: 100, depth: 9, num features 13, valid accuracy for this fold, 0.7575592195537393\n",
      "num estimators: 100, depth: 9, num features 13, valid accuracy for this fold, 0.7494944602809989\n",
      "num estimators: 100, depth: 9, num features 13, valid accuracy for this fold, 0.7264931410272913\n",
      "avgfold accuracy: 0.7417421358513133\n",
      "standard deviation: 0.017297401658299176\n",
      "avg train time: 49.17155804634094\n",
      "num estimators: 1000, depth: 9, num features 13, valid accuracy for this fold, 0.752576498114129\n",
      "num estimators: 1000, depth: 9, num features 13, valid accuracy for this fold, 0.7247905190577555\n",
      "num estimators: 1000, depth: 9, num features 13, valid accuracy for this fold, 0.7525828205357576\n",
      "num estimators: 1000, depth: 9, num features 13, valid accuracy for this fold, 0.7554008800819196\n",
      "num estimators: 1000, depth: 9, num features 13, valid accuracy for this fold, 0.7175283537871391\n",
      "avgfold accuracy: 0.7405758143153403\n",
      "standard deviation: 0.01605196665936716\n",
      "avg train time: 492.3447031497955\n",
      "num estimators: 100, depth: 9, num features 15, valid accuracy for this fold, 0.7567061878986023\n",
      "num estimators: 100, depth: 9, num features 15, valid accuracy for this fold, 0.7343660685107851\n",
      "num estimators: 100, depth: 9, num features 15, valid accuracy for this fold, 0.7543791160473599\n",
      "num estimators: 100, depth: 9, num features 15, valid accuracy for this fold, 0.7599608852480189\n",
      "num estimators: 100, depth: 9, num features 15, valid accuracy for this fold, 0.715249059824659\n",
      "avgfold accuracy: 0.744132263505885\n",
      "standard deviation: 0.016989790154498988\n",
      "avg train time: 57.696100044250485\n",
      "num estimators: 1000, depth: 9, num features 15, valid accuracy for this fold, 0.7557534327456061\n",
      "num estimators: 1000, depth: 9, num features 15, valid accuracy for this fold, 0.7275077680823443\n",
      "num estimators: 1000, depth: 9, num features 15, valid accuracy for this fold, 0.7561263681795045\n",
      "num estimators: 1000, depth: 9, num features 15, valid accuracy for this fold, 0.7577869722045406\n",
      "num estimators: 1000, depth: 9, num features 15, valid accuracy for this fold, 0.7143805734723017\n",
      "avgfold accuracy: 0.7423110229368595\n",
      "standard deviation: 0.01794610549927504\n",
      "avg train time: 596.1519293785095\n",
      "num estimators: 100, depth: 9, num features 17, valid accuracy for this fold, 0.7565196147599305\n",
      "num estimators: 100, depth: 9, num features 17, valid accuracy for this fold, 0.7274954908110041\n",
      "num estimators: 100, depth: 9, num features 17, valid accuracy for this fold, 0.7514643699194329\n",
      "num estimators: 100, depth: 9, num features 17, valid accuracy for this fold, 0.7566315187409478\n",
      "num estimators: 100, depth: 9, num features 17, valid accuracy for this fold, 0.7089180507724389\n",
      "avgfold accuracy: 0.7402058090007508\n",
      "standard deviation: 0.018990394394087923\n",
      "avg train time: 65.99020953178406\n",
      "num estimators: 1000, depth: 9, num features 17, valid accuracy for this fold, 0.7548287605088899\n",
      "num estimators: 1000, depth: 9, num features 17, valid accuracy for this fold, 0.7317809807012128\n",
      "num estimators: 1000, depth: 9, num features 17, valid accuracy for this fold, 0.761716267178915\n",
      "num estimators: 1000, depth: 9, num features 17, valid accuracy for this fold, 0.768835967121468\n",
      "num estimators: 1000, depth: 9, num features 17, valid accuracy for this fold, 0.7078014254622652\n",
      "avgfold accuracy: 0.7449926801945502\n",
      "standard deviation: 0.022372606469149046\n",
      "avg train time: 650.7305725574494\n",
      "num estimators: 100, depth: 9, num features 20, valid accuracy for this fold, 0.7538116460666905\n",
      "num estimators: 100, depth: 9, num features 20, valid accuracy for this fold, 0.727458658996983\n",
      "num estimators: 100, depth: 9, num features 20, valid accuracy for this fold, 0.7381570047285664\n",
      "num estimators: 100, depth: 9, num features 20, valid accuracy for this fold, 0.7585162224743771\n",
      "num estimators: 100, depth: 9, num features 20, valid accuracy for this fold, 0.7086096098891197\n",
      "avgfold accuracy: 0.7373106284311474\n",
      "standard deviation: 0.018130013357155163\n",
      "avg train time: 76.56782178878784\n",
      "num estimators: 1000, depth: 9, num features 20, valid accuracy for this fold, 0.749847076324488\n",
      "num estimators: 1000, depth: 9, num features 20, valid accuracy for this fold, 0.7314011074820941\n",
      "num estimators: 1000, depth: 9, num features 20, valid accuracy for this fold, 0.7552523497663965\n",
      "num estimators: 1000, depth: 9, num features 20, valid accuracy for this fold, 0.7609235325049124\n",
      "num estimators: 1000, depth: 9, num features 20, valid accuracy for this fold, 0.7201372784721136\n",
      "avgfold accuracy: 0.743512268910001\n",
      "standard deviation: 0.015324371390494154\n",
      "avg train time: 764.1778861045838\n",
      "num estimators: 100, depth: 10, num features 1, valid accuracy for this fold, 0.5375568663418305\n",
      "num estimators: 100, depth: 10, num features 1, valid accuracy for this fold, 0.5101930239822052\n",
      "num estimators: 100, depth: 10, num features 1, valid accuracy for this fold, 0.5372831694107418\n",
      "num estimators: 100, depth: 10, num features 1, valid accuracy for this fold, 0.5129027943062205\n",
      "num estimators: 100, depth: 10, num features 1, valid accuracy for this fold, 0.5572089021088841\n",
      "avgfold accuracy: 0.5310289512299765\n",
      "standard deviation: 0.017491781943994412\n",
      "avg train time: 6.253942441940308\n",
      "num estimators: 1000, depth: 10, num features 1, valid accuracy for this fold, 0.5273606411861423\n",
      "num estimators: 1000, depth: 10, num features 1, valid accuracy for this fold, 0.501296154866945\n",
      "num estimators: 1000, depth: 10, num features 1, valid accuracy for this fold, 0.5560067546465659\n",
      "num estimators: 1000, depth: 10, num features 1, valid accuracy for this fold, 0.5216408822959621\n",
      "num estimators: 1000, depth: 10, num features 1, valid accuracy for this fold, 0.5543872868886385\n",
      "avgfold accuracy: 0.5321383439768507\n",
      "standard deviation: 0.020731523163431187\n",
      "avg train time: 62.03547058105469\n",
      "num estimators: 100, depth: 10, num features 3, valid accuracy for this fold, 0.6364067927383208\n",
      "num estimators: 100, depth: 10, num features 3, valid accuracy for this fold, 0.5840998503256185\n",
      "num estimators: 100, depth: 10, num features 3, valid accuracy for this fold, 0.6107382506448877\n",
      "num estimators: 100, depth: 10, num features 3, valid accuracy for this fold, 0.6256768973883523\n",
      "num estimators: 100, depth: 10, num features 3, valid accuracy for this fold, 0.6116921373814508\n",
      "avgfold accuracy: 0.613722785695726\n",
      "standard deviation: 0.017595534910060513\n",
      "avg train time: 14.025803995132446\n",
      "num estimators: 1000, depth: 10, num features 3, valid accuracy for this fold, 0.6355481685185129\n",
      "num estimators: 1000, depth: 10, num features 3, valid accuracy for this fold, 0.574661517434628\n",
      "num estimators: 1000, depth: 10, num features 3, valid accuracy for this fold, 0.6318764703205162\n",
      "num estimators: 1000, depth: 10, num features 3, valid accuracy for this fold, 0.6239061245952453\n",
      "num estimators: 1000, depth: 10, num features 3, valid accuracy for this fold, 0.622670355414994\n",
      "avgfold accuracy: 0.6177325272567793\n",
      "standard deviation: 0.022068328151785502\n",
      "avg train time: 139.8159441947937\n",
      "num estimators: 100, depth: 10, num features 5, valid accuracy for this fold, 0.6935767928283862\n",
      "num estimators: 100, depth: 10, num features 5, valid accuracy for this fold, 0.6236418719950024\n",
      "num estimators: 100, depth: 10, num features 5, valid accuracy for this fold, 0.6862059413896304\n",
      "num estimators: 100, depth: 10, num features 5, valid accuracy for this fold, 0.6734956042029908\n",
      "num estimators: 100, depth: 10, num features 5, valid accuracy for this fold, 0.6577635015912183\n",
      "avgfold accuracy: 0.6669367424014456\n",
      "standard deviation: 0.02482984951626208\n",
      "avg train time: 21.690477085113525\n",
      "num estimators: 1000, depth: 10, num features 5, valid accuracy for this fold, 0.7086077828581379\n",
      "num estimators: 1000, depth: 10, num features 5, valid accuracy for this fold, 0.6207874064083745\n",
      "num estimators: 1000, depth: 10, num features 5, valid accuracy for this fold, 0.6723236570091752\n",
      "num estimators: 1000, depth: 10, num features 5, valid accuracy for this fold, 0.6683105932711556\n",
      "num estimators: 1000, depth: 10, num features 5, valid accuracy for this fold, 0.648837826437394\n",
      "avgfold accuracy: 0.6637734531968474\n",
      "standard deviation: 0.02889526572524292\n",
      "avg train time: 217.68539123535157\n",
      "num estimators: 100, depth: 10, num features 7, valid accuracy for this fold, 0.7142368126628058\n",
      "num estimators: 100, depth: 10, num features 7, valid accuracy for this fold, 0.6537260615777377\n",
      "num estimators: 100, depth: 10, num features 7, valid accuracy for this fold, 0.7170693978205385\n",
      "num estimators: 100, depth: 10, num features 7, valid accuracy for this fold, 0.7010313748281811\n",
      "num estimators: 100, depth: 10, num features 7, valid accuracy for this fold, 0.6804217768175238\n",
      "avgfold accuracy: 0.6932970847413575\n",
      "standard deviation: 0.023643408155573878\n",
      "avg train time: 29.505866384506227\n",
      "num estimators: 1000, depth: 10, num features 7, valid accuracy for this fold, 0.7255624839258141\n",
      "num estimators: 1000, depth: 10, num features 7, valid accuracy for this fold, 0.6583619953454698\n",
      "num estimators: 1000, depth: 10, num features 7, valid accuracy for this fold, 0.7070836426335798\n",
      "num estimators: 1000, depth: 10, num features 7, valid accuracy for this fold, 0.6914985378093894\n",
      "num estimators: 1000, depth: 10, num features 7, valid accuracy for this fold, 0.67205961276064\n",
      "avgfold accuracy: 0.6909132544949785\n",
      "standard deviation: 0.02398932007426262\n",
      "avg train time: 293.4262556552887\n",
      "num estimators: 100, depth: 10, num features 9, valid accuracy for this fold, 0.7446918284592898\n",
      "num estimators: 100, depth: 10, num features 9, valid accuracy for this fold, 0.678363295291847\n",
      "num estimators: 100, depth: 10, num features 9, valid accuracy for this fold, 0.7272232083211052\n",
      "num estimators: 100, depth: 10, num features 9, valid accuracy for this fold, 0.7124392291441803\n",
      "num estimators: 100, depth: 10, num features 9, valid accuracy for this fold, 0.6854872375777044\n",
      "avgfold accuracy: 0.7096409597588254\n",
      "standard deviation: 0.02492879364280455\n",
      "avg train time: 36.97072644233704\n",
      "num estimators: 1000, depth: 10, num features 9, valid accuracy for this fold, 0.7196676984167489\n",
      "num estimators: 1000, depth: 10, num features 9, valid accuracy for this fold, 0.6772901173382153\n",
      "num estimators: 1000, depth: 10, num features 9, valid accuracy for this fold, 0.7319503361890967\n",
      "num estimators: 1000, depth: 10, num features 9, valid accuracy for this fold, 0.7150490318176367\n",
      "num estimators: 1000, depth: 10, num features 9, valid accuracy for this fold, 0.6880678431318186\n",
      "avgfold accuracy: 0.7064050053787032\n",
      "standard deviation: 0.020430909773133263\n",
      "avg train time: 370.5154183864594\n",
      "num estimators: 100, depth: 10, num features 11, valid accuracy for this fold, 0.7184738930202257\n",
      "num estimators: 100, depth: 10, num features 11, valid accuracy for this fold, 0.6971690056674051\n",
      "num estimators: 100, depth: 10, num features 11, valid accuracy for this fold, 0.7260291655092429\n",
      "num estimators: 100, depth: 10, num features 11, valid accuracy for this fold, 0.7269043072353065\n",
      "num estimators: 100, depth: 10, num features 11, valid accuracy for this fold, 0.694721353614848\n",
      "avgfold accuracy: 0.7126595450094056\n",
      "standard deviation: 0.013980001184455571\n",
      "avg train time: 45.28260054588318\n",
      "num estimators: 1000, depth: 10, num features 11, valid accuracy for this fold, 0.733636979083794\n",
      "num estimators: 1000, depth: 10, num features 11, valid accuracy for this fold, 0.6986661105784941\n",
      "num estimators: 1000, depth: 10, num features 11, valid accuracy for this fold, 0.7374694819397428\n",
      "num estimators: 1000, depth: 10, num features 11, valid accuracy for this fold, 0.7143585272926871\n",
      "num estimators: 1000, depth: 10, num features 11, valid accuracy for this fold, 0.6885569917557663\n",
      "avgfold accuracy: 0.714537618130097\n",
      "standard deviation: 0.019066125662624514\n",
      "avg train time: 450.96121549606323\n",
      "num estimators: 100, depth: 10, num features 13, valid accuracy for this fold, 0.7390282110094039\n",
      "num estimators: 100, depth: 10, num features 13, valid accuracy for this fold, 0.7071170258671274\n",
      "num estimators: 100, depth: 10, num features 13, valid accuracy for this fold, 0.7308696816705404\n",
      "num estimators: 100, depth: 10, num features 13, valid accuracy for this fold, 0.7398610688290482\n",
      "num estimators: 100, depth: 10, num features 13, valid accuracy for this fold, 0.6920875754269158\n",
      "avgfold accuracy: 0.7217927125606072\n",
      "standard deviation: 0.018993105197945163\n",
      "avg train time: 52.740701007843015\n",
      "num estimators: 1000, depth: 10, num features 13, valid accuracy for this fold, 0.7326908537506767\n",
      "num estimators: 1000, depth: 10, num features 13, valid accuracy for this fold, 0.6959046272674585\n",
      "num estimators: 1000, depth: 10, num features 13, valid accuracy for this fold, 0.7454579291504955\n",
      "num estimators: 1000, depth: 10, num features 13, valid accuracy for this fold, 0.7316746464450778\n",
      "num estimators: 1000, depth: 10, num features 13, valid accuracy for this fold, 0.6910772953843778\n",
      "avgfold accuracy: 0.7193610703996172\n",
      "standard deviation: 0.021728000227364194\n",
      "avg train time: 533.5922345161438\n",
      "num estimators: 100, depth: 10, num features 15, valid accuracy for this fold, 0.7383512188360939\n",
      "num estimators: 100, depth: 10, num features 15, valid accuracy for this fold, 0.7106109928521004\n",
      "num estimators: 100, depth: 10, num features 15, valid accuracy for this fold, 0.7275697815359392\n",
      "num estimators: 100, depth: 10, num features 15, valid accuracy for this fold, 0.7355326156145352\n",
      "num estimators: 100, depth: 10, num features 15, valid accuracy for this fold, 0.687681376111724\n",
      "avgfold accuracy: 0.7199491969900786\n",
      "standard deviation: 0.018804068568408414\n",
      "avg train time: 60.36418733596802\n",
      "num estimators: 1000, depth: 10, num features 15, valid accuracy for this fold, 0.7289971058959863\n",
      "num estimators: 1000, depth: 10, num features 15, valid accuracy for this fold, 0.7104345973506371\n",
      "num estimators: 1000, depth: 10, num features 15, valid accuracy for this fold, 0.7393847893854956\n",
      "num estimators: 1000, depth: 10, num features 15, valid accuracy for this fold, 0.7297225989169642\n",
      "num estimators: 1000, depth: 10, num features 15, valid accuracy for this fold, 0.6906164658912901\n",
      "avgfold accuracy: 0.7198311114880747\n",
      "standard deviation: 0.017355075866377665\n",
      "avg train time: 615.0638067245484\n",
      "num estimators: 100, depth: 10, num features 17, valid accuracy for this fold, 0.7335891317988177\n",
      "num estimators: 100, depth: 10, num features 17, valid accuracy for this fold, 0.7001067039317962\n",
      "num estimators: 100, depth: 10, num features 17, valid accuracy for this fold, 0.7323129956323928\n",
      "num estimators: 100, depth: 10, num features 17, valid accuracy for this fold, 0.7520230813937399\n",
      "num estimators: 100, depth: 10, num features 17, valid accuracy for this fold, 0.6818111965525915\n",
      "avgfold accuracy: 0.7199686218618677\n",
      "standard deviation: 0.025360895570556263\n",
      "avg train time: 69.6148853302002\n",
      "num estimators: 1000, depth: 10, num features 17, valid accuracy for this fold, 0.7328626036128265\n",
      "num estimators: 1000, depth: 10, num features 17, valid accuracy for this fold, 0.704284406601561\n",
      "num estimators: 1000, depth: 10, num features 17, valid accuracy for this fold, 0.7362151484667339\n",
      "num estimators: 1000, depth: 10, num features 17, valid accuracy for this fold, 0.7410049908209485\n",
      "num estimators: 1000, depth: 10, num features 17, valid accuracy for this fold, 0.6865647508015502\n",
      "avgfold accuracy: 0.720186380060724\n",
      "standard deviation: 0.02113910749336106\n",
      "avg train time: 684.6092881202698\n",
      "num estimators: 100, depth: 10, num features 20, valid accuracy for this fold, 0.7246428778722132\n",
      "num estimators: 100, depth: 10, num features 20, valid accuracy for this fold, 0.7096259223751464\n",
      "num estimators: 100, depth: 10, num features 20, valid accuracy for this fold, 0.740035300770622\n",
      "num estimators: 100, depth: 10, num features 20, valid accuracy for this fold, 0.7521812931853615\n",
      "num estimators: 100, depth: 10, num features 20, valid accuracy for this fold, 0.6903257492192436\n",
      "avgfold accuracy: 0.7233622286845174\n",
      "standard deviation: 0.021864422975777432\n",
      "avg train time: 79.70295844078063\n",
      "num estimators: 1000, depth: 10, num features 20, valid accuracy for this fold, 0.7296972398934025\n",
      "num estimators: 1000, depth: 10, num features 20, valid accuracy for this fold, 0.7176994921181723\n",
      "num estimators: 1000, depth: 10, num features 20, valid accuracy for this fold, 0.7418589297923752\n",
      "num estimators: 1000, depth: 10, num features 20, valid accuracy for this fold, 0.7558077104032325\n",
      "num estimators: 1000, depth: 10, num features 20, valid accuracy for this fold, 0.6828675199372622\n",
      "avgfold accuracy: 0.725586178428889\n",
      "standard deviation: 0.024827919213929288\n",
      "avg train time: 794.7116433620453\n"
     ]
    }
   ],
   "source": [
    "## hyperparameters for submission\n",
    "# max_depth_list = np.linspace(1,10, num = 10, dtype=int)\n",
    "# num_features = np.linspace(1,20, num = 20, dtype = int)\n",
    "\n",
    "# revised hyperparameters\n",
    "max_depth_list = np.linspace(1,10, num = 10, dtype=int)\n",
    "num_features = np.linspace(1,20, num = 10, dtype = int)\n",
    "n_ests = [10**2, 10**3]\n",
    "\n",
    "accuracies = {}\n",
    "stdevs = {}\n",
    "times = {}\n",
    "for depth in max_depth_list:\n",
    "    for num in num_features:\n",
    "        for ests in n_ests:\n",
    "            fold_accuracy = []\n",
    "            avg_train_time = 0\n",
    "            for fold in [1,2,3,4,5]:      \n",
    "                # subset of the data\n",
    "                random.seed(fold)\n",
    "                X_train_fold, X_val_fold, y_train_fold, y_val_fold = splitter(X_cv)\n",
    "                ros = RandomOverSampler(random_state=0, sampling_strategy = 'minority')\n",
    "                X_train_fold, y_train_fold = ros.fit_resample(\n",
    "                    X_train_fold, y_train_fold) # oversample minority class\n",
    "                \n",
    "                X_train_fold = X_train_fold.select_dtypes('number')\n",
    "                X_val_fold = X_val_fold.select_dtypes('number')    \n",
    "                    \n",
    "                RF = RandomForestClassifier(n_estimators = ests, max_depth=depth, max_features = num) \n",
    "                \n",
    "                startt = time.time()\n",
    "                RF.fit(X_train_fold.drop(columns = [\"ID\",\"pre_nucleus_id\",\"post_nucleus_id\"]),y_train_fold)\n",
    "                endt = time.time(); elapsed = endt-startt\n",
    "\n",
    "                y_hat_valid = RF.predict(X_val_fold.drop(columns = [\"ID\",\"pre_nucleus_id\",\"post_nucleus_id\"]))\n",
    "                \n",
    "                valid_acc = balanced_accuracy_score(y_val_fold, y_hat_valid)\n",
    "                fold_accuracy.append(valid_acc)\n",
    "                avg_train_time += elapsed\n",
    "                print(f\"num estimators: {ests}, depth: {depth}, num features {num}, valid accuracy for this fold, {valid_acc}\")\n",
    "            avg_fold_accuracy = sum(fold_accuracy)/len(fold_accuracy)\n",
    "            fold_std = np.std(fold_accuracy)\n",
    "            print(f\"avgfold accuracy: {avg_fold_accuracy}\")\n",
    "            print(f\"standard deviation: {fold_std}\")\n",
    "            print(f\"avg train time: {avg_train_time/5}\")\n",
    "            accuracies[(ests, depth, num)] = avg_fold_accuracy\n",
    "            stdevs[(ests, depth, num)] = fold_std\n",
    "            times[(ests, depth, num)] = avg_train_time/5\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame({'Param Pair':accuracies.keys(), 'Avg Accuracy':accuracies.values()})\n",
    "std_df = pd.DataFrame({'Param Pair':stdevs.keys(), 'Standard Dev':stdevs.values()})\n",
    "time_df = pd.DataFrame({'Param Pair':times.keys(), 'Training Time':times.values()})\n",
    "rf_hyperparams = acc_df.merge(std_df, on='Param Pair').merge(time_df, on='Param Pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the 5 best parameters\n",
    "rf_hyperparams = rf_hyperparams.sort_values(by='Avg Accuracy', ascending=False).reset_index()\n",
    "rf_hyperparams.head(5)\n",
    "rf_hyperparams.to_csv('final_rf_hyperparams.csv')\n",
    "# hyperparameters for submitted models were saved to rf_depth_feat_cv.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6, 7)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best parameters\n",
    "rf_best_param = rf_hyperparams['Param Pair'][0]\n",
    "rf_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Param Pair</th>\n",
       "      <th>Avg Accuracy</th>\n",
       "      <th>Standard Dev</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Num Estimators</th>\n",
       "      <th>Tree Depth</th>\n",
       "      <th>Num Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>(100, 6, 7)</td>\n",
       "      <td>0.773917</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   Param Pair  Avg Accuracy  Standard Dev  Training Time  \\\n",
       "0    106  (100, 6, 7)      0.773917      0.008084       0.008084   \n",
       "\n",
       "   Num Estimators  Tree Depth  Num Features  \n",
       "0             100           6             7  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Standard Deviation\n",
    "rf_hyperparams[\"Standard Dev\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyperparams[\"Num Estimators\"] = rf_hyperparams.apply(get_param0, axis=1)\n",
    "rf_hyperparams[\"Tree Depth\"] = rf_hyperparams.apply(get_param1, axis=1)\n",
    "rf_hyperparams[\"Num Features\"] = rf_hyperparams.apply(get_param2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Random Forest: Tuning Number of Features')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAHJCAYAAACG+j24AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACLGElEQVR4nOzdeXhTVcIG8Dd70rTpBqXssrYiu1BEZVUYR1AQGZ1RcAPkUxRlVBZlXMAdEAUERUEHN1xAxRF3FBXZdUYRAVnEFigtdE2a5S7n+yNNaGgLTZo2afP+nqfQ3NzcnJOkvW/POfccjRBCgIiIiChGaSNdACIiIqJIYhgiIiKimMYwRERERDGNYYiIiIhiGsMQERERxTSGISIiIoppDENEREQU0xiGiIiIKKYxDBFR0DhXKzUm/DwTwxBFzPjx45GRkRHwlZmZid69e2PMmDH48MMP67U8GRkZWLx4cb0+5+nPX91Xr169Ilau0+3cuRO33nprUI8ZOnToGesX7tc+JycHGRkZWLt2bdiOebbnGjNmDGRZrnT/1q1bkZGRga1bt9Z5WYDIf46rs3fvXowePRpdu3bF5ZdfXuU+ixcvPuNnZMWKFWEv19KlS+vkuNSw6CNdAIptXbp0wUMPPeS/rSgKcnNz8eqrr2L69OlISkrCoEGDIljC+jV27Fj87W9/q7Rdq42ev1veffddHDhwIKjHLFmyBB6Px3/7jjvuQJcuXXD77bf7t6Wnp4etjGlpaXj77bfRpk2bsB3zbH799Ve89NJLuO222+rtORuS559/HkePHsXzzz+PlJSUM+779ttvV7m9RYsWYS/Xc889hzvuuCPsx6WGhWGIIio+Ph49e/astH3gwIHo378/1q5dG1NhKD09vcrXo6Hr0qVLwG2j0YiUlJQ6q6vRaKz319Fms+H555/HpZdeik6dOtXrczcEhYWF6Ny5c41+nhvjzwBFt+j5c5OoApPJBKPRCI1G499WUFCARx55BEOGDEHXrl2RlZWFKVOmICcnx7/P+PHj8cADD2D58uUYPHgwunXrhr///e/4+eefA46/bds2XHvttejRowf+8pe/4IcffqhUhtLSUjzxxBO49NJL0a1bN4wcORLvvfdewD5Dhw7FkiVL8Pjjj6Nfv37o1asX7rnnHjgcDixfvhwDBw7E+eefjzvvvBOFhYVheW1qWq7HH38cN954I7p3744HHngAAFBUVIQHH3wQF154Ibp164ZrrrkGmzdvDnjspk2bcM0116BXr17o27cvbrvtNn9L0MyZM/H+++/jyJEjAd1Q48ePx9ChQ2tdt/Hjx2P8+PEB207vZlq7di26dOmC//3vf7j22mvRrVs3DBkyJKCr4/Ruspo8BgDy8vIwbdo0ZGVloW/fvnjwwQexcOHCGtVt8uTJiI+Px8yZM6EoSrX7rV27FhkZGQGfW8D7ns2cOdN/OyMjA2+99RZmzpyJ888/H1lZWXj00Ufhcrnw1FNP4YILLkC/fv3wwAMPwO12BxzLbrfj3nvvRa9evdC/f388+uijcDqdAft8+eWXGDNmDLp164aLLroIjz76KMrKyvz3L168GMOGDcOSJUuQlZWFiy++GMXFxVXWKS8vD7NmzcKgQYPQvXt3jB07Fl999VVAXbZt24bt27eHrftyx44dGDduHHr06IGsrCzMmDEDBQUFAfts374dEyZMQN++fdG1a1cMHToUixcvhqqq/nIB3pZL3/czZ86s9H6f/nnyfSZXr16NIUOGoHfv3ti0aVONyqWqqv8z5SvTggULIElSrV8TCh3DEEWUEAKyLPu/3G43Dh48iFmzZsHhcGDUqFH+/SZPnoxNmzbh3nvvxYoVK3DHHXdg8+bNAd1sAPDZZ5/hq6++wuzZs/HMM8/gxIkTuPPOO/0nqF9//RW33HILEhISsGjRItxwww345z//GXAMl8uF6667Dh999BEmTpyIpUuX4vzzz8cDDzyAF154IWDflStX4tixY1i4cCFuu+02/Oc//8HVV1+N77//HnPnzsU///lPfPXVV1i0aNFZXw9VVQNeD99XKOV644030K1bNyxduhRjx46F2+3GjTfeiK+++grTpk3DkiVLkJ6ejokTJ/oDUXZ2Nm6//XZ07doVy5Ytw2OPPYZDhw7h1ltvhaqquP322zFo0CA0bdoUb7/9NgYPHgwAeOihh7BkyZIavOPhoaoq7r77blx++eVYvnw5evfujaeffhrfffddyI/xeDy48cYb8eOPP+L+++/HE088gT179mDlypU1KlNKSgoefPBB7Nq1Cy+//HJY6jlv3jwYjUYsWbIEo0ePxmuvvYbRo0fj2LFjmD9/PsaPH4/33nsPr732WsDjXnvtNTgcDjz77LOYPHky3n33Xdx7773++z/66CNMmTIF7du3x/PPP4877rgD69atw+233x4wmPjo0aPYuHEjFi5ciFmzZiExMbFSGU+cOIGxY8dix44dmDZtGhYvXoyWLVtiypQpWLduHQBvt1eXLl3QpUuXgM9Ndar6GfAFGMAbcm666SaYzWY8++yzuP/++7Ft2zbccMMNcLlcAIA9e/bgpptuQlJSEhYuXIhly5ahT58+WLJkCT755BN/uQBv93R1XXNnsmTJEsyYMQMPPvggevXqVaNyvfTSS3jrrbcwZcoUrFy5Ev/4xz+wYsUKLFu2LOjnp/BhNxlF1Pbt23HeeecFbNNoNOjcuTOee+45DBkyBID3L0+LxYIZM2agT58+AIB+/frhzz//rPRLTJZlrFixAvHx8QAAh8OBGTNm4LfffkPXrl3x4osvIjU1FcuWLYPBYAAAJCcnY9q0af5jrF27Fvv27cPq1av9g5cHDBgAWZaxdOlS/P3vf0dSUhIAb1ffwoULodfrceGFF+L999/H8ePH8e677yIhIQEA8N133+HHH3886+uxdOlSLF26tNL277//Hk2bNg2qXC1atAg4Ab7zzjvYs2cP3nnnHfTo0QOAtzty/PjxmD9/PtasWYOff/4ZLpcLkydPRrNmzQB4u+6++uorlJWVoU2bNkhJSanUDdWxY8ez1i2chBC4/fbb/eOrzj//fHzxxRf45ptvMGDAgJAes27dOhw8eBBr1qxB165dAQAXXHABLr300hqX6/LLL8cnn3yCJUuWYOjQobXuLuvYsSPmzJkDAMjKysK7774LSZIwf/586PV6XHzxxfjss88qfbY6dOiA559/HlqtFoMGDYJGo8Hjjz+Offv2oVOnTpg/fz4GDBiA+fPn+x9zzjnn4KabbsLGjRv9YUWW5YCfuaq88sorKCgowGeffYaWLVsCAAYNGoSbbroJTz/9NEaOHImePXv6fx5r0gV2+u8EALj22mv9r8WCBQvQrl07vPjii9DpdACAHj16YMSIEVizZg2uv/567NmzBxdeeCHmzZvnH3N30UUXYcOGDdi6dStGjBjhL0uo3dPXXXcdLrvsMv/tmpRr27Zt6Nq1K66++moA3vfVYrH4f1dQZDAMUUSdd955eOSRRwB4A8+zzz4LSZLw7LPPon379v79mjVrhlWrVkEIgZycHBw+fBgHDx7Ejz/+GDAwF/CeQHy/eH2PBeDvJti5cyeGDBniD0IAMHz4cP8vL8DbjdayZctKV3FdeeWVeO+99/C///3PP/ahe/fu0OtP/Sg1adIEcXFxAb/ckpKSsG/fvrO+Htdccw2uueaaStuTk5ODLte5554bsM/mzZvRtGlTnHfeeQGtTUOGDMHTTz+N4uJi9OjRAyaTCWPHjsVll12GgQMHol+/fujevftZy17fKr4GvjFIFbt5gn3Mli1b0Lp1a38QArxBd8iQIUFdCfbwww/j8ssvx6xZs0JqbaiuvDqdDsnJyTjvvPMCPm9JSUkoLS0NeNxll10WMOh++PDhePzxx7F9+3bodDrk5uZi8uTJAZ+Dvn37Ij4+Hps2bQpouTn9c3S6bdu2oVevXv4g5HPllVdi1qxZOHjwYNBh+fRuXwBITU0F4P05/t///ocJEyb4W5YBoHXr1ujQoQM2bdqE66+/HqNHj8bo0aPhdrtx6NAhHD58GL/99hsURQlbl1TF16am5erXrx8WLFiA6667DkOHDsXgwYMxbty4sJSHQscwRBFltVrRrVs3/+0ePXrgyiuvxC233IK1a9cGXHWybt06PPPMMzh27BiSkpJw7rnnwmw2VzqmxWIJuO07Kfia2YuLi/3hwkev1wdsKy4uRtOmTSsdu0mTJgCAkpIS/7aKwcsnLi6u+kqfQVpaWsDrcbpgynV6GYqKipCfn1/lX90AkJ+fj44dO+L111/H8uXL8d5772HVqlWw2Wy47rrrcPfddweM4Yq00997rVZ71vlizvSYwsJC/wm3oqq2nUlqair+9a9/4Z577sGKFSv8rXChCPWzdfpnxFeHkpISFBUVAQAeeeQR/x8iFeXl5QXctlqtZ3yu4uJitG7dutL2qj6TNXWmn4GSkhKoqoqXXnoJL730UqX7TSYTAG+X8ty5c/Hhhx9ClmW0atUKvXr1gl6vD9u8QhXfi5qWa+LEibBarVizZg3mz5+PefPmoVOnTpg9ezYuuOCCsJSLgscwRFGlSZMmePDBB3HXXXfhsccew4IFCwB4ByXOmDED48ePx4QJE/ytPU8//TR27twZ1HMkJSXhxIkTAduEEAGDQxMTE3H48OFKj83PzweASmGqvtSmXAkJCTjnnHMCukYqatWqFQBvS5fvUvidO3fi7bffxgsvvIDMzEz89a9/DUMtzuz0wcdna+0Jl2bNmuGPP/6otP3kyZNBH2vkyJH45JNPsHjxYsyaNSvgPl+grDgGBvB254aLL/D4+D4fqampsNlsAIDp06cjKyur0mOrGhd0JomJif7jV/Wc4f5ZsVqt0Gg0uOmmmzBixIhK9/v+GHrsscfw2Wef4dlnn8WFF17oDy79+/c/4/E1Gk1In8Galkur1eL666/H9ddfj5MnT2Ljxo144YUXcOedd2LTpk0wGo1nfS4KPw6gpqhz2WWXYcCAAfjPf/6Dbdu2AQB++uknqKqKO++80x+EFEXxXwV2+onlTPr3749vv/024Oqa7777LqDpvG/fvjhy5Ah++umngMeuW7cOBoMhYt1GtSlXVlYWjh07htTUVHTr1s3/tWnTJrz88svQ6XR49dVXMWTIEHg8HhiNRvTv3x9z584F4B1MC9TtnEfx8fHIzc0N2BZs2A1VVlYWcnJy8Ntvv/m3uVyuMw7KPpNHHnkEcXFxeOaZZwK2+1p7KtbzwIEDlQJMbXz77bcBtz/++GNoNBpkZWWhffv2SE1NRU5OTsDnoFmzZliwYAF2794d1HP17dsXP/30E44cORKwfd26dWjatCnatm1b6/pUFB8fjy5duuDgwYMB5e/UqRMWL17s79LcuXMn+vXrh0svvdQfhHbt2oWCgoKA3xenf56tVisKCwsDrtCryWewpuX6+9//jkcffRSAN5yOGTMG119/PUpKSmC322v34lDIGIYoKt1///0wGAx49NFHoSiK/yQ/Z84cbNmyBZ999hluvvlm7NmzB0BwrQdTpkxBWVkZJkyYgA0bNuC9997zP5/PmDFj0LFjR0yZMgWrV6/G999/jzlz5mDNmjWYPHmy/6/r+labco0ZMwYtWrTAzTffjPfffx9btmzBM888g+eeew5paWkwGAy44IILkJ+fjylTpmDjxo34/vvvMWvWLBiNRv9gdpvNhhMnTmDjxo3+LpX9+/cHfRKtypAhQ3DkyBE88cQT2Lp1K55//nl88MEHtT5uTYwcORIdOnTAlClT8OGHH+Lrr7/GrbfeipMnT4bUPdikSRM88MADlcbz9OvXD2azGU8++SQ2btyI9evXY8qUKf6B7+Hwyy+/4IEHHsDmzZuxfPlyLFq0CGPHjsU555wDnU6HadOmYfXq1Xj00UexadMmfPLJJ5gwYQJ2795dbTdqdW6++WYkJSXhpptuwocffoiNGzdi2rRp2LJlC6ZNm1Yn4fmf//wnvv/+e9xzzz3YuHEjNmzY4L8q0lf+7t274/vvv8dbb72Fbdu2YdWqVZg0aRI0Gk3AH0I2mw0//vgjtm/fDiEEhgwZArfbjQceeABbtmzBqlWrsHz58oAxhbUpV9++ffHWW2/hhRdewNatW7Fu3Tq88soryMrKOutklFR32E1GUal9+/YYP348Vq5cibfeegvjxo3Dgw8+iFdeeQWffvopmjRpgn79+mHJkiWYMmUKdu7cWePJGc855xy8/vrrePLJJzFt2jSkpqZixowZePLJJ/37WCwWvPbaa1iwYAGee+452O12tG/fHo899hjGjh1bV9U+q9qUKy4uDm+88QYWLFiAefPmobS0FC1btsQ999yDW265BQCQmZmJF154Ac8//zz++c9/QlEUdO3aFStXrvQPaB8zZgw2btyIKVOmYOrUqbj11lvxyCOP4MiRI9iwYUOt6nf11Vfjzz//xPvvv4/Vq1ejb9++WLRoEf7xj3/U6rg1odfrsWLFCjz22GN4+OGHodfrceWVVyIpKQmHDh0K6ZhXXnklPvnkk4DXxWazYfHixViwYAGmTJmCli1b4o477ghr6JsyZQp27dqF//u//0NCQgImTpwYMMvy3/72N1itVrz88st4++23ERcXh969e2P+/PlVjv85k6ZNm+Ktt97CggUL8Oijj0KSJGRmZmLp0qW45JJLwlanii6++GKsWLECS5YswdSpU2EwGHDeeefhlVde8V8VNnPmTP/FGB6PB61atcJtt92G/fv3Y8OGDVAUBTqdDv/3f/+HpUuXYtKkSVi/fj0uuugizJgxA6+99ho+++wznHfeeViyZAn+/ve/h6Vcd911F4xGI9asWYPnn38eCQkJGDp0KO655546ea2oZjSCK9QREeH333/HwYMHMXz48ICWoLFjxyI9Pb1e51EiovrFliEiIni7Wu+66y5cd911GDZsGBRFwfr167Fr166A+ZqIqPFhyxARUblPP/0UK1aswIEDByCEQJcuXXDbbbfh4osvjnTRiKgOMQwRERFRTOPVZERERBTTGIaIiIgopjEMERERUUxjGCIiIqKYxkvra0gIAVVt/GPNtVpNTNQTiK26ArFVX9a18Yql+rKu4TluTWaQZxiqIVUVKCgI30KK0Uiv1yI52YqSkjLIcs3X+mqIYqmuQGzVl3VtvGKpvqxreKSkWKHTnT0MsZuMiIiIYhrDEBEREcU0hiEiIiKKaREPQ6qqYtGiRRgwYAB69uyJSZMmITs7u8p9Fy9ejIyMjCq/Zs2a5d/v0KFDuPXWW9GrVy9cdNFFmDNnDpxOZ31ViYiIiBqQiIehpUuX4s0338TcuXOxevVqqKqKiRMnwuPxVNr3lltuwffffx/wNWHCBMTFxeGmm24CABQWFmLcuHHQ6/V49913MW/ePHzxxRd46qmn6rlmRERE1BBENAx5PB6sXLkSU6dOxeDBg5GZmYmFCxciNzcXn3/+eaX9rVYrmjZt6v/Kz8/HqlWr8OCDDyIjIwMA8Prrr0Ov12PhwoXo2LEjLrzwQkydOhU///wzuAwbERERnS6iYWjPnj1wOBzo37+/f5vNZkOXLl2wffv2sz5+zpw56NOnD6666ir/tu+//x7Dhg2DyWTyb/vb3/6GtWvX1miuASIiIootEZ1nKDc3FwDQvHnzgO1paWn++6rz9ddf46effsIHH3wQsP3QoUO45JJL8MQTT+Czzz6DwWDAsGHDcNdddwUEpFDo9RHvVaxTOp024P/GLJbqCsRWfVnXxiuW6su61q+IhiHfoGaj0Riw3WQyobi4+IyPfeWVVzBkyBCce+65AdvtdjteeukljBgxAkuWLMHRo0cxd+5c5OfnY968eSGXVavVIDnZGvLjGxKbzRLpItSbWKorEFv1ZV0br1iqL+taPyIahsxmMwDv2CHf9wDgdrthsVT/ohw9ehRbt27F8uXLK92n1+vRrl07PPzwwwCArl27QlEU3H333Zg5cyZSU1NDKquqCpSUlIX02IZCp9PCZrOgpMQJRWncM57GUl2B2Kov69p4xVJ9WdfwsNksNWpximgY8nWP5eXloU2bNv7teXl5/gHRVfnyyy+RkpKCiy66qNJ96enp6NSpU8A23+0jR46EHIYANPop0X0URWVdG6lYqi/r2njFUn1Z1/oR0c7IzMxMxMfHY+vWrf5tJSUl2L17N/r27Vvt43bs2IGsrCzo9ZWzXN++fStdObZv3z7odDq0atUqvBUgIiKiBi+iYchoNGLcuHGYP38+vvrqK+zZswfTpk1Deno6hg8fDkVRkJ+fD5fLFfC43bt3IzMzs8pjTpgwAdnZ2XjooYdw6NAhfPfdd3jqqacwatQopKSk1Ee1iIiIqAGJ+Kr1U6dOhSzLmD17NlwuF/r27YsVK1bAYDAgJyfHf2XYmDFj/I/Jz89HUlJSlcdr3749Vq1ahaeffhqjRo1CQkICrrzySkybNq2eakREREQNiUZwJsIaURQVBQWOSBejTun1WiQnW1FY6Gj0fdSxVFcgduorhIBOp0VSUhwKCh2QJBWAgCoACEAt/3UnhIAAIETg9xDl+/r2Cbjf+7+qev/XAtBoNdBoNNAA0MD7jwYa+KY002i833vv0/j30Zbv4L+vwmO05Rs15fdXf58mZt5Xn1iqL+saHikp1ugfQE1EjY8qRECQUAWqv43A+3yPR4X7VN9jVFF+7PJAUr49YBu8YSi+wFl+ZYqoFHa8uejU83vv8E3IWv69EP50I4SAxpdAIPyTt576O1JT/jDvMbUaDUR5BYRGA43vPgBalN8H+MORPyDBn6D8//lCj//pgYD7DAYdEgqdcNhdUNXy7RoNtJoKQUwDaDXaCt9rvF/lQU6r9e7v/d93Hyrcx8lqqfFjGCKKIUIIqBVChS+I+G9DlAeNaoJM+TEURUBRVaiq97GqKqCowh9ITgWd01pYyo+B8oChQkALDVQAGl8oKQ8dEN6TuSgPIP6WF1+IqNj64r/t3aiFBjqdr1VGE9DC4g8YOBUe6psvSPnylD88+beJCveh/L4K28pvS7IKj6TCJamQZeW0Y+JU+KvQ2qXxBTKhgfAFPPhClOZUmPKFJI0Gep0WOq33NdXptNBrGaiocWEYImqAXB4ZLo9SIbScChyyKqD6gooqoAgBrQaIK3CiuMTpb4auGHQgBFScOgbKT5jemFHhBOq77W91OBVEKrZIBLZMaPyBpOK2uqLTaWEx6yFJ+qidn6ViGCvfEtJxdDotEuKMEIoCRdHVqkwVW9sqfqZkVYUkq/4WP1+Xo//z4G9YE/4ghAoBSVODQKWt+L0mMERptfCHNKK6wjBE1EAIIeBwySiyu1BU6oFHVsr//K8QXDSiUkDRaAC9XgejrHpbBspPUDp/SDnVnVIxtFBs0Wq9nXahRqrArtCKXZveQOWRlcBWyPJ9NRoAQnPWQKXTamEy6JDkkFDmcAHCGwZ9rVcMVFQbDENEUU5VBUrLPCgsdaHYIUFRBeLMOsTHmc/+4HI6nRbxcUaoihK1rSXUsPlCdTgCVcVWKF+gkmQFsqoCJW6UlLogK2pIgUqn1UCv82436LTQMlARGIaIopYkqygt8+BEsQt2pwStBrBaDDA08gWDKXb5ApW2mm5DnU4LW4IJWqjVhvqzBSr36ePkgghU3nFRWuh0Gui1Wuh0gF7LQNUYMAwRRRmXR0ax3YOTJS443TKMBi0S4w3QaRmCiM7mbIGqJs4UqBRZgVuqYaDSaqCFpnwwenm5NPCPmdLrtd7vy8OUTqv1D0Q3GnUwuSR4JAWqKjgQvY4xDBFFgYrjgQpLPfBICiwmHVJsJv4VSVTP6jZQCciSAmfAgPUKjyu/uk+v1yGxyA2H3eUfo+7t4tOWf5WHqSoHoWug05a3RPmDGH+PnAnDEFEEKaoKe5mEglIXSsrHA1nNeiTEGSJdNCKqhdoEKiEENFot4s0GyB4JcvnVfAED0cuD1akH+eaNKL84ooruPYPuzK1RuvKpEnTawIAVCxiGiCJAkhWUOCScLCkfD6QFrGaOByIi+AOJXq+F0aCDTlvzQOK/mk8NnFdMlhW4PFW0Rgnhz1G+cVKa8iAUS61RDENE9cjpllHscKOgxI0ytwyTQYukeGP5Zc1ERLWj0Wig02hQgxUoAlTs1vNNvHrm1qgKc0zVsjXKYNAhTlLC/EoEh2GIqI5VHg+kwmLSIpXjgYgoSgR06wU5N0JtW6P0eh1SSj1IsRpgNtRu8tBQMQwR1ZGK44GKHR6oKjgeiIgandq2Rmm0GpS5JCTHRS6SMAwRhVlV44HizQboOR6IiMjP1xql02mhKOLsD6hDDENEYcLxQLGlYneAd5FaBN723a9WcVsE7l/lfuLU4ren7gt8jBCATq+Fyy1DUdTTjq1658WpcAxVeJdrMRm8S1sYDbry/723K35vNOhgMupg0mthMnpvG/Vadu1So8QwRFQLvvFAhaUuFNl944F0HA8UpYQQcEsK3JIKt0eBW1Lg8ijwlP/vlhT/drdHgUuqsM23XVKgqCJgbphYEhicdKcFK60/YDFYUUPCMEQUAkVVUVomla8X5oFQgTiOB6ozQggoiggMJ77AUiGkuD0KPLK3RaS0zAOXW66wvwp3PVyxoqt0yTEqzdtSaZ8z7Vdhn8B5YAC9TguL2QhJkr2DUSvsV90lz6oKeKRTr5mn/HXxSKdeo4q3ffv6wp9HUuGRVJRCqvVrVVWwOhWmKgcri8mApikeQFVgMuhgMekYqCgsGIaIglBxPJDDKUHD8UBBkWQVBSXeABnQGlMhsFTXQqOq4WuK0Wo1MPtaKyr8bzaWt3QYT52EzeUtGeYK23S66oNKffKu1WVBSamzThfgFUJAVlS4PSrccnlQ8ihwyyo8njMHK89p94UzWGk0QJxJjzizHnFmA6zm8u9NeljNhvLt5d+X78dua6oKwxBRDTjdMoocbhSWuOF0KzAaNEjkeKAqCSFQ5pZxotiFk8UunPB9FTlRZPfU+vhGg7baIGM06GAx6ZGUYIZQVRjLJ60zGyvsVx5m2KJQcxqNBga9Dga9DvGoXeunP1hVDEu+YFWhdc/tUSq0YJ3a1yWpsJd54PJ4Q5XDJcPhkgG4avT8ZqOuPDQZyoOSL0BVCFO+YGXS8w+dGMEwRFSNiuOBCks98MgK4kx6pNiMPJHCO5C3yO4+FXaKXThR7MTJYhec7uq7o8xGHZLiTYEB5bRgc+p7LcxGvf97k+Hs3SL11VpCoQkIVpbgglXF99bjkVHm9gahMpeMMpdU4XsZDrd06nuXDKdbBgC4yrtWT5a4a/ScRr22csvTaa1NFcMVx0I1TAxDRKfxjQcqKHGhpKx8PJBFD5s1NscDeSTF38qT72/tcaKgxA3lDF1XSfFGpCaa0STRjCaJlvL/zYgz63myoFrT6bRIiDMiIc5Yo/1VVcDpORWOylxShe/Lw5T7VJgqc8lQhYBHVuGxe2rcqqnTaqpueTIbYDH6Win1/j8AfH8UMERFFsMQUTlJVlDs8OBksRtlrtiaH0gIAbtTQn5Rxa4tbytPSVn14zn0Og1SbWY0SToVdlITzUi1mWDQR2YmWaKqaLUaWM0GWM0GNK3B/kIIuD1KhYB0quWpYpg61TolQVa8UyCUlEln/LmpikaDgHFqZpMe8XFG6DQIGMdmNuoDQlTFUGVgoAoZwxDFvMDxQDJMBl2jHQ8kKypOFDn93Vq+Vp4TxS54pOq7lKxmvb+F51RrjxmJ8ewypMZJo9HAbNLDbNIj1Xb2/YUQkGS1vIuu6pYnp7t8uoYK0zq4PQrU8pmYfV14xY5Qy4xTYcrf9awP6IY+vUXKfNr2WA1UDEMUs0rLPDh8vBQni1yQZAUWkx4pjWR+IOdpA5hPlrhQUOrGySInquvZ0miAlAQTUhMDW3maJJphMfFXBdGZaDQa7/xJBh2SEkw1fpxvQLkvGLnKr56UZBUarQ5FpU44XXJAeHJ5Ktwu/1+Ur/3lC1Sh0mo0MBm1FQKTvlJgOluo0usaXqDibziKSSUOD07m2VFQ5ITZqGuQ44GEECi2e6ps5fFeXVM1o0FbZStPSoIJumAXFyKiWqk4oDwh7tT2YC4E8LVKBQSm0+bh8m0/9f2pQOXbx7dSvdOtnPEiiLPxBqrqA9Pp4cliMiDRZkLLFEvIz1lbDEMUc+xOCTn5dlgsJqTYzA3uiqODR0vw9U9HcLzACfkMZbfFGcrDjgVNky1o2zwRcUYt4jhRHVGjUrFVqmKgCoYvUFUXnnytUWcMWAGB6tQVfDWrAzBnQhZaNokPrQK1xDBEMaXMJeHP46VQVIHmCSaUlDojXaQac7plfLE9G//df9K/TafVIMVm8rfyNPUNYE40w2Q4NYCZl5sT0ZlUDFShEuVX31UfmOSASVUrTrpqNRuCnmohnBiGKGY43TKy8+zwSApSEs2RLk6NCSGw+49CfLr1T3/3V9/MNGR1SUNyvKlRDvQmoobHuwiwt2vMZq3543Q6LdyKqFUQqy2GIYoJbo+C7Dw7HC4ZyQkN5wqoEocH67f8iX3ZRQCAJolmXHHROWidFpmmZCKixohhiBo9j6QgO98Ou9OD5ISGcbWYEAI79+bjq51H4JYUaLUaXNwtHRd3bw49BzkTEYUVwxA1apKs4sgJO4rsbqQ0kCB0otiF//zwB/48bgcAtGxixRUXnYO05MhdaUFE1JgxDFGjJSsqjp6wo6DEjeSE6J9EUVFV/LDrOL7971EoqoBBr8XQ3i3RNzMt6stOkaMK76zHsqJCCAFVAEIt/x8CQkX5pH7l24SARqOBVoPy/zXQaL2XQ2s0vv9P3a8p/5+oMWMYokZJUVUcPeFAfrELyfFG6LTR3bV09IQDH236A8cLvVe3dWhpw4j+bZEUX/PJ26hh8YYTb1gROC3E+O7zf+/93/tADQABaACtTgtJBRwOD4QqAI0GWq0vyAA6jRZ6gxZarQZ6nQY6jRZaHaCq3p8RRTkVpFQhoKqALARUVQXKy6AKAUADCAAab5lOhaRqQpT2tLDFQEVRjmGIGh1VCOSeLENekRNJVmNUTyTokRR889+j2Lr7OIQALCY9Lstqja7tU3jyiFIVQ4wqBATOHGJQ3kIDoYHQABqUhwsNoNUAWmjKw8OpgKHTaqHXaaHTaqDTacpva/zhwhd4DAYdkpOtKCkpg6qI8u0aaLWngkhNecNQeQtSed1821SBCvcJqAAURfWGKFX41+RSVNX7Osi+10hAxanXofzF8FayPFx5W6YqhqfAEFWx5YqorjAMUaMihEBuQRlyC8uQaI3uRVYPHi3Bf374w78adrf2KRie1RpWc8ObDbsxUctP6or/BO/9UlUBDTTeE7hWUynEaDWAVqfzBhhfiNFpoffvU6HVxhdafI+tEGJ8+9aEXq+FzWqE4pEgy7WbP0qr0UCrq13gEP4AdVq4Kg9Tp8JV+TZFhayqUBRvS5WseFulVAHI6qlAJeA9nlargVsRcDjckGX1VIAsf121FV7XioGQ3cx0NgxD1GgIIZBX6ETuyTLEWwxRu2q60y3j8+3Z+F/55Ik2qxEj+rdBp1ZJkS1YDBCiQsBRykOP6j15lzdVlJ9UvS0xep0WVrMORqMORp3WG27KW2oCT7bBhZjGSqPRQKfRoDaNsaK8ZS0gOKmnwpDNZkFBoQOe8gVOZVlAKm+lkhXv+ymrAqrsDVJK+fHKm6YgBBiiqBKGIWo0Tha7cPSkA3FmXcDsy9GiqskTs85Nw5DeLaOyvA2RL9yUuSTYnRI8klI+47ZvnI0Geo23xUanBUwmA4wG76KU+gphR6/VQlcehqh++cYjaaEBTvux0Ou1SLaZAUWptiXMF6J8rXm+AebitG3BhigfX0jSVBGiKrYAUsPCMESNQkGJCzkn7DAbdTAbo+9j7Z088TD2ZRcD4OSJoVDVUyc5X+hRlAqtOuUnJKNeB0scYNRrEWfUw2TQQq/3jsHxjcPxBp/gxtRQw+Dr7gu2YbhicKr0ve+zJwSUWoQo/9gonBofptFooEHgoHSNxjumzNfNSHUv+s4aREEqsruRk2eHQa+FxRRdH2nf5Ilf7syBR1I5eWI1hKgYdE51YZVf1ARRPtBWV95io9NoYDbrYSpfS8kXcPQ6DUwmPZqmxqOkxFnrcTQUO3zjwOoqRMly4Jgo7z7lA87hHYR/atC9gEanhapxw17qhqz4riT0/aOBwKkxbKcCVeB0CJoaBC/fHwSx3sUbXWcOoiCVODzIybNDo0XUDTyuNHliUyuuuDC2J08UQsAjqfDICmTF+8vf+ytdeIOO1tvFYDLqYDIYYTRoYdDpvON0dKfG8ei01Y/P0eu1UX0FITUuoYYoH98cUP4rEMvHR+l0WiQmesdHSbLqv//UlYqnxlb5g5UKf/jyBTH/AHb/48tbroT3Sj/f8TTlAcvfpXza/wEhC4HTKlTZugVNpXAWzSIehlRVxZIlS/Duu++itLQUffv2xYMPPojWrVtX2nfx4sVYsmRJlccZM2YMnnjiiYBtQghMnDgRHo8Hr732Wp2UnyLH7pSQnW+HKgRsVmOki+OnqCp++CUX3/7vmH/yxEvOb4k+GbE3eaIQApKsegOQpAIawGjQIc5kgMWkh7E8uOjKx+n4wk6svU4Uu7S+9HAavV6L+DgjJHftrhSsOCC9YqA6fSLO0+e08t8G/EFL8Qeuiq1b4tQ8Wb5je294W7zKjwFxKliVt2mdeg10GsRbIzunWsTD0NKlS/Hmm2/iySefRHp6OubNm4eJEyfio48+gtEYeIK75ZZb8Pe//z1g2yuvvIK33noLN910U6Vj//vf/8b333+PrKysuqwCRUCZS0J2nh2SrETVxIRHyidPzIvhyRNlWYVbVuDxqFAhYNDpYDbq0CTJjDiTAWajLqKrUxPFkoAB6XWkUutWFQHr9DBWsXVLq9UgwWaGPoLjoyIahjweD1auXIl7770XgwcPBgAsXLgQAwYMwOeff46RI0cG7G+1WmG1Wv23d+/ejVWrVmHu3LnIyMgI2Hfv3r14/vnn0bNnz7quBtUzp1tGdp4dbo+MxPjoaBGK5ckTFdXb6uP2KN6WMJ0WJqMOyclmxFn0MBu9V/c19teBKFZV17pVU3q9FsnJVhQWOiI2zi+iYWjPnj1wOBzo37+/f5vNZkOXLl2wffv2SmHodHPmzEGfPn1w1VVXBWx3u9249957MXXqVPz66684cuRIWMobzRP4hYNvnEU0j7dwexQcK3DAJSlISTSHfIL1Lc8RjmU6Dhwpxkeb/kBhqRsA0L1DKi7r1wZWS/SMYQpnfVVVwCMrcHsUyIoKrVYLk0GLpilxSLAYYDZ6A1Ckuroawuc4XGKprkBs1Zd1rV8RDUO5ubkAgObNmwdsT0tL899Xna+//ho//fQTPvjgg0r3zZs3D2lpaRg3bhxmzZoVlrJqtRokJ1vPvmMjYLNF5wBfj6Tg+NFiqBot2rRICstl0dZa9FM7XBI+3HgA2387DgBISjDhb0M7oUu71FqXq66EUl/foGeXR4ZHVqHRaGCxmNA01YBEqwkWsx4Wkz7qro6L1s9xXYilugKxVV/WtX5ENAw5nd5xFaePDTKZTCguLj7jY1955RUMGTIE5557bsD2b7/9Fh999BHWrVsX1mZ5VRUoKSkL2/GikU6nhc1mQUmJs3yiuughKyoOHy9FYYkLyQlm2O2uWh1Pp9XCajXB4XBDUYOrqxACvx4qwPrNh+FwydAAyOrSDJec3womow4lpc5ala0uBFtfSVbhlhR4JAVCeOfssZj0SLYaYTF5w49BrwUgILsllLqluq9EDUXz5zjcYqmuQGzVl3UND5vNUqMWp4iGIbPZDMA7dsj3PeDt5rJYqk+IR48exdatW7F8+fKA7QUFBbj//vvx8MMPo1mzZmEvb6zMWaIoalTVVVZUHMm340SxC0kJRu+SCkp4Btp516CqeV3PNnlitP/Sqq6+iuINP27Ju/CmQaeF2aRHWpIFcSY9zOVXflX8AyOaPiNVibbPcV2KpboCsVVf1rV+RDQM+brH8vLy0KZNG//2vLy8SgOiK/ryyy+RkpKCiy66KGD7xo0bkZ+fj/vvvx/3338/AG/QUlUVvXr1wscff4wWLVrUQU2orqiqwLGTDpwodiEx3hiWMS+hqGryxAHdm+OibulR1z1UE6oq/C0/siKg02lgMujQNMkMq9noHfRs1HGGZiKKCRENQ5mZmYiPj8fWrVv9YaikpAS7d+/GuHHjqn3cjh07kJWVBb0+sPjDhg1D7969A7bNnz8fubm5mD9/PtLS0sJfCaozqvAGobxCJ2xWY8RCx4kiJz764TCy8xru5ImqEHC5ZZQ4PHC5ZWi0GpgNOiTFmxFvMcBi8i5jwvl9iCgWRTQMGY1GjBs3DvPnz0dKSgpatmyJefPmIT09HcOHD4eiKCgoKEBCQkJAN9ru3btx9dVXVzpefHw84uMD13qyWq0wm81o27ZtndeHwkcIgeMFZcgtKENCnKF8fEr9asiTJ54+2aFWr0GqXod4iwFpSRZYTN4rvhpiqxYRUbhFfNLFqVOnQpZlzJ49Gy6XC3379sWKFStgMBiQk5ODSy65BE888QTGjBnjf0x+fj6SkpIiV2iqU0II5Bc5cexkGeLjDBGZoK8hTp5Y1WSHFpN3skOb1YT0ZjaU2V0xM/6AiKimNIJL4taIoqgoKHBEuhh1KhomvgKAE8VO5OSVr0BfRwuv6nRa2BIsKCkNvHrBIyn4+qej2PZb9E+eWN1khwkWY6XJDqPlva0PrGvjFUv1ZV3DIyXFGv1XkxGdrqDEhSP5DhgNdReEqnPgaDE+/uEwiuweAEC39ikYntU66haAFUKgxOGBKgCTQYcUm3fcj9moh9nEQc9ERMFiGKKoUWx3IyffAb1Ogzhz/X00y1wyvtiejf8dOAkAsFmNGNG/LTq1Sqy3MtSUEALFdg+MBh1aNo1HvEUfsSvsiIgaC4YhigolZR5k5zug0QhYLfWz3pgQArsOnvRPnggAWeemYUjvljBF4UKiQggUlnpgNevROi0ecVHWYkVE1FAxDFHE2Z0ScvLsUFQVidb6CUIlDg/e+foAfj3obQ1qmmTGyAtPTZ4YbVRVoMjuRkKcEa2axsNSz12IRESNGX+jUkSVuWTk5NvhkRQkJdTPlVplLhkrP/4NhaVu6LQaXBzlkycqqorCUg+S401o1TQeJmP0tVoRETVkDEMUMU63Nwg5XTKSEuqnRUhVBdZ+exCFpW6k2Mz4x6UdkWozn/2BEaIoKorsHqTaTGjZJD4i0wwQETV2DEMUEW5JQU6+HfYyCck2Y71dtv7Vjzk4eLQEBr0WE644D/FmXdSuJybLKoodHjRJNKNFk/iITDxJRBQL+NuV6p0kKziSZ0eJw4OkhPoLQrsOFmDzruMAgNED2qFF0+gcHwR45zsqdniQlmxBy6YMQkREdYktQ1SvvCvQO1BgdyMlwVRvy1rkFpRh3aY/AAAXdk1H1/ap9fK8oXBLCuxlEtJT4tA81Rr1S38QETV0DENUb7xByI6TJS4kJxjr7SRf5pLxzob9kBUVHVrYMLR3y3p53lC43DLK3ApaNLGiWUocJ1AkIqoHDENUL1TVuwL9iWIXEuON9TZRoKoKrNl4AEV2D5ITTBgzqH3UtrSUuWR4JAWtmlrRNMkSdct/EBE1VgxDVOdUIZB7sgx5hU7YrMZ6vYT9q505OHSsFAa9FtcM7RC18/M4nBJkRaBVWjxSbWYGISKiehSdZwZqNIQQOF5QhtzCMiTEGep1IPAvB09i86/eAdOjLj4HzZLj6u25g1Fa5oEQGrROi0dKFF/mT0TUWDEMUZ0RQiC/yIljJ8tgNevrdY6c3JNl+GjTYQDARd3S0eWclHp77mAUOzzQabVok2ZFYnz9TDpJRESBGIaozpwsceHICQfiTLp6nTW5zCXhbd+A6ZY2DOkVfQOmhRAosUswGLRolRYPW1z9TDpJRESVMQxRnSgsdeNIvgMmgw7mehyno6oC7208iGJH+YDpgdE3YFoIgaJSDyzlC65aueAqEVFEMQxR2BXb3cjOs0Ov0yDOXL8fsS935uCP8gHT1w7tGHUDplXVG4Ti4wxo1TS+3l8fIiKqjL+JKaxKyjzIzndAoxGwWuq36+eXgyexpcKA6bRkS70+/9moqkBhqRuJVhNapVlhNvLHj4goGvC3MYWN3SkhJ88ORVWRaK3fIHTsZBk+Kp9hOhoHTPsWXE2ON6FlWjxMXHCViChqMAxRWJS5vCvQS5KKxHpagf7Uc0vlM0wLdIzCAdOyoqLY7kGqzYyWTa0w6BmEiIiiCcMQ1ZrL4w1CTpeMpHoOQqcPmL4qygZMS7KKEoeEpkkWtGhirdcJJ4mIqGYYhqhW3JKCnDwH7GUSkm31twK9zxc7onfAtEdSUOqU0CzFguap1npbgoSIiIITPWcOanAkWcWRPDuKHW4kJ5jqPQj9fOAktu72DpgefXG7qBow7fYoKHPJaJ5qRXpyXFS1VhERUSCGIQpZscONQrs3CNX3yf7YSQf+88MfAICLu6fj3HOS6/X5z8TpluFyK2jeJA5pyVx5nogo2jEMUUh8Ewca9Np6D0IOl4R3NhzwD5ge3DN6BkyXuSR4JBUtufI8EVGDwTBEIXG6ZThcEqz1PGmgqgqs+cY7YDolymaYtpdJUFWBNs0SuOAqEVEDwjBEIbE7JciKCn09rkIPAF/syMYfuaUw6rW4ZmjHel3q40xKHB5oNBq0bpaA5AQuuEpE1JBEx5mEGhTfTMr1ufgq4BswnQcAGDUgOgZMCyFQ7PDAoNehddN42Op5skkiIqo9XutLQXO4JJS5ZVhM9ReGAgdMN8e5bSM/YFoIgWK7ByaDHm2bJTAIERE1UGwZoqCVlkkQAvU2b07AgOlWiRjcs0W9PO+ZCCFQWOqBtXzl+TiuPE9E1GAxDFFQZEVFkd0Ncz11kSmqivd8A6ZtJowZ0C7iA6ZVVaDI7kZCnBGtmsZH1USPREQUPHaTUVAcTgluj1JvYeiL7Tk4XD5g+tooGDCtqCoKyleeb5OWwCBERNQI8Dc5BaXY4QE0qJfWmf/tP4Ftv3kHTI8e0A5NkyI7YNq38nyqzYRWTeO54CoRUSPBMEQ15pYUlJRJ9TJw+ugJB/7zw2EAwIAezZEZ4QHTsqyi2OFBk0QzWjaN54KrRESNCH+jU415u8hkmAx1G4YcTgnvbNgPRRXoFAUDpj2SgmKHB2nJFgYhIqJGiC1DVCNCeAcN6/XaOl1iQlFVvPvNAZSUSUi1mXDVwHYRXdLCLSmwl0lonmJFeioXXCUiaowYhqhGXB4FdqcMSx0PnP5iew7+PG6H0VA+w7Qxch9Rl1tGmVtBiyZWNEvhgqtERI0V2/upRhxOCZKswFiHXWT//f3UgOmrIjxguswlw+VR0KqpFekMQkREjRpbhuisVCFQaHfDaKi77HzkhAMfb/YOmB7Yozky2kRuwLTDKUFWBFqlxSPVZubK80REjVzEW4ZUVcWiRYswYMAA9OzZE5MmTUJ2dnaV+y5evBgZGRlVfs2aNcu/35o1a3DFFVegZ8+eGD58OJYvXw5FUeqrSo2Ob4X6uppTx+6U8G75gOnOrRMxKIIDpkvLPFBUoE2zBDRJtDAIERHFgIiHoaVLl+LNN9/E3LlzsXr1aqiqiokTJ8Lj8VTa95ZbbsH3338f8DVhwgTExcXhpptuAgCsW7cODz30EMaNG4d169bh7rvvxosvvohly5bVc80aj9IyDxRF1MlVVN4Zpn0Dps0YPSByA6aLHR5oNFq0bRbPleeJiGJIRMOQx+PBypUrMXXqVAwePBiZmZlYuHAhcnNz8fnnn1fa32q1omnTpv6v/Px8rFq1Cg8++CAyMjIAAG+99RZGjx6Na6+9Fm3atMHll1+OW265Be+99159V69RUFQVxXYPzHU0t9Dn204NmL52aIeIDJgWQqC41AODzhuEEuMZhIiIYklExwzt2bMHDocD/fv392+z2Wzo0qULtm/fjpEjR57x8XPmzEGfPn1w1VVX+bfde++9SElJCdhPq9WiuLg4vIWPEQ6XjDKXjMT48K/I/t/fT2D7Ht+A6fZoEoEB06oQKCr1wFK+4KqVC64SEcWciIah3NxcAEDz5s0Dtqelpfnvq87XX3+Nn376CR988EHA9vPPPz/gdmlpKd566y0MGDCg1uXV6yPeq1indOXdYLoK3WFlLhlanQaGMF9FdiTfjvVbvAOmB/dqgS7tUs7yiPDSabVQVYESuwe2eCPaNGvc64xV9d42Vqxr4xVL9WVd61dEf/s7nU4AgNEY2OpgMpnO2pLzyiuvYMiQITj33HOr3cfhcOD222+H2+3G9OnTa1VWrVaD5GRrrY7RUNhs3hYaSVag5DuQlhqPuDC2mJQ6PHh7w37IikDX9qm4YmDHer90XRUCJ4udaJ6WgHNaJDbqIFSR772NBaxr4xVL9WVd60dEzwBmsxmAd+yQ73sAcLvdsFiqf1GOHj2KrVu3Yvny5dXuk5+fj8mTJyMnJwcrVqxAq1atalVWVRUoKSmr1TGinU6nhc1mQUmJ07soaakbJwocSEowQZbksDyHrKhY9cleFNu963xdcdE5sNtdYTl2MOxlElKS49DEZoKrzA1Xmbvey1CfTn9vGzPWtfGKpfqyruFhs1lq1OIU0TDk6x7Ly8tDmzZt/Nvz8vL8A6Kr8uWXXyIlJQUXXXRRlfcfOHAAEydOhKqqeOONN9CpU6ewlFeWG/cH0kdRVMiyipPFTghVQKgCCkRYjv3plsM4fLzUO8P0kA4w6DT1/oPulhTIioqWafGArMTM+wqcem9jAevaeMVSfVnX+hHRzsjMzEzEx8dj69at/m0lJSXYvXs3+vbtW+3jduzYgaysLOj1lbNcdnY2brzxRlgsFqxevTpsQSjWuD0KSsskWMzhy8veAdP5ACI4YFoVsJdJSEuxIIlXjRERESLcMmQ0GjFu3DjMnz8fKSkpaNmyJebNm4f09HQMHz4ciqKgoKAACQkJAd1ou3fvxtVXX13lMe+//354PB4888wz0Ov1yM/P99/XtGnTOq9TY+FwSXBLCqyW8HxEjuTb/TNMD+rZAhltksJy3GCVODywWY1olhzHCRWJiAhAFCzHMXXqVMiyjNmzZ8PlcqFv375YsWIFDAYDcnJycMkll+CJJ57AmDFj/I/Jz89HUlJSpWMdP34c27ZtAwCMGjWq0v179+6ts3o0JkIIFJa6YQjTCvX2MgnvfH0AiiqQ0ToJA3s0P/uD6oDLLUOr1aJ5qrVOJpAkIqKGSSOECM9gkEZOUVQUFDgiXYw6pddrkZxsxZFjxdjzRwEsZh0M+tpdUq8oKlZ9tg/ZeXY0STRjwohzYarjle+rLIeqoqjUg1ZN49EsJc5f18JCR0z0x8dSfVnXxiuW6su6hkdKirVGA6j55zFV4nBKkBRR6yAEAJ9ty0Z2nh0mgw7XDO0YkSAEACV2CckJJjRJMp99ZyIiiikMQxRAUQUKSl0whWGF+p/25WPHXu+YrdED26FJYmSCSJlLgsGgQ3qKFTotP/JERBSIZwYK4HBKcLhkWGq5FllOnh3rt/wJwDvDdEbrpDCULniyosLlUZGeYkFcGK+MIyKixoNhiAKU2N0QQtRqWnR7mYR3vykfMN0mCQO6R2bAtBACxXYPUm0mpNjYPUZERFVjGCI/WVFxssRVq3E9iqLi3W8OoLRMQpNEM0YPaBexS9jtTgkWkx7NUuLqfbkPIiJqOBiGyM/hlOB0S7AYQ+9O+rTCgOlrh3aEKcwLvNaUJCuQFYHmqVaYa1EfIiJq/BiGyK+kzANAA602tFaUH/flY2f5gOmrBrZDaoQGTAshUOKQ0DTJjKR449kfQEREMY1hiAB4W1KK7R5YTaGtTp+TZ8cnFQZMd47QgGkAKCmTEG8xcJZpIiKqEYYhAgDYnTJcHgXmEK4iKy3z+GeYzozggGnAuwgrBNA81RqWeZKIiKjxYxgiAECx3Q2dThN0S4pvwLTd6e2WGhXBAdO+RVibJltgs7J7jIiIaoZhiODyyCgp8155FaxPt2YjJ89xaobpCA2YBoASh4REqwlpSZaIlYGIiBoehiGC3SlBkpWgg8yP+/Kxc593wPSYQe2QGsG5fJxuGVqtBumpcVyElYiIgsKzRowTQqCo1AODPriPwslil3+G6SG9WqBTq6Q6KF3NKKqKMpeM9BQL4i2hDQAnIqLYxTAU45xuGQ6XhLggu8h+OXgSqipwTnoCLo7ggGkAKLZLSLGZInYpPxERNWwMQzGutEyCrKjQB9kytPfPIgBAj46pEb183eGSYOIirEREVAs8e8QwVRUosruDXn6jsNSN44VOaDSIaPeYLKvweFSkp8SFNPibiIgICCEM/fTTT3VRDooAh0uCwx38CvV7DhcCANo2S4jYSvBCCBQ7PEixmZBsM0WkDERE1DgEHYb+8Y9/4LLLLsNLL72EvLy8uigT1ZNShwQIBN29tKe8iyyzbVL4C1VDdqeEOLMB6SlWLsJKRES1EnQYev3119GnTx+8+OKLGDJkCCZNmoRPP/0UkiTVRfmojsiKiiKHG+Ygu8jsTgnZeXYAQEab5Loo2ll5JAWKIpCeEhd0Fx8REdHpgg5Dffr0waOPPopNmzbhySefhKqquOeee3DxxRdj7ty5+PXXX+uinBRmDqcEt0cJuovMN3C6RWocEiMwy7MQAqVlEpokWbgIKxERhUXIA6hNJhOuuOIKrFixAh999BE6d+6MN954A2PHjsWYMWOwfv36cJaTwqzY4QE0CPpKsD1/escLZbSNTKtQicODhDgjmiVbuAgrERGFRcijX51OJz7//HN8+OGH2LZtG8xmM6699loMHjwY33zzDe677z7s2rUL06dPD2d5KQzcklK+/EZwrUIuj4xDx0oBAJltkuqgZGfm9ijQQIP0lDguwkpERGETdBj64Ycf8OGHH+KLL75AWVkZ+vbti0cffRSXXXYZzGbvpHdDhgyBRqPB6tWrGYaikLeLTIY1yKuw9ucUQ1UFUm1mNK3n9b9UVcDulNCiiZWLsBIRUVgFHYZuueUWpKWlYfz48bj66qvRpk2bKvfr0KEDLr744loXkMJLCO/cQnq9NoQusiIAkbmKrMThQWK8qd5DGBERNX5Bh6EXX3wRAwYMgLbC5diKokCnC+y2GD9+PMaPH1/7ElJYuTwK7E4ZliCvwpJlFftzigHUfxeZ0y1Dp9OiORdhJSKiOhD0mWXQoEF4+eWXceutt/q37dixAxdffDFef/31sBaOws9RvkK9McgV6g8eK4FHVpEQZ0CLJtY6Kl1liqKizC2jWbIFVjMXYSUiovALOgytXLkSzz77LM455xz/tjZt2uCyyy7Dk08+iXfffTec5aMwUoVAQakbRkPwrSv+LrI2SfV6FVexXUJKghlNEtk9RkREdSPobrLVq1fj7rvvDmgZat68OWbPno0mTZrg1Vdfxd/+9rewFpLCo8wlo8wtId4SXAuLqgrsKw9D9TnRosMpwWTSIT0lDlotL6MnIqK6EXQTwfHjx9GtW7cq7+vRowdycnJqXSiqG3anB4oigh5382eeHWVuGWajDm3T4+uodIFkWYVbUtGci7ASEVEdCzoMtWzZEps3b67yvu3btyM9Pb3WhaLwU1QVxXYPzEHOLQQAe8sXZu3cOinodcxC4VuEtUmiGUkJXISViIjqVtB/cl9zzTWYN28eJEnCpZdeitTUVBQUFODrr7/GK6+8gnvuuacuykm15HDJKHPJSAxyCQshRMB4ofpgd0qwWgxolhLHRViJiKjOBR2GbrrpJhw/fhyvvfYaXn31Vf92nU6HG2+8ETfffHM4y0dhUurwAEDQY29yC5wodnig12nRoaWtLooWwCMpkBWB1mlxMAV5xRsREVEoQhqMMWPGDNx+++346aefUFxcDJvNhu7duyM5OTLrVdGZSbKKYkdoXWS+tcg6trTV+RIYvkVY01MiswgsERHFppBHpiYkJGDgwIGVth88eBDt27evVaEovBwuCU6PjJQQxt/sPVwEAMish4VZfYuwpiXHcRFWIiKqN0GHoeLiYixcuBDbtm2Dx+OBEAKA96/6srIyFBcX47fffgt7QSl0xXY3dBpN0AGjoMSFvCInNBqgU6vEOiqdl8sjexdhTY2DQc9ZpomIqP4EfdZ5/PHH8d5776Ft27bQ6XRISEhAt27dIEkSSkpKMGfOnLooJ4XI7VFQWibBYg6+EdA3cPqc9IQ6vbxdVQUcThlNUyywxbF7jIiI6lfQYei7777DnXfeiWXLluHaa69Feno6nn32WXz66afIyMjA/v3766KcFCKHS4JbUmAMobVlT/kl9Zl1PNFisd2DpHgTmnKWaSIiioCgz5AlJSXo1asXAO/K9Lt27QIAWK1W3HLLLfjmm2/CWkAKnRAChaVuGEJYob60zIOcfAcAIKMOL6kvc8kw6LVI5yKsREQUIUGffZKTk1FaWgoAOOecc3Dy5EkUFRUBAJo1a4bjx4+HtYAUOqdbgcMphdTFtbe8i6xFEytsdXRll6KocHpkNEvhIqxERBQ5QYeh/v3744UXXsCRI0fQpk0bJCYm4v333wcAfP3110FfXq+qKhYtWoQBAwagZ8+emDRpErKzs6vcd/HixcjIyKjya9asWf79Nm/ejDFjxqBHjx647LLL8PHHHwdbzUbB4ZIgKSKkAcn1MdFisV1CaoIZqTZ2jxERUeQEfZacOnUqTp48iRkzZkCj0WDy5Ml46qmn0K9fP7z66qu4+uqrgzre0qVL8eabb2Lu3LlYvXo1VFXFxIkT4fF4Ku17yy234Pvvvw/4mjBhAuLi4nDTTTcBAA4cOIDJkydjwIABWLt2Lf72t79h+vTp1S4h0lipqreLzBTCCvVOt4w/jnlb/+rqknq7U4LZpPPOMs1FWImIKIKC7j9p1aoV1q9fjz/++AMAcPPNN6NJkyb48ccf0b17d1x11VU1PpbH48HKlStx7733YvDgwQCAhQsXYsCAAfj8888xcuTIgP2tViusVqv/9u7du7Fq1SrMnTsXGRkZAIB///vfyMjIwLRp0wB4xzXt3r0bL7/8Mvr37x9sdRusMreMMpeEhLjgu59+zy6CKgSaJJrRJNEc9rJJsgqPpNb5VWpEREQ1EXSzwYQJE/DTTz8hMzPTv+2KK67AQw89FFQQAoA9e/bA4XAEhBSbzYYuXbpg+/btZ338nDlz0KdPn4Dn3bFjR6XQc8EFF2Dnzp3+OZFiQWmZB6oAdCEMSv7NdxVZ26Qwl4qLsBIRUfQJ+s/yH3/8MWyzA+fm5gIAmjdvHrA9LS3Nf191vv76a/z000/44IMPKh0zPT290vGcTicKCwuRkpIScnn1DWQyQFlRUeqUEGfRBxWGdFotPLKC33OKAQBdzkkJKUydSYnDg8R4I1qmxcMYwbXHfPUKd/2iVSzVl3VtvGKpvqxr/Qo6DA0YMADr1q3D+eefD4OhdlcAOZ1OAIDRGHi1kslkQnFx8Rkf+8orr2DIkCE499xzA7a7XK5Kx/PdrmocUk1ptRokJ1vPvmMUKCx1QWfQoWmCJejxOLsOnIAkq0hKMCGzXZOwLovh9iiwCA06t05Csi383W+hsMXY4O1Yqi/r2njFUn1Z1/oRdBgymUxYt24dPvnkE3To0AFxcXEB92s0Gvz73/+u0bHMZu8J0ePx+L8HALfbDYul+hfl6NGj2Lp1K5YvX15l+U4PPb7bZzrm2aiqQElJWciPr0/ZeaVw2N0wBhmEdFotfjlwAgCQ0ToJpXZX2MqkqgIFpW60SI2DkGUUFjrCduxQ6HRa2GwWlJQ4oShqRMtSH2Kpvqxr4xVL9WVdw8Nms9SoxSnoMJSbm+ufdBFApXE4wYzL8XWP5eXloU2bNv7teXl5/gHRVfnyyy+RkpKCiy66qMpj5uXlBWzLy8tDXFwcEhISaly2qshy9H8gPZKCgmI3jHpt0B8qRRX49eBJAEDn1olh/VAW2d2INxuQkmCGoggA0TF+S1HUBvG+hkss1Zd1bbxiqb6sa/0IOgy99tprYXvyzMxMxMfHY+vWrf4wVFJSgt27d2PcuHHVPm7Hjh3IysqCXl+5+H369MG2bdsCtm3ZsgW9e/eGVtv4+14dLhkuKbQV6v/MLYXDJSPOpEfbZrULjhW53DK00CA9hYuwEhFR9InomcloNGLcuHGYP38+vvrqK+zZswfTpk1Deno6hg8fDkVRkJ+fD5crsLtm9+7dAVezVTR+/Hj8/PPPmD9/Pg4cOICVK1fi008/xcSJE+ujShFXbHdDrw1+hXrg1FVkndskhW3uH0VVYXfJSEuxIIGLsBIRURQKumVo6NChZz3RfvXVVzU+3tSpUyHLMmbPng2Xy4W+fftixYoVMBgMyMnJwSWXXIInnngCY8aM8T8mPz8fSUlJVR6vU6dOWLp0KebNm4d///vfaNWqFebNmxcTcwy5PDJKykJbfkMI4V+Y9dwwTrRYYpeQEm9CEy7CSkREUSros2ZWVlalMORwOPDLL7/A7XbjxhtvDOp4Op0O9913H+67775K97Vq1Qp79+6ttP1///vfGY85cOBADBw4MKhyNAZ2pwRJVmCzBn+V37GTZSh2eGA0aNGhZWJYylPmkrgIKxERRb2gw9CTTz5Z5XZJknD77bf7L5en+iWEQFGpN8yEwr8WWdsUGEIYfH06WVHhdCtom56AOC7CSkREUSxsf64bDAbccMMNeO+998J1SAqC0y3D4ZJgMYa2vMXeP71dZN07Nql1WYQQKHF4kJpoRkpCdMwnREREVJ2w9l0UFxfD4Yjs/DGxqrRMgqyoIc2SfbLYhfwiF7QaDbqck1rrsjicMsxGPdK5CCsRETUAQTcjnL78BQAoioLc3Fy8/vrr6NOnTzjKRUFQVYEiuxtmY2jLW+wpbxVq1yIBFrMekiSFXBZJViDJKs5pboM5xFYqIiKi+hT02WrmzJnV3terVy/861//qlWBKHgOlwSHW0ZiCAOnAWDP4SIAQGYtryLzdo/JSEs2Iymel9ETEVHDEHQYquqyeY1Gg/j4eNhstrAUioJT6pAA4V1OI1glDg+OnPB2bWa2qV0YKi2TEG/Ro1lyXFjXNCMiIqpLQZ89W7ZsCUVRsHnzZrRs2RItW7aE0+nECy+8gKNHj9ZFGekMZEVFkSP0LrK92UUAgJZNrbBZQ2/NcUsKVCGQnmqN6Gr0REREwQo6DP33v//F6NGjsWLFCv+2kpISrFu3DldddRX27dsX1gLSmTmcknc1eFOI44XKJ1rMbJMUchlUVcBeJiEtOQ62OF5GT0REDUvQYWjBggXo3bs33n//ff+2Xr164auvvkL37t3x9NNPh7WAdGbFDg+gQUjdUk63jMO5dgC1Gy9U6pBgsxqRlmRh9xgRETU4QYehX3/9FRMmTIDZHDh/jMlkwo033njW2aEpfNySUr78RmitQr/nFEMVAk2TzEi1hTYfkNMtQ6PVoHmqlbNMExFRgxT02ctsNuP48eNV3ldYWBgTK8NHC28XmQxTiGN0TnWRhdYqpKgqylwymiVbEG9h9xgRETVMQSeXAQMGYNGiRZXWDDtw4AAWL14ck2uCRYIQ3rmF9HptSF1Tkqxg/5ESAEBm26SQylDmkmGzGtEkibNMExFRwxX0pfX33nsv/v73v+Oqq65Cq1atkJKSgsLCQmRnZ6NVq1aYPn16XZSTTuPyKLA7ZVhCvIrswJESyIqKRKsR6SlxIR3DI6lolmIK6ZJ+IiKiaBF0GGratCk++ugjrF27Fj/++COKiorQrFkzjBs3DmPGjIHVaq2LctJpHE4JsqzAGOpEi76FWdskhdSypKoCWq0GcSbOMk1ERA1bSGcys9mMPn36YNy4cQCA/Px87N69GyaTKayFo6qpQqCg1A1DiCvUK6qKfeXzC2WEeBWZW1JgMuhCnt+IiIgoWgR9Nj1+/DhGjRqFO+64w79t9+7dmDx5MsaNG4eioqJwlo+qUOaSUeaWYAmxVeZwrh0uj4I4kx5t0uJDOobboyDeYuAVZERE1OAFfSZ7+umn4fF4MH/+fP+2QYMGYe3atSgqKsKCBQvCWkCqzO70QFFEyEFkb/nCrJ3bJIW8qryigleQERFRoxD02fSHH37Avffei549ewZs79KlC+666y58/fXX4SobVUFRVRTbPTCHOLeQECJgvFAoJFmFQacJuWWKiIgomgQdhjweD3S6qk/EFosFDoej1oWi6jmcMspcMizG0ILI0RMOlJZJMOq1aN88tIV13R4FZpMeJo4XIiKiRiDoMNSjRw+88sorkCQpYLssy1i1ahW6d+8etsJRZaVlHgAIuXvL1yrUsVUi9PrQutk8kgqb1QAtl94gIqJGIOjmhalTp2L8+PG45JJLMHDgQKSmpqKgoACbNm3CyZMn8dprr9VFOQne7qliR+hdZACwt5ZdZEIIQAPEmTheiIiIGoegw1DPnj3x9ttv44UXXsA333yDoqIiJCQkoE+fPrj99ttx7rnn1kU5CYDdKcHpkZGSENoUBieKnDhR7IJWq0HHVokhHcN3SX2o66ERERFFm5AGnnTp0gWLFi2q8r5Dhw6hXbt2tSoUVa3E4YZOowl5ZXhfF1m75gkwhzjmyC2pSIwzwKBnGCIiosYhLJPEyLKM9evX44YbbsDll18ejkPSadweBaVlEizm0K/g2vNn7RZmBQBZVpEQx8k1iYio8ajVtdHZ2dl45513sHbtWpw8eRJWqxWjR48OU9GoIodLgltSYLWE9paVODw4eqIMAJDROimkY8iKCr1Oyy4yIiJqVII+s6qqig0bNuCtt97C5s2bIYRAnz59MHPmTAwbNgxmM1cwDzchBApL3TCEuEI9cKqLrFWaFfFxoQ1+dksKzEZdyF1sRERE0ajGZ7Xjx4/j7bffxnvvvYe8vDy0bdsWkyZNwvLlyzF16lT07du3LssZ05xuBQ5n5LvIPB4VycnmkC/rJyIiikY1Orvedttt+O6772CxWPCXv/wFV111Fc4//3yUlpbixRdfrOsyxjyHS4KkCNhCnBeozCXjcG4pgNpdUq9CIC7EbjoiIqJoVaMz29dff42MjAxMnz4dF1xwQbUzUFP4qaq3i8wU4gr1APB7ThGEANKSLUixhdaNKckqTHodl+AgIqJGp0Zn2Dlz5sBisWDixIm48MIL8eijj+K3336r67IRgDK3jDKXVKtBy3sOFwEIvVUIAFweBRaTHiYDgzARETUuNfoz/5prrsE111yDAwcOYM2aNVi3bh3eeOMNtGvXDhqNBna7va7LGbNKyzxQBaALcYV6j6TgwNFiALUbLyTJKmxWY8iPJyIiilZBnWE7dOiA6dOnY+PGjXj++efRrl076HQ6TJkyBddddx3efPNNFBQU1FVZY46sqCgsdcNsDL2L7MDREsiKQFK8Ec1SLCEdQ1UFtFquUk9ERI1TSGdZnU6HoUOH4vnnn8e3336L++67D6WlpZgzZw4GDhwY7jLGLIdLhsuj1OpS9j2HvVeRZbRJDvmyfN8SHGauUk9ERI1Qrf/UT0lJwc0334ybb74ZP//8M9auXRuOchGA0jI3tJrQV6hXVBW/55R3kbVNCrkcbo+CFJsZ+hC76oiIiKJZWPs9unfvju7du4fzkDHLIykotku1ao35I7cULo+COLMerZvGh3wcVQUS4jheiIiIGif+qR+lHC4ZbkmGqRZhaG/5VWQZrZNCbl2SZAV6vZZdZERE1GgxDEWpYrsbOm3oK9QLIfxLcNSui0wtX4KDYYiIiBonhqEo5PLIKCmTanX11pF8B+xOCUaDFu2a20I+jkdSYbMaQg5lRERE0S7os+0HH3xQ7X0ajQZWqxVt2rRB586da1OumGZ3SpBkBTZraAuqAqcWZu3UMjHkgc+qKgANEGcKvRxERETRLugw9MADD0BVVQDerhgfX8uBEAIajQb9+vXDsmXLYLGENrdNrFKFQFGpB8ZaLL/h7SIrX5i1bS0WZpUUmIxcgoOIiBq3oM+4L7/8MiwWC6ZNm4YNGzbg559/xtdff40ZM2bAYrHg8ccfx7Jly/DHH39g0aJFZz2eqqpYtGgRBgwYgJ49e2LSpEnIzs6udn9JkrBgwQL//uPGjau0NMgPP/yAq6++Gj179sSll16KFStWBFvNiHG5ZThcEiy1mFvoRLELBSXeMUcdWyaGfBy3pMBq0sMQ4gKxREREDUHQZ7mnnnoKkyZNwq233ooWLVrAaDSiefPmuOmmm3D77bfj9ddfx+DBg3HnnXfis88+O+vxli5dijfffBNz587F6tWroaoqJk6cCI/HU+X+Dz/8MNauXYvHH38ca9asQUpKCiZNmoTSUu+q7AcPHsTkyZMxZMgQfPTRR/jnP/+JRYsW4Y033gi2qhFRWiZBVlToaxFAfBMttmthq9XVaLIieEk9ERE1ekGfcQ8ePFjtXELnnnsu9u/fDwBo27YtTpw4ccZjeTwerFy5ElOnTsXgwYORmZmJhQsXIjc3F59//nml/bOzs7FmzRo89thjGDBgADp06IBHH30URqMRu3btAgB8++23iIuLwx133IHWrVvj8ssvx4ABA/Ddd98FW9V6p6gqiuzuWl+55b+KrBYLs8qKCr1OW6sFYomIiBqCoPtiWrdujc8++wwXXXRRpfu++OILNG/eHACQm5uLlJSUMx5rz549cDgc6N+/v3+bzWZDly5dsH37dowcOTJg/02bNiEhISFgyQ+bzYYNGzb4b6empqKoqAj/+c9/MGLECOzbtw87d+7EDTfcEGxVK6lNa01NOMtkuCUFtngjdNrQnqvI7saxk2XQADj3nJSgFnj1PadOq4WsCMRZ9IiPM4Y8R1E0870uoS6A29DEUn1Z18YrlurLutavoMPQxIkTMWvWLJw8eRJ/+ctfkJqaihMnTuDLL7/El19+iTlz5uDQoUN49tlnz7pOWW5uLgD4A5RPWlqa/76KDh06hNatW+Pzzz/H8uXLcfz4cXTp0gUzZ85Ehw4dAAB//etfsXXrVtx3332YPn06FEXBFVdcgf/7v/8LtqoBtFoNkpOttTrG2ZS6VVitJiQnxoV8jP/uPwkAOKeFDS3SQruk3mo1oUxW0bJJPFJTQ5+5uiGw2WJrgH8s1Zd1bbxiqb6sa/0IOgxdddVV0Gg0WLRoEb766iv/9jZt2mDevHkYOXIkPv74Y3To0AH33HPPGY/ldDoBAEZj4LgUk8mE4uLiSvvb7XYcPnwYS5cuxfTp02Gz2bBs2TJcd911WL9+PVJTU3Hy5EkcOXIEU6dOxaBBg7B792489dRTWLx4MaZOnRpsdf1UVaCkpCzkx5+NrKj482ghoNGgpNQZ8nF+2pcHAOjcKjHo4+i0WlitJtjtLthLXVASTSgsdIRclmim02lhs1lQUuKEoqiRLk6di6X6sq6NVyzVl3UND5vNUqMWp5AuWRo9ejRGjx6NP//8EwUFBUhPT0d6err//hEjRmDEiBFnPY7ZbAbgHTvk+x4A3G53lZfk6/V62O12LFy40N8StHDhQgwaNAjvv/8+Jk6ciAceeADNmzfHbbfdBgDo0qULhBB4+OGHMW7cuLN23Z2JLNfdB7LY7obDKSM5wRjyh6HMJeFwrncgeefWSSEfx+mWoddqYNBp67TO0UBR1EZfx4piqb6sa+MVS/VlXetH0B10o0ePxquvvooTJ06gTZs26NmzZ0AQCoaveywvLy9ge15eHpo1a1Zp//T0dOj1en8QAryBqnXr1sjJyQEA7Ny5E926dQt4XM+ePSHLsn+faFTs8ECjQa1met6XXQwhgGbJFiQnmEI+jltSYDHpYTJw8DQRETV+QYehFi1aYMGCBRg0aBAmTJiAjz76CC6XK6Qnz8zMRHx8PLZu3erfVlJSgt27d6Nv376V9u/bty9kWcYvv/zi3+ZyuZCdnY22bdsCAJo1a4a9e/cGPG7v3r3QaDT+faKNW1JQUiYhrpaTG4ZjokUAkGQVNisvqSciotgQdBhaunQpfvjhBzzyyCMQQmDmzJm48MILMWPGDPzwww8Bs1KfjdFoxLhx4zB//nx89dVX2LNnD6ZNm4b09HQMHz4ciqIgPz/fH7b69Onjf64dO3Zg//79mD59OnQ6HUaNGgUAuPnmm/Huu+9i1apVyM7Oxpdffoknn3wS1113HRITQ5+AsC45nBLcHrlWs057JAUHjpQAqN0l9YqiQqvRcNZpIiKKGSGd8RISEjB27FiMHTsWJ0+exKeffopPP/0UkyZNQpMmTbBx48YaH2vq1KmQZRmzZ8+Gy+VC3759sWLFChgMBuTk5OCSSy7BE088gTFjxgAAFi9ejPnz5+OOO+6Ay+VC7969sWrVKv9YoGuvvRYmkwmvvPIKnnnmGTRr1gzXXXcdJk2aFEpV65wQAkV2N/R6ba26yPYfKYaiCiQnmJCWHPqIfJdHgdnEVeqJiCh2aEQwTTlV2LdvHz755BNs2LABe/fuRefOnbFu3bpwlS9qKIqKgoLwX1nldMv4PacYZqMWxlqM0Vn77UHsOliAC85rhuF9W4d0DJ1OC0kFLAYNmqfU7TQCkabXa5GcbEVhoSMmBifGUn1Z18YrlurLuoZHSoq17q4my87Oxn/+8x+sX78e+/fvR5MmTTBy5Eg89dRTyMzMDOWQMcvhlCDLCoy1WKFeUVT8nu2diqA2XWSAd6FYq5njhYiIKHYEHYauvvpq7N69G2azGcOGDcPMmTPRv39/aMtnL/atWk81U1rmqfXM1odyS72Lqpr1aJ0W+iSJHlmB0WDgeCEiIoopQZ/1kpKS8OSTT2L48OEBcwHl5eXhnXfewZo1a/D111+HtZCNmQqgttlx7+EiAEBGm6RaBVG3R0FCvBlmow6KUqveUyIiogYj6DC0YsWKgNvfffcdVq9ejY0bN0KWZbRq1SpshaOzE0Jgb3YRACCzTe0uqfdIKpITzOWBimGIiIhiQ0j9IQUFBXjvvffwzjvv4MiRI4iPj8dVV12FUaNGoU+fPuEuI51BTr4DdqcEk0GHds0TQj6OqgpoNIDVYoAqyWEsIRERUXQLKgxt2bIFb7/9Nr788ksoioLzzz8fR44cwfPPP4+srKy6KiOdwZ7D3okWO7VKrNWKvx5JgcmgQ5xZDzvDEBERxZAahaFXX30Vb7/9Ng4dOoS2bdvi9ttvx1VXXYW4uDhkZWVxwHSECCGw588iAEBm26RaHcstKUhJtMCg5/xCREQUW2oUhp588klkZGRg1apVAS1ApaWldVYwOrv8IhcKS93QaTXo2LJ2s2vLikBCHC+pJyKi2FOjfpURI0bg8OHDmDx5Mm6//XZ88cUXkGV2pUSaby2y9i1stZqwUVZU6HXaWq+NRkRE1BDV6Oy3YMEC2O12fPTRR1i7di3uvPNOJCcn49JLL4VGo2E3WYTsKb+kvrYLs7o9CsxGHcwmdpEREVHsqfGI2/j4ePzjH//Au+++i48++gijRo3Chg0bIITA/fffj+eeew779++vy7JSBUWlbuQWlEGjATq3rl0XmUdSkWA1QstQS0REMSiky486deqEmTNnYuPGjVi8eDHat2+Pl156CVdccQWuvPLKcJeRquAbON06LR5Wc+hLeQghIABY2UVGREQxqlZnQL1ej2HDhmHYsGE4ceIE3n//fbz//vvhKhudgW+8UK0nWpRVGPVamBmGiIgoRtVuUawKmjRpgkmTJmH9+vXhOiRVw+GSkJ1nB1D7hVndHgUWsx6mWgzAJiIiasjCFoao/uz7swhCAOkpcUhKMNXqWJIskMhL6omIKIYxDDVA4ZpoUVFV6LRgFxkREcU0hqEGxi0pOHi0BEDtxwu5PSqMBh0svKSeiIhiGMNQA7M/pxiKKpCSYELTJHOtjuWWFNisBui0/BgQEVHs4lmwgfF1kWW0Tar1ZJeqKmA1c7wQERHFNoahBkRWVPyeUwSg9l1kkqzAoGcXGREREcNQA3LoWCk8kop4iwGtmlprdSyXR4HFpOMl9UREFPMYhhqQveUTLWa0qX0XmSSrsMUZua4cERHFPIahBkJVBfb6Lqmv5USLqiqggQYWMy+pJyIiYhhqIHLy7XC4ZJgMOpyTnlCrY3kkBUajDhYjwxARERHDUAPhu4qsc+tE6HS1e9vckoJ4sx4GPd9+IiIing0bACEE9hwOz8KsACArAglcgoOIiAgAw1CDkFfoRJHdA71Ogw4tbbU6liyrMOi0MLOLjIiICADDUIPg6yJr3yIRxlpeCu+WFJiMOpg5vxAREREAhqEG4VQXWVKtj+WRVCRYjdDyknoiIiIADENRr7DUjeOFTmg0QOfWSbU6lhACAoCVq9QTERH5MQxFuT3lEy22aZaAuFrOC+SRfKvUMwwRERH5MAxFuT2HiwCEp4vMLSmIM+lqPe6IiIioMWEYimJ2p4TsPDuA8IQhSRaw8ZJ6IiKiAAxDUWxfdhEAoHlqHBLjTbU6lqKq0GkBM7vIiIiIAjAMRbFwXkXm9vjGC7GLjIiIqCKGoSjl9ig4dKwUAJDZtvazTrslBTarATot33IiIqKKeGaMUr/nFENRBVJtJjRJNNfqWEIICCFgNXO8EBER0ekYhqKU75L6jDbJ0NRygkRJVqHXsYuMiIioKgxDUUiWVezPKQYAZLZNqvXx3JICi0kHEy+pJyIiqiTiYUhVVSxatAgDBgxAz549MWnSJGRnZ1e7vyRJWLBggX//cePG4bfffgvY59ChQ7j11lvRq1cvXHTRRZgzZw6cTmddVyVsDh4rgUdWkRBnQMsm1lofT5JV2OKMtW5hIiIiaowiHoaWLl2KN998E3PnzsXq1auhqiomTpwIj8dT5f4PP/ww1q5di8cffxxr1qxBSkoKJk2ahNJS72DjwsJCjBs3Dnq9Hu+++y7mzZuHL774Ak899VR9VqtW9pYvzJrRJqnWAUZVBTTQ1Hr2aiIiosYqomHI4/Fg5cqVmDp1KgYPHozMzEwsXLgQubm5+Pzzzyvtn52djTVr1uCxxx7DgAED0KFDBzz66KMwGo3YtWsXAOD111+HXq/HwoUL0bFjR1x44YWYOnUqfv75Zwgh6ruKQVNVgb3l8wtltgnPVWQmI5fgICIiqk5Ew9CePXvgcDjQv39//zabzYYuXbpg+/btlfbftGkTEhISMHDgwID9N2zY4D/G999/j2HDhsFkOjVJ4d/+9jesXbu2QXQTZefZUeaSYTbq0DY9vtbHc0sK4i0G6HURbwQkIiKKShFtLsjNzQUANG/ePGB7Wlqa/76KDh06hNatW+Pzzz/H8uXLcfz4cXTp0gUzZ85Ehw4d/PtccskleOKJJ/DZZ5/BYDBg2LBhuOuuuwICUij0+vAHCr1OC135FwB/q1BGmyQYDeF4ezRIjDfVqOy+MuhiIDjFUl2B2Kov69p4xVJ9Wdf6FdEw5BvUbDQGzn9jMplQXFxcaX+73Y7Dhw9j6dKlmD59Omw2G5YtW4brrrsO69evR2pqKux2O1566SWMGDECS5YswdGjRzF37lzk5+dj3rx5IZdVq9UgObn2g5lPl1Dihkavg81qghDCP16od2Y6bAmWWh1bklUkQYvmzWywWgw1fpzNVrvnbUhiqa5AbNWXdW28Yqm+rGv9iGgYMpu9kwl6PB7/9wDgdrthsVR+UfR6Pex2OxYuXOhvCVq4cCEGDRqE999/HxMnToRer0e7du3w8MMPAwC6du0KRVFw9913Y+bMmUhNTQ2prKoqUFJSFtJjz6S01AWHS4JGVXHshAOFpW4YdFq0SDWjpLR2V8DZnRIMOi1cTjc8rqoHpFek02lhs1lQUuKEoqi1eu5oF0t1BWKrvqxr4xVL9WVdw8Nms9SoxSmiYcjXPZaXl4c2bdr4t+fl5SEjI6PS/unp6dDr9f4gBHgDVevWrZGTk+Pfp1OnTgGP890+cuRIyGEI8M7/E26yokIp/9r9RwEAoH1LG3QaTa0/FE6XjKTUOKiKgIqaDx5XFLVO6hqNYqmuQGzVl3VtvGKpvqxr/YhoZ2RmZibi4+OxdetW/7aSkhLs3r0bffv2rbR/3759IcsyfvnlF/82l8uF7OxstG3b1r/P6VeO7du3DzqdDq1atarD2tTenvIusnAszCqEgAZAnLnm3WNERESxKKJhyGg0Yty4cZg/fz6++uor7NmzB9OmTUN6ejqGDx8ORVGQn58Pl8sFAOjTpw8uvPBCzJgxAzt27MD+/fsxffp06HQ6jBo1CgAwYcIEZGdn46GHHsKhQ4fw3Xff4amnnsKoUaOQkpISyeqeUUGJC3mFTmg0QOfWSbU+nkdSYTDoYDFy1mkiIqIzifgw9alTp2Ls2LGYPXs2/vGPf0Cn02HFihUwGAw4duwYLr74Yqxfv96//+LFi5GVlYU77rgDY8eOhd1ux6pVq/xBp3379li1ahUOHjyIUaNGYebMmbj88svxyCOPRKqKNeJrFWqbnhCWOYHckoI4kw5GLsFBRER0RhrREGYijAKKoqKgwBH24x44Wowyl4R3vzmAnDwHLuvXBlnnptX6uCeL3WjbLB5Nkmo+Ol+v1yI52YrCQkej76OOpboCsVVf1rXxiqX6sq7hkZJirdEA6oi3DBHgcErIyfMGrXCMF1IUFTotYOESHERERGfFMBQFDhwtAQC0aBIHm9V4lr3PzrcEh5njhYiIiM6KYSgK7D/inWAyHGuRAYBb8q54r9Py7SUiIjobni0jzOVRkB3GLjIhBFQhYDXXvoWJiIgoFjAMRdj+nGKoqkCTRHNQg52rI8kqDDodLCZ2kREREdUEw1CE7c0uBOBdmDUcvJfU62HiJfVEREQ1wjAUQR5JwYEj3sHTmW3DM17II6mwWQ3QaDRhOR4REVFjxzAUQbv/KIQkq4i3GNAiNa7Wx1NVAY1WE5ZJG4mIiGIFw1AE/Xd/PgCgY0tbWFpy3JICs0HHMERERBQEhqEIspoN0Gk1OK9deNZMc0sK4i0G6Gsw2yYRERF5sQkhgq4e3AHdOqRCkpWwHE9RBOItXKWeiIgoGGxCiCCtRgODPjxvgfeSei27yIiIiILEMNRIuCUFZpMeJi7BQUREFBSGoUbCU74Eh5aX1BMREQWFYagREEJAAyDOzPFCREREwWIYagQ8kgqDQQcLu8iIiIiCxjDUCLgkBVazDkYuwUFERBQ0hqFGQJFVJMRxlXoiIqJQMAw1cIqiQqvjEhxEREShYhhq4NySApNBB4uRYYiIiCgUDEMNnEdSYYszQqvlJfVEREShYBhqwIQQUISAlUtwEBERhYxhqAGTZBVGPbvIiIiIaoNhqAFzSwosRj2MBr6NREREoeJZtAHzSCps8UZouAQHERFRyBiGGihVFdBoNZx1moiIqJYYhhoot6TAbNBxfiEiIqJaYhhqoNweBfEWA/Q6voVERES1wTNpA6WoQDwvqSciIqo1hqEGSJJVGLgEBxERUVgwDDVAbkmB2aSHiYOniYiIao1hqAHySCoS4gzQ8pJ6IiKiWmMYamCEEAAAq5njhYiIiMKBYaiB8a9Sb2IXGRERUTgwDDUwbkmF1ayDQc8wREREFA4MQw2MIquIjzNGuhhERESNBsNQA6IoKrS8pJ6IiCisGIYaELekwGzUw2JkGCIiIgoXhqEGxO1RYbMYodXyknoiIqJwiXgYUlUVixYtwoABA9CzZ09MmjQJ2dnZ1e4vSRIWLFjg33/cuHH47bffqtxXCIEJEyZg/PjxdVX8eiOEgAqBOAtbhYiIiMIp4mFo6dKlePPNNzF37lysXr0aqqpi4sSJ8Hg8Ve7/8MMPY+3atXj88cexZs0apKSkYNKkSSgtLa2077///W98//33dV2FeiHJKox6HbvIiIiIwiyiYcjj8WDlypWYOnUqBg8ejMzMTCxcuBC5ubn4/PPPK+2fnZ2NNWvW4LHHHsOAAQPQoUMHPProozAajdi1a1fAvnv37sXzzz+Pnj171lNt6pZbUhBn0sNoiHh+JSIialQiembds2cPHA4H+vfv799ms9nQpUsXbN++vdL+mzZtQkJCAgYOHBiw/4YNGwKO4Xa7ce+992Lq1Klo165d3VainngkFQlWIzRcgoOIiCisItrnkpubCwBo3rx5wPa0tDT/fRUdOnQIrVu3xueff47ly5fj+PHj6NKlC2bOnIkOHTr495s3bx7S0tIwbtw4zJo1K2zl1evDnx31Oi105V/VUVQBg16HhDhDnZTBx1eGM5WlsYilugKxVV/WtfGKpfqyrvUromHI6XQCAIzGwEkETSYTiouLK+1vt9tx+PBhLF26FNOnT4fNZsOyZctw3XXXYf369UhNTcW3336Ljz76COvWrQtrK4pWq0FysjVsx/NJKHFDo9fBZjVVu4/DJaGJyYAW6YnQ18OHxWaz1PlzRItYqisQW/VlXRuvWKov61o/IhqGzGYzAO/YId/3gLeby2Kp/KLo9XrY7XYsXLjQ3xK0cOFCDBo0CO+//z7GjBmD+++/Hw8//DCaNWsW1rKqqkBJSVlYjwkApaUuOFwSNKpa7T6FpW6kJppRWuIM+/NXpNNpYbNZUFLihKJUX57GIJbqCsRWfVnXxiuW6su6hofNZqlRi1NEw5CveywvLw9t2rTxb8/Ly0NGRkal/dPT06HX6wO6xMxmM1q3bo2cnBxs3LgR+fn5uP/++3H//fcD8AYtVVXRq1cvfPzxx2jRokXI5ZXl8H8gZUWFUv5VHUlSYTHo6uT5q6Ioar09V6TFUl2B2Kov69p4xVJ9Wdf6EdEwlJmZifj4eGzdutUfhkpKSrB7926MGzeu0v59+/aFLMv45Zdf0K1bNwCAy+VCdnY2RowYgWHDhqF3794Bj5k/fz5yc3Mxf/58pKWl1X2lwkySVRi4BAcREVGdiegZ1mg0Yty4cZg/fz5SUlLQsmVLzJs3D+np6Rg+fDgURUFBQQESEhJgNpvRp08fXHjhhZgxYwbmzJmDpKQkLFq0CDqdDqNGjUJ8fDzi4+MDnsNqtcJsNqNt27YRqmXtuD0KzCY9TEauUk9ERFQXIj5MferUqRg7dixmz56Nf/zjH9DpdFixYgUMBgOOHTuGiy++GOvXr/fvv3jxYmRlZeGOO+7A2LFjYbfbsWrVKqSkpESwFnXHI6mwWQ3Q8pJ6IiKiOqERQohIF6IhUBQVBQWOsB/3wNFilLkkJMQZK90nhEBhqQcdWybCZq18f7jp9VokJ1tRWOho9H3UsVRXILbqy7o2XrFUX9Y1PFJSrDUaQB3xliGqnltSYDLoYDGxi4yIiKiuMAxFMbekwmrWwaBnGCIiIqorDENRTJZVxFfRfUZEREThwzAUpWRFhV6nRRwvqSciIqpTDENRyi0pMBt1MBsZhoiIiOoSw1CU8nhUJFiM0Gp5ST0REVFdYhiKQkIIqBCIs7BViIiIqK4xDEUhSVZh1OtgYRcZERFRnWMYikIuj4I4LsFBRERULxiGopAkq/Uy4zQRERExDEUdVRXQarlKPRERUX1hGIoyviU4zOwiIyIiqhcMQ1HG7VEQbzFAX4OF5YiIiKj2eMaNMoqKKlewJyIiorrBMBRFJFmBQa9lFxkREVE9YhiKIm6PWr4EB8MQERFRfWEYiiIeSYXNaoBGwyU4iIiI6gvDUJQQQgAaIM5kiHRRiIiIYgrDUJTwXVLP+YWIiIjqF8NQlHB7FFjNehj0fEuIiIjqE8+8UUJWBC+pJyIiigCGoSigKAJ6nRYWE68iIyIiqm8MQ1HALSnll9RzvBAREVF9YxiKAqoKJFiM0Gp5ST0REVF9YxiKAga9FlYLW4WIiIgigWEoCpiNOph5ST0REVFEMAxFAYtJD5OBg6eJiIgigc0REabTaGCN46zTREREkcIwFGEpiWaY2SpEREQUMQxDEWbjRItEREQRxTFDREREFNMYhoiIiCimMQwRERFRTGMYIiIiopjGMEREREQxjWGIiIiIYhrDEBEREcU0hiEiIiKKaQxDREREFNMYhoiIiCimMQwRERFRTGMYIiIiopjGMEREREQxTSOEEJEuREMghICqNv6XSqfTQlHUSBejXsRSXYHYqi/r2njFUn1Z19rTajXQaDRn3Y9hiIiIiGIau8mIiIgopjEMERERUUxjGCIiIqKYxjBEREREMY1hiIiIiGIawxARERHFNIYhIiIiimkMQ0RERBTTGIaIiIgopjEMERERUUxjGCIiIqKYxjBEREREMY1hiIiIiGIaw1AMKSoqwoMPPoiBAweid+/e+Mc//oEdO3ZUu/+yZcuQkZFR6ashOH78eJVlX7t2bZX7FxYW4p577kHfvn2RlZWFRx55BE6ns55LHZqtW7dWWdeMjAxccsklVT5m586dVe6/devWei59cF588UWMHz8+YNtvv/2GcePGoWfPnhg6dChWrVp11uN88sknuPzyy9G9e3eMHj0amzdvrqsih6yqum7YsAFXX301evXqhaFDh+Kpp56Cy+Wq9hiKoqB79+6V3ufFixfXdfGDUlVdZ8+eXancQ4cOPeNxGsL7ClSu7/jx46v9Gf7ggw+qPc7NN99caf/TX8dIONu5ZvPmzRgzZgx69OiByy67DB9//PFZj/nGG2/gkksuQffu3XHddddh9+7d4S20oJhx8803i5EjR4rt27eLgwcPikceeUR0795dHDhwoMr977rrLnHfffeJvLy8gK+G4JtvvhHdunUTx48fDyi70+mscv9x48aJq6++WuzatUv88MMPYsiQIWL69On1XOrQuN3uSu/R559/LjIyMsR7771X5WPeeOMNcemll1Z6nNvtrufS19zrr78uMjMzxbhx4/zbCgoKRL9+/cSsWbPE/v37xXvvvSe6detWbb2FEGLz5s3ivPPOE//+97/F/v37xZNPPim6du0q9u/fXx/VqJGq6rp9+3Zx7rnnimXLlolDhw6Jb775RgwcOFDMnDmz2uPs379fdO7cWfz2228B77Pdbq+PatRIVXUVQoixY8eKZ555JqDcJ0+erPY4DeF9FaLq+hYWFgbU8/jx4+K6664TI0aMOON71b9/f/Hmm28GPLawsLAeanFmZzrX7N+/X3Tr1k0888wzYv/+/eLll18WXbp0ET/88EO1x1u7dq3o3r27+PDDD8Xvv/8u7rvvPpGVlXXGz0OwGIZixB9//CE6d+4sduzY4d+mqqq49NJLxbPPPlvlY/7617+KV155pZ5KGF7Lly8XV1xxRY32/fHHH0Xnzp0Dfml+9913IiMjQ+Tm5tZVEeuMw+EQQ4YMOeNJ8qGHHhL/93//V4+lCl1ubq6YPHmy6Nmzp7jssssCTiIvvPCCuPjii4UkSf5tCxYsEMOHD6/2eLfccou46667ArZde+214l//+lfYyx6sM9X1nnvuETfddFPA/u+//74477zzqg2xH3/8sejdu3edljlUZ6qrqqqiZ8+e4vPPP6/x8aL5fRXizPU93WuvvSa6du1a7R+qQghx4sQJ0blzZ/Hrr7/WRXFDdrZzzb/+9S8xduzYgMf885//FLfccku1xxw+fLh4+umn/bclSRKDBg0SL7zwQtjKzW6yGJGcnIzly5ejW7du/m0ajQYajQYlJSWV9vd4PPjjjz/Qvn37+ixm2OzduxcdOnSo0b47duxA06ZNA/bPysqCRqPBzp0766qIdeaFF16A0+nEjBkzqt0nmNcn0n799VcYDAasW7cOPXr0CLhvx44dyMrKgl6v92+74IIL8Mcff+DEiROVjqWqKn788Uf0798/YHu/fv2wffv2uqlAEM5U11tuuaXSe6rVaiFJEux2e5XHi+b3+Ux1/fPPP1FWVlbj3z/R/r4CZ65vRQUFBXj22Wdx2223nbH+e/fuhUajQbt27eqiuCE727lmx44dld6nCy64ADt37oQQotLxTp48iT/++CPgMXq9Hn369Anre6s/+y7UGNhsNgwaNChg22effYbDhw/j/vvvr7T//v37oSgKPvvsMzz22GNwu93o27cv7rvvPqSlpdVXsUO2b98+JCcn4/rrr8ehQ4fQtm1b3HbbbRg4cGClfY8fP47mzZsHbDMajUhKSsKxY8fqq8hhUVBQgFdffRX33HMPkpKSqt3v999/R3JyMsaMGYPjx4+jc+fOmDZtGrp3715/ha2hoUOHVjtWJDc3F507dw7Y5vt8Hjt2DE2aNAm4r6SkBGVlZUhPT6/0mNzc3DCWOjRnqmuXLl0CbkuShFdffRVdu3ZFSkpKlY/Zt28fZFnGhAkTsGfPHjRr1gw33ngjRo0aFfayB+tMdd23bx8A4LXXXsO3334LrVaLgQMHYtq0aUhISKi0f7S/r8CZ61vRSy+9BLPZjAkTJpxxv3379iEhIQFz5szBpk2bEBcXh8suuwy33347jEZjuIodtLOda95///0q3yen04nCwsJKn2Xf+3f67+i0tDTs2bMnbOVmy1CM+vHHHzFr1iwMHz4cgwcPrnS/75eRxWLBc889h8ceewwHDx7EDTfccMYBm9FAlmUcPHgQxcXFuPPOO7F8+XL07NkTt956a5UDKp1OZ5W/PEwmE9xud30UOWzefPNNJCQk4Nprr612n2PHjqG0tBRlZWWYPXs2li5diiZNmmDcuHHYv39/PZa29lwuV6X3zmQyAUCV753vs1vVYxrSey3LMqZPn47ff/8dDz30ULX7/f777ygqKsL48eOxYsUK/OUvf8GsWbPw3nvv1WNpg7dv3z5otVqkpaXhhRdewMyZM/H999/j9ttvh6qqlfZvLO+r3W7HO++8gwkTJvg/x9XZt28f3G43unfvjpdffhm33XYb3n33XcyePbueSlszp59rqvqZ9d32eDyVHu+7kKWu31u2DMWgL7/8Evfeey969+6N+fPnV7nP6NGjMXDgwICU3qlTJwwcOBAbNmzA5ZdfXl/FDZper8fWrVuh0+lgNpsBAF27dsXvv/+OFStWVGqiNZvNVf4Qut1uxMXF1UuZw+WDDz7A6NGj/fWuSvPmzbF9+3ZYLBYYDAYAQLdu3bB792689tpreOSRR+qruLVW1Xvn+wVZ1XvnO8FU9RiLxVJHpQwvu92Ou+++G9u2bcOSJUvO2Jr3n//8B4qiwGq1AgAyMzNx9OhRrFixAmPHjq2vIgfttttuw3XXXYfk5GQAQOfOndG0aVNcc801+OWXXyp1MzWG9xXw/m72eDy4+uqrz7rvnDlzMGPGDCQmJgLwvkYGgwHTpk3D9OnTK7WKRkJV5xqTyVTpffLdruq98v0uq+v3li1DMeb111/HnXfeiSFDhuCFF144418fpzdXpqWlISkpKWqanc/EarVWCgSdOnXC8ePHK+2bnp6OvLy8gG0ejwdFRUUNokvQZ8+ePcjOzsYVV1xx1n1tNps/CAHesScdOnSo8vWJZlW9d77bzZo1q7R/UlIS4uLiqnxMVftHm7y8PFx//fX473//ixUrVlTqjjid2Wz2ByGfzp07R/3PsFar9Qchn06dOgFAlWVv6O+rz5dffolBgwbBZrOddV+9Xu8PQj5neo3qW3XnmubNm1f5PsXFxVXZBerrHqvr95ZhKIa8+eabmDt3Lq6//no888wzZ+xXXrhwIf7yl78EDGjLyclBYWEhOnbsWB/FDdnvv/+O3r17V5ozZ9euXVWWvW/fvsjNzcXhw4f927Zt2wYAOP/88+u2sGG0Y8cOpKamIjMz84z7ffvtt+jVqxeys7P922RZxp49e6L+vT1d3759sXPnTiiK4t+2ZcsWtGvXDqmpqZX212g06N27t//99dm6dSv69OlT5+WtjeLiYtx4440oKCjAG2+8gb59+55x/5KSEmRlZVWaW+uXX37xnzSj1fTp03HTTTcFbPvll18AoMrPaEN+XyuqanBxdcaPH49Zs2YFbPvll19gMBhwzjnn1EHpau5M55o+ffpUep+2bNmC3r17Q6utHElSU1PRrl27gN/nsixjx44dZ/0ZCAbDUIw4dOgQHn/8cQwbNgyTJ0/GiRMnkJ+fj/z8fJSWlsLj8SA/P9/fFDls2DAcOXIEDz/8MA4dOoTt27fjzjvvRO/evTFgwIAI1+bMOnTogPbt22POnDnYsWMHDhw4gCeeeAL//e9/cdttt0FRFOTn5/vHGfTo0QO9e/fGtGnT8PPPP2PLli148MEHMXr06Ab1V+Xu3burnRQzPz8fDocDANC7d28kJydjxowZ2LVrF/bu3YsZM2agqKio0gko2l199dWw2+144IEHsH//fqxduxavvvoqJk+e7N+ntLQUBQUF/ts333wzPv74Y7zyyis4cOAAnn76afz222+48cYbI1GFGnviiSeQnZ2NefPmISUlxf/zm5+f7w+DRUVFKCoqAuBt/bvggguwcOFCbNy4EX/88QeWL1+OdevW4c4774xgTc7uL3/5CzZv3owlS5bgzz//xMaNG3H//fdj5MiR/qvjGsv76nPs2DEUFhZW+8eMw+FAfn6+//Zf/vIXfPjhh3jrrbeQnZ2N9evX4+mnn8aECRMQHx9fX8Wu5GznmvHjx+Pnn3/G/PnzceDAAaxcuRKffvopJk6c6D9Gxc8x4L2S8pVXXsH777+P/fv34/7774fL5QpvV2/YLtKnqLZs2TLRuXPnKr9mzJghtmzZIjp37iy2bNnif8wPP/wgrr32WtGzZ0+RlZUlZs2aJYqKiiJYi5rLz88XM2fOFBdddJHo1q2buPbaa8X27duFEEJkZ2eLzp07izVr1vj3P3HihLjzzjtFz549Rb9+/cRDDz0kXC5XpIofkokTJ4q77767yvs6d+4sFi1a5L99+PBhceedd4qsrCzRo0cPccstt4i9e/fWV1FDNmPGjErzs/zvf/8T11xzjejatasYMmSIeO211yo9ZsiQIQHb3n//fTFs2DDRrVs3cdVVV51xwrdIqVhXWZZFt27dqv0Zzs7OFkJ4Jw+t+PqUlpaKxx9/XAwaNEh07dpVjBo1SnzxxRcRqc+ZVPW+rl+/XowePVp0795dXHTRReLJJ58M+JlsqO+rENV/jk+f76yiRYsWic6dOwdse/3118Vf//pX/2d/2bJlQlGUOit3TZztXCOEEBs3bhQjR44UXbt2FZdddpn4+OOPA45x+udYCCFefvllMXDgQNG9e3dx3XXXid27d4e13Bohqriwn4iIiChGsJuMiIiIYhrDEBEREcU0hiEiIiKKaQxDREREFNMYhoiIiCimMQwRERFRTGMYIiIiopjGMEREFCacto2oYWIYIqKgjR8/Hl26dPGvF3W6oUOHYubMmfVcqlPGjx+PjIyMar+qK3eoSkpKMH36dOzYsSOsxyWi+qGPdAGIqGFSFAWzZs3C2rVrz7job6R06dIFDz30UJX3+da3CpfffvsNH374Ia6++uqwHpeI6gfDEBGFJCEhAb///juef/55TJs2LdLFqSQ+Ph49e/aMdDGIqAFgNxkRheTcc8/F6NGj8fLLL2PXrl1n3DcjIwOLFy8O2LZ48WJkZGT4b8+cORMTJkzA22+/jUsvvRTdu3fH3//+dxw6dAhff/01rrjiCvTo0QN/+9vf8Ntvv4WtHu+++y5GjBiBrl27YvDgwVi8eLF/FfiK+4wZMwY9e/ZE9+7dMWrUKHzyyScAgK1bt+KGG24AANxwww0YP348gKq7CteuXYuMjAzk5OT4X4Nhw4ZhyZIlyMrKwsUXX4zi4uIalaugoAD33HMPLrroInTr1g2jRo3CBx98ELbXhSiWsGWIiEJ2//33Y9OmTZg1axbWrFlT6+6yn376CXl5eZg5cybcbjcefvhh3HrrrdBoNJg6dSosFgseeugh3Hvvvfj444/PeCwhBGRZrrRdp9NBo9EAAF588UUsXLgQ48aNw6xZs/Dbb79h8eLFOHbsGB5//HEAwBtvvIFHH30Ud955J84//3wUFxfjpZdewr333otevXrhvPPOw4MPPog5c+bgwQcfRL9+/YKq89GjR7Fx40YsXLgQRUVFSExMrFG57rvvPpw8eRKPPPII4uPj8eGHH2LGjBlIT0/HBRdcEFQZiGIdwxARhSwxMRFz5szBbbfdFpbuMofDgWeffdY/pmfbtm1YvXo1Xn31VfTv3x8AcPjwYTz11FMoKSmBzWar9ljbt2/HeeedV2n7M888gxEjRqC0tBRLly7Ftddei9mzZwMALr74YiQlJWH27Nm4+eab0alTJ2RnZ2PChAm4/fbb/cdo2bIlxowZg507d2LEiBHo2LEjAKBjx47+72tKlmXMmDEDffr0AYAal2vbtm2YMmUKLr30UgBAVlYWkpKSonL8FlG0YxgioloZOnQorrzySrz88ssYPnx4lQGkphITEwMGNzdp0gQA0KNHD/+2pKQkADhrGDrvvPPwyCOPVNrepk0bAN5WKJfLhaFDhwa0IA0dOhQAsGnTJnTq1Mnf1VVSUoKDBw/i8OHD2Lp1KwDA4/GEUs1Kzj33XP/3NS1Xv379sHjxYuzevRsDBgzAoEGDMGPGjLCUhyjWMAwRUa3Nnj0bmzdv9neXhSo+Pr7K7XFxcUEfy2q1olu3btXeX1RUBAC49dZbq7w/Ly8PAPDnn3/iwQcfxObNm2EwGNC+fXtkZmYCCN+8QlarNehyLVy4EC+88AI++eQTfPbZZ9BqtbjwwgsxZ84ctGzZMizlIooVDENEVGuJiYl4+OGHMWXKFCxdurTKfU4flFxWVlYfRauWr1Vp/vz5OOeccyrd36RJE6iqiltvvRUGgwHvvfcezj33XOj1euzfvx8ffvjhWZ8jlDrXpFyA92q+++67D/fddx8OHjyIr776CkuXLsUjjzyC5cuXn/V5iOgUXk1GRGFx6aWXYuTIkVi+fDkKCgoC7ouPj8fx48cDtv3444/1WbxKevToAYPBgOPHj6Nbt27+L71ej2eeeQY5OTkoLCzEoUOHMHbsWP99APDtt98CAFRVBeAdlH26+Ph45ObmBmzbuXNnWMp15MgRDBo0CJ9++ikAoH379pg0aRIuvPBCHD16tFavC1EsYssQEYXNv/71L2zZsgUnTpwI2D548GB8/PHH6NGjB9q2bYu1a9fi8OHDESqlV3JyMiZOnIjnnnsOdrsd/fr1w/Hjx/Hcc89Bo9EgMzMTCQkJaNmyJd544w2kp6fDZrPhu+++w6pVqwAATqcTgLeVBgC++eYbJCYmIjMzE0OGDMGLL76IF198ET169MCGDRuwZcuWsJUrPT0djz76KOx2O9q0aYNdu3Zh48aNmDx5ct29aESNFMMQEYVNUlISHn74Ydxxxx0B22fNmgVZlvHUU09Br9fj8ssvxz333OO/WipS7r77bjRt2hRvvvkmXn75ZSQmJqJ///745z//6Q84S5cuxWOPPYaZM2fCaDSiY8eOWLZsGR5//HHs2LED48ePR6dOnTBy5Ei88cYb+O677/Cf//wHkydPRkFBAVasWAFJkjB48GA89thjuO2228JSriVLluCZZ57Bc889h8LCQjRv3hx33HFHtWONiKh6GsGVBYmIiCiGccwQERERxTSGISIiIoppDENEREQU0xiGiIiIKKYxDBER0f+3WwcCAAAAAIL8rQe5KII1GQIA1mQIAFiTIQBgTYYAgDUZAgDWZAgAWAvnUkom2ThO5AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create plots to visualize hyperparameter tuning\n",
    "plt.figure()\n",
    "sns.scatterplot(rf_hyperparams, x='Num Estimators', y='Avg Accuracy', hue=\"Num Features\")\n",
    "plt.title('Random Forest: Tuning Number of Estimators')\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(rf_hyperparams, x='Tree Depth', y='Avg Accuracy', hue='Num Features')\n",
    "plt.title('Random Forest: Tuning Tree Depth')\n",
    "\n",
    "plt.figure()\n",
    "sns.lineplot(rf_hyperparams, x='Num Features', y='Avg Accuracy')\n",
    "plt.title('Random Forest: Tuning Number of Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on Query Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Balanced Accuracy on Query Set: 0.7525127017780182\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=rf_best_param[0], max_depth=rf_best_param[1], max_features=rf_best_param[2])\n",
    "rf_model.fit(X_train_use.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1), y_train_use)\n",
    "\n",
    "preds = rf_model.predict(X_query.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "rf_accuracy = balanced_accuracy_score(y_query, preds)\n",
    "print(f\"Random Forest Balanced Accuracy on Query Set: {rf_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adp_dist</td>\n",
       "      <td>0.507509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RL_pre</td>\n",
       "      <td>0.079940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pre_nucleus_z</td>\n",
       "      <td>0.037698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pre_rf_x</td>\n",
       "      <td>0.035877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>post_test_score</td>\n",
       "      <td>0.034787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>post_nucleus_y</td>\n",
       "      <td>0.033918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pre_skeletal_distance_to_soma</td>\n",
       "      <td>0.030270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pre_nucleus_y</td>\n",
       "      <td>0.026625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>V1_pre</td>\n",
       "      <td>0.017294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>post_oracle</td>\n",
       "      <td>0.016790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RL_post</td>\n",
       "      <td>0.016425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pre_test_score</td>\n",
       "      <td>0.014966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>post_skeletal_distance_to_soma</td>\n",
       "      <td>0.012857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>area3</td>\n",
       "      <td>0.011916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>area2</td>\n",
       "      <td>0.010397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>post_rf_y</td>\n",
       "      <td>0.010016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>post_nucleus_x</td>\n",
       "      <td>0.009780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>post_rf_x</td>\n",
       "      <td>0.009347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>area1</td>\n",
       "      <td>0.009129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>post_nucleus_z</td>\n",
       "      <td>0.008572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pre_rf_y</td>\n",
       "      <td>0.008548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AL_post</td>\n",
       "      <td>0.008177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>minicol_dist</td>\n",
       "      <td>0.007325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>nuclei_adp_dist</td>\n",
       "      <td>0.006381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fw_similarity</td>\n",
       "      <td>0.005739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pre_nucleus_x</td>\n",
       "      <td>0.005723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_distance</td>\n",
       "      <td>0.005418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>V1_post</td>\n",
       "      <td>0.005025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>me_similarity</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AL_pre</td>\n",
       "      <td>0.003768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dendritic_coor_y</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pre_oracle</td>\n",
       "      <td>0.001360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>axonal_coor_z</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dendritic_coor_z</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>axonal_coor_y</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dendritic_coor_x</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>axonal_coor_x</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Features  Importances\n",
       "6                         adp_dist     0.507509\n",
       "26                          RL_pre     0.079940\n",
       "19                   pre_nucleus_z     0.037698\n",
       "11                        pre_rf_x     0.035877\n",
       "14                 post_test_score     0.034787\n",
       "21                  post_nucleus_y     0.033918\n",
       "8    pre_skeletal_distance_to_soma     0.030270\n",
       "18                   pre_nucleus_y     0.026625\n",
       "28                          V1_pre     0.017294\n",
       "13                     post_oracle     0.016790\n",
       "29                         RL_post     0.016425\n",
       "10                  pre_test_score     0.014966\n",
       "7   post_skeletal_distance_to_soma     0.012857\n",
       "34                           area3     0.011916\n",
       "33                           area2     0.010397\n",
       "16                       post_rf_y     0.010016\n",
       "20                  post_nucleus_x     0.009780\n",
       "15                       post_rf_x     0.009347\n",
       "32                           area1     0.009129\n",
       "22                  post_nucleus_z     0.008572\n",
       "12                        pre_rf_y     0.008548\n",
       "31                         AL_post     0.008177\n",
       "35                    minicol_dist     0.007325\n",
       "36                 nuclei_adp_dist     0.006381\n",
       "24                   fw_similarity     0.005739\n",
       "17                   pre_nucleus_x     0.005723\n",
       "25                     rf_distance     0.005418\n",
       "30                         V1_post     0.005025\n",
       "23                   me_similarity     0.004800\n",
       "27                          AL_pre     0.003768\n",
       "4                 dendritic_coor_y     0.001876\n",
       "9                       pre_oracle     0.001360\n",
       "2                    axonal_coor_z     0.000724\n",
       "5                 dendritic_coor_z     0.000591\n",
       "1                    axonal_coor_y     0.000223\n",
       "3                 dendritic_coor_x     0.000142\n",
       "0                    axonal_coor_x     0.000070"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract feature importances\n",
    "rf_features = pd.DataFrame({\"Features\":X_train_use.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).columns, \n",
    "              \"Importances\":abs(rf_model.feature_importances_)}).sort_values(by='Importances', ascending=False)\n",
    "rf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Random Forest Feature Importance')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create plot of feature importances\n",
    "sns.set_theme(palette='deep')\n",
    "plt.figure(dpi=250)\n",
    "g = sns.barplot(xg_features, y='Features', x='Importances', palette='mako')\n",
    "g.set_yticklabels(g.get_yticklabels(), fontsize=5)\n",
    "plt.title(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Accuracy Score: 0.7539140409694007\n",
      "Private Accuracy Score: 0.7519353474431347\n"
     ]
    }
   ],
   "source": [
    "leaderboard_path = \"Data/leaderboard_data.csv\"\n",
    "sub_data = cleaner(leaderboard_path, feature_path, morph_path)\n",
    "sub_data = sub_data.select_dtypes('number')\n",
    "preds = rf_model.predict(sub_data.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "sub_data['connected_pred'] = preds==1\n",
    "submission_data = sub_data.filter(['ID','connected_pred'])\n",
    "submission_data.to_csv('final_submission_data_rf.csv',index=False)\n",
    "\n",
    "# evaluate against real leaderbaord data\n",
    "solutions = pd.read_csv('Data/solution_data.csv')\n",
    "leaderboard = pd.concat([solutions,submission_data], axis=1)\n",
    "public = leaderboard[leaderboard[\"Usage\"]==\"Public\"]\n",
    "print(\"Public Accuracy Score:\", balanced_accuracy_score(public[\"connected\"], public[\"connected_pred\"]))\n",
    "private = leaderboard[leaderboard[\"Usage\"]==\"Private\"]\n",
    "print(\"Private Accuracy Score:\", balanced_accuracy_score(private[\"connected\"], private[\"connected_pred\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Use the L2 penalty and tune C, which is inversely related to lambda. We tested values of C [0.1, 10] with a step size of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, penalty: l2 | valid accuracy for this fold, 0.6978207531875673\n",
      "C: 0.1, penalty: l2 | valid accuracy for this fold, 0.7525586375117582\n",
      "C: 0.1, penalty: l2 | valid accuracy for this fold, 0.7472334302692076\n",
      "C: 0.1, penalty: l2 | valid accuracy for this fold, 0.760036531702322\n",
      "C: 0.1, penalty: l2 | valid accuracy for this fold, 0.7414621781174313\n",
      "avgfold accuracy: 0.7398223061576574\n",
      "standard deviation: 0.021874978597806946\n",
      "avg train time: 1.3971328735351563e-05\n",
      "C: 0.6, penalty: l2 | valid accuracy for this fold, 0.6966368299753921\n",
      "C: 0.6, penalty: l2 | valid accuracy for this fold, 0.7528159185655814\n",
      "C: 0.6, penalty: l2 | valid accuracy for this fold, 0.7501586389847523\n",
      "C: 0.6, penalty: l2 | valid accuracy for this fold, 0.7533238314006587\n",
      "C: 0.6, penalty: l2 | valid accuracy for this fold, 0.7416216960188846\n",
      "avgfold accuracy: 0.7389113829890539\n",
      "standard deviation: 0.021550123783743953\n",
      "avg train time: 1.0967254638671875e-05\n",
      "C: 1.1, penalty: l2 | valid accuracy for this fold, 0.694594282543409\n",
      "C: 1.1, penalty: l2 | valid accuracy for this fold, 0.7547075210925327\n",
      "C: 1.1, penalty: l2 | valid accuracy for this fold, 0.7497678613372338\n",
      "C: 1.1, penalty: l2 | valid accuracy for this fold, 0.7533026134927444\n",
      "C: 1.1, penalty: l2 | valid accuracy for this fold, 0.7413558328497958\n",
      "avgfold accuracy: 0.7387456222631432\n",
      "standard deviation: 0.022558067117813132\n",
      "avg train time: 1.0585784912109375e-05\n",
      "C: 1.6, penalty: l2 | valid accuracy for this fold, 0.696620317971165\n",
      "C: 1.6, penalty: l2 | valid accuracy for this fold, 0.7529188309871107\n",
      "C: 1.6, penalty: l2 | valid accuracy for this fold, 0.7498329909451535\n",
      "C: 1.6, penalty: l2 | valid accuracy for this fold, 0.7509377392780376\n",
      "C: 1.6, penalty: l2 | valid accuracy for this fold, 0.7415685233850668\n",
      "avgfold accuracy: 0.7383756805133068\n",
      "standard deviation: 0.02123338435140949\n",
      "avg train time: 1.0538101196289063e-05\n",
      "C: 2.1, penalty: l2 | valid accuracy for this fold, 0.6928324391832863\n",
      "C: 2.1, penalty: l2 | valid accuracy for this fold, 0.7511129888114338\n",
      "C: 2.1, penalty: l2 | valid accuracy for this fold, 0.7498764106837668\n",
      "C: 2.1, penalty: l2 | valid accuracy for this fold, 0.7509801750938662\n",
      "C: 2.1, penalty: l2 | valid accuracy for this fold, 0.7415153507512491\n",
      "avgfold accuracy: 0.7372634729047205\n",
      "standard deviation: 0.02249994858656853\n",
      "avg train time: 1.1396408081054687e-05\n",
      "C: 2.6, penalty: l2 | valid accuracy for this fold, 0.6929315112086487\n",
      "C: 2.6, penalty: l2 | valid accuracy for this fold, 0.7511129888114338\n",
      "C: 2.6, penalty: l2 | valid accuracy for this fold, 0.7500066698996062\n",
      "C: 2.6, penalty: l2 | valid accuracy for this fold, 0.7509377392780376\n",
      "C: 2.6, penalty: l2 | valid accuracy for this fold, 0.7415685233850668\n",
      "avgfold accuracy: 0.7373114865165586\n",
      "standard deviation: 0.022472331798346606\n",
      "avg train time: 8.630752563476563e-06\n",
      "C: 3.1, penalty: l2 | valid accuracy for this fold, 0.6928819751959675\n",
      "C: 3.1, penalty: l2 | valid accuracy for this fold, 0.7510615326006691\n",
      "C: 3.1, penalty: l2 | valid accuracy for this fold, 0.7499849600302997\n",
      "C: 3.1, penalty: l2 | valid accuracy for this fold, 0.7508528676463805\n",
      "C: 3.1, penalty: l2 | valid accuracy for this fold, 0.7443263710519072\n",
      "avgfold accuracy: 0.7378215413050448\n",
      "standard deviation: 0.022605039274092284\n",
      "avg train time: 1.3303756713867187e-05\n",
      "C: 3.6, penalty: l2 | valid accuracy for this fold, 0.6929149992044217\n",
      "C: 3.6, penalty: l2 | valid accuracy for this fold, 0.7510272284601593\n",
      "C: 3.6, penalty: l2 | valid accuracy for this fold, 0.7498981205530733\n",
      "C: 3.6, penalty: l2 | valid accuracy for this fold, 0.750895303462209\n",
      "C: 3.6, penalty: l2 | valid accuracy for this fold, 0.7443440952631798\n",
      "avgfold accuracy: 0.7378159493886087\n",
      "standard deviation: 0.022584500320967652\n",
      "avg train time: 1.068115234375e-05\n",
      "C: 4.1, penalty: l2 | valid accuracy for this fold, 0.6928159271790593\n",
      "C: 4.1, penalty: l2 | valid accuracy for this fold, 0.7510443805304142\n",
      "C: 4.1, penalty: l2 | valid accuracy for this fold, 0.7498981205530733\n",
      "C: 4.1, penalty: l2 | valid accuracy for this fold, 0.750810431830552\n",
      "C: 4.1, penalty: l2 | valid accuracy for this fold, 0.7414976265399764\n",
      "avgfold accuracy: 0.7372132973266149\n",
      "standard deviation: 0.02247917980008434\n",
      "avg train time: 1.1110305786132813e-05\n",
      "C: 4.6, penalty: l2 | valid accuracy for this fold, 0.6928324391832863\n",
      "C: 4.6, penalty: l2 | valid accuracy for this fold, 0.7509757722493947\n",
      "C: 4.6, penalty: l2 | valid accuracy for this fold, 0.74991983042238\n",
      "C: 4.6, penalty: l2 | valid accuracy for this fold, 0.750810431830552\n",
      "C: 4.6, penalty: l2 | valid accuracy for this fold, 0.7415153507512491\n",
      "avgfold accuracy: 0.7372107648873725\n",
      "standard deviation: 0.022467362850020888\n",
      "avg train time: 1.0013580322265625e-05\n",
      "C: 5.1, penalty: l2 | valid accuracy for this fold, 0.6908559397682115\n",
      "C: 5.1, penalty: l2 | valid accuracy for this fold, 0.7492213862844824\n",
      "C: 5.1, penalty: l2 | valid accuracy for this fold, 0.7500283797689129\n",
      "C: 5.1, penalty: l2 | valid accuracy for this fold, 0.750810431830552\n",
      "C: 5.1, penalty: l2 | valid accuracy for this fold, 0.7415330749625217\n",
      "avgfold accuracy: 0.7364898425229361\n",
      "standard deviation: 0.02305797367288646\n",
      "avg train time: 1.010894775390625e-05\n",
      "C: 5.6, penalty: l2 | valid accuracy for this fold, 0.6908559397682115\n",
      "C: 5.6, penalty: l2 | valid accuracy for this fold, 0.7492042342142275\n",
      "C: 5.6, penalty: l2 | valid accuracy for this fold, 0.750071799507526\n",
      "C: 5.6, penalty: l2 | valid accuracy for this fold, 0.750810431830552\n",
      "C: 5.6, penalty: l2 | valid accuracy for this fold, 0.741479902328704\n",
      "avgfold accuracy: 0.7364844615298443\n",
      "standard deviation: 0.023058873443625144\n",
      "avg train time: 1.1444091796875e-05\n",
      "C: 6.1, penalty: l2 | valid accuracy for this fold, 0.6908394277639844\n",
      "C: 6.1, penalty: l2 | valid accuracy for this fold, 0.7491870821439727\n",
      "C: 6.1, penalty: l2 | valid accuracy for this fold, 0.7500935093768326\n",
      "C: 6.1, penalty: l2 | valid accuracy for this fold, 0.750810431830552\n",
      "C: 6.1, penalty: l2 | valid accuracy for this fold, 0.7414621781174313\n",
      "avgfold accuracy: 0.7364785258465547\n",
      "standard deviation: 0.023065310631231482\n",
      "avg train time: 1.0967254638671875e-05\n",
      "C: 6.6, penalty: l2 | valid accuracy for this fold, 0.6907898917513032\n",
      "C: 6.6, penalty: l2 | valid accuracy for this fold, 0.7473983920385506\n",
      "C: 6.6, penalty: l2 | valid accuracy for this fold, 0.7501369291154458\n",
      "C: 6.6, penalty: l2 | valid accuracy for this fold, 0.7508316497384662\n",
      "C: 6.6, penalty: l2 | valid accuracy for this fold, 0.7414090054836135\n",
      "avgfold accuracy: 0.7361131736254759\n",
      "standard deviation: 0.02290371640881585\n",
      "avg train time: 1.0061264038085938e-05\n",
      "C: 7.1, penalty: l2 | valid accuracy for this fold, 0.6908394277639844\n",
      "C: 7.1, penalty: l2 | valid accuracy for this fold, 0.7473812399682958\n",
      "C: 7.1, penalty: l2 | valid accuracy for this fold, 0.7501369291154458\n",
      "C: 7.1, penalty: l2 | valid accuracy for this fold, 0.7507892139226376\n",
      "C: 7.1, penalty: l2 | valid accuracy for this fold, 0.7414621781174313\n",
      "avgfold accuracy: 0.7361217977775589\n",
      "standard deviation: 0.022879443775837057\n",
      "avg train time: 9.965896606445312e-06\n",
      "C: 7.6, penalty: l2 | valid accuracy for this fold, 0.6908889637766656\n",
      "C: 7.6, penalty: l2 | valid accuracy for this fold, 0.7473640878980409\n",
      "C: 7.6, penalty: l2 | valid accuracy for this fold, 0.7501152192461391\n",
      "C: 7.6, penalty: l2 | valid accuracy for this fold, 0.7507255601988948\n",
      "C: 7.6, penalty: l2 | valid accuracy for this fold, 0.741479902328704\n",
      "avgfold accuracy: 0.7361147466896888\n",
      "standard deviation: 0.022848164547522833\n",
      "avg train time: 1.0967254638671875e-05\n",
      "C: 8.1, penalty: l2 | valid accuracy for this fold, 0.6908889637766656\n",
      "C: 8.1, penalty: l2 | valid accuracy for this fold, 0.7456440060736383\n",
      "C: 8.1, penalty: l2 | valid accuracy for this fold, 0.750223768592672\n",
      "C: 8.1, penalty: l2 | valid accuracy for this fold, 0.7507255601988948\n",
      "C: 8.1, penalty: l2 | valid accuracy for this fold, 0.741479902328704\n",
      "avgfold accuracy: 0.7357924401941149\n",
      "standard deviation: 0.02270235266490042\n",
      "avg train time: 1.1348724365234375e-05\n",
      "C: 8.6, penalty: l2 | valid accuracy for this fold, 0.6927333671579239\n",
      "C: 8.6, penalty: l2 | valid accuracy for this fold, 0.7456611581438932\n",
      "C: 8.6, penalty: l2 | valid accuracy for this fold, 0.7501803488540589\n",
      "C: 8.6, penalty: l2 | valid accuracy for this fold, 0.7507467781068091\n",
      "C: 8.6, penalty: l2 | valid accuracy for this fold, 0.7414444539061587\n",
      "avgfold accuracy: 0.7361532212337687\n",
      "standard deviation: 0.021970042999186448\n",
      "avg train time: 1.0728836059570312e-05\n",
      "C: 9.1, penalty: l2 | valid accuracy for this fold, 0.6927498791621509\n",
      "C: 9.1, penalty: l2 | valid accuracy for this fold, 0.7456440060736383\n",
      "C: 9.1, penalty: l2 | valid accuracy for this fold, 0.7502020587233655\n",
      "C: 9.1, penalty: l2 | valid accuracy for this fold, 0.7505982527514091\n",
      "C: 9.1, penalty: l2 | valid accuracy for this fold, 0.7414444539061587\n",
      "avgfold accuracy: 0.7361277301233444\n",
      "standard deviation: 0.021945148732360448\n",
      "avg train time: 9.822845458984376e-06\n",
      "C: 9.6, penalty: l2 | valid accuracy for this fold, 0.692766391166378\n",
      "C: 9.6, penalty: l2 | valid accuracy for this fold, 0.7456783102141481\n",
      "C: 9.6, penalty: l2 | valid accuracy for this fold, 0.7502020587233655\n",
      "C: 9.6, penalty: l2 | valid accuracy for this fold, 0.7506194706593234\n",
      "C: 9.6, penalty: l2 | valid accuracy for this fold, 0.7414444539061587\n",
      "avgfold accuracy: 0.7361421369338748\n",
      "standard deviation: 0.021944398280732594\n",
      "avg train time: 9.012222290039062e-06\n"
     ]
    }
   ],
   "source": [
    "# submission hyperparameters unchanged in revised version\n",
    "C_vals = np.arange(0.1, 10, 0.5)\n",
    "penalties = ['l2']\n",
    "\n",
    "accuracies = {}\n",
    "for C_val in C_vals:\n",
    "    for penalty_val in penalties:\n",
    "        fold_accuracy = []\n",
    "        avg_train_time = 0\n",
    "        for fold in [1,2,3,4,5]:      \n",
    "            # subset of data\n",
    "            random.seed(fold)\n",
    "            X_train_fold, X_val_fold, y_train_fold, y_val_fold = splitter(X_cv)\n",
    "            ros = RandomOverSampler(random_state=0, sampling_strategy = 'minority')\n",
    "            X_train_fold, y_train_fold = ros.fit_resample(\n",
    "                X_train_fold, y_train_fold)  \n",
    "            \n",
    "            X_train_fold = X_train_fold.select_dtypes('number')\n",
    "            X_val_fold = X_val_fold.select_dtypes('number')    \n",
    "            \n",
    "            startt = time.time()\n",
    "            LR = LogisticRegression(max_iter= 200, C=C_val, penalty=penalty_val, solver='newton-cholesky') \n",
    "            endt = time.time(); elapsed = endt-startt\n",
    "\n",
    "            LR.fit(X_train_fold.drop(columns = [\"ID\",\"pre_nucleus_id\",\"post_nucleus_id\"]),y_train_fold)\n",
    "            y_hat_valid = LR.predict(X_val_fold.drop(columns = [\"ID\",\"pre_nucleus_id\",\"post_nucleus_id\"]))\n",
    "            valid_acc = balanced_accuracy_score(y_val_fold, y_hat_valid)\n",
    "            fold_accuracy.append(valid_acc)\n",
    "            avg_train_time += elapsed\n",
    "            print(f\"C: {C_val}, penalty: {penalty_val} | valid accuracy for this fold, {valid_acc}\")\n",
    "        avg_fold_accuracy = sum(fold_accuracy)/len(fold_accuracy)\n",
    "        fold_std = np.std(fold_accuracy)\n",
    "        print(f\"avgfold accuracy: {avg_fold_accuracy}\")\n",
    "        print(f\"standard deviation: {fold_std}\")\n",
    "        print(f\"avg train time: {avg_train_time/5}\")\n",
    "        accuracies[(C_val, penalty_val)] = avg_fold_accuracy # store values\n",
    "        stdevs[(C_val, penalty_val)] = fold_std\n",
    "        times[(C_val, penalty_val)] = avg_train_time/5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame({'Param Pair':accuracies.keys(), 'Avg Accuracy':accuracies.values()})\n",
    "std_df = pd.DataFrame({'Param Pair':stdevs.keys(), 'Standard Dev':stdevs.values()})\n",
    "time_df = pd.DataFrame({'Param Pair':times.keys(), 'Training Time':times.values()})\n",
    "lr_hyperparams = acc_df.merge(std_df, on='Param Pair').merge(time_df, on='Param Pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Param Pair</th>\n",
       "      <th>Avg Accuracy</th>\n",
       "      <th>Standard Dev</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.1, l2)</td>\n",
       "      <td>0.739822</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(0.6, l2)</td>\n",
       "      <td>0.738911</td>\n",
       "      <td>0.021550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(1.1, l2)</td>\n",
       "      <td>0.738746</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(1.6, l2)</td>\n",
       "      <td>0.738376</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>(3.1, l2)</td>\n",
       "      <td>0.737822</td>\n",
       "      <td>0.022605</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>(3.6, l2)</td>\n",
       "      <td>0.737816</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>(2.6, l2)</td>\n",
       "      <td>0.737311</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>(2.1, l2)</td>\n",
       "      <td>0.737263</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>(4.1, l2)</td>\n",
       "      <td>0.737213</td>\n",
       "      <td>0.022479</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>(4.6, l2)</td>\n",
       "      <td>0.737211</td>\n",
       "      <td>0.022467</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>(5.1, l2)</td>\n",
       "      <td>0.736490</td>\n",
       "      <td>0.023058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>(5.6, l2)</td>\n",
       "      <td>0.736484</td>\n",
       "      <td>0.023059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>(6.1, l2)</td>\n",
       "      <td>0.736479</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>(8.6, l2)</td>\n",
       "      <td>0.736153</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19</td>\n",
       "      <td>(9.6, l2)</td>\n",
       "      <td>0.736142</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>(9.1, l2)</td>\n",
       "      <td>0.736128</td>\n",
       "      <td>0.021945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14</td>\n",
       "      <td>(7.1, l2)</td>\n",
       "      <td>0.736122</td>\n",
       "      <td>0.022879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>(7.6, l2)</td>\n",
       "      <td>0.736115</td>\n",
       "      <td>0.022848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13</td>\n",
       "      <td>(6.6, l2)</td>\n",
       "      <td>0.736113</td>\n",
       "      <td>0.022904</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>(8.1, l2)</td>\n",
       "      <td>0.735792</td>\n",
       "      <td>0.022702</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index Param Pair  Avg Accuracy  Standard Dev  Training Time\n",
       "0       0  (0.1, l2)      0.739822      0.021875            0.0\n",
       "1       1  (0.6, l2)      0.738911      0.021550            0.0\n",
       "2       2  (1.1, l2)      0.738746      0.022558            0.0\n",
       "3       3  (1.6, l2)      0.738376      0.021233            0.0\n",
       "4       6  (3.1, l2)      0.737822      0.022605            0.0\n",
       "5       7  (3.6, l2)      0.737816      0.022585            0.0\n",
       "6       5  (2.6, l2)      0.737311      0.022472            0.0\n",
       "7       4  (2.1, l2)      0.737263      0.022500            0.0\n",
       "8       8  (4.1, l2)      0.737213      0.022479            0.0\n",
       "9       9  (4.6, l2)      0.737211      0.022467            0.0\n",
       "10     10  (5.1, l2)      0.736490      0.023058            0.0\n",
       "11     11  (5.6, l2)      0.736484      0.023059            0.0\n",
       "12     12  (6.1, l2)      0.736479      0.023065            0.0\n",
       "13     17  (8.6, l2)      0.736153      0.021970            0.0\n",
       "14     19  (9.6, l2)      0.736142      0.021944            0.0\n",
       "15     18  (9.1, l2)      0.736128      0.021945            0.0\n",
       "16     14  (7.1, l2)      0.736122      0.022879            0.0\n",
       "17     15  (7.6, l2)      0.736115      0.022848            0.0\n",
       "18     13  (6.6, l2)      0.736113      0.022904            0.0\n",
       "19     16  (8.1, l2)      0.735792      0.022702            0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the 5 best parameters\n",
    "lr_hyperparams = lr_hyperparams.sort_values(by='Avg Accuracy', ascending=False).reset_index()\n",
    "lr_hyperparams.head(5)\n",
    "lr_hyperparams.to_csv('final_lr_hyperparams.csv')\n",
    "lr_hyperparams\n",
    "# hyperparameters for submitted models were saved to 'logreg_l2_C_cv.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 'l2')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best parameters\n",
    "lr_best_param = lr_hyperparams['Param Pair'][0]\n",
    "lr_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021874978597806946"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get Standard Deviation\n",
    "lr_hyperparams[\"Standard Dev\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_hyperparams[\"C\"] = lr_hyperparams.apply(get_param0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Logistic Regression: Tuning C')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(lr_hyperparams, x='C', y='Avg Accuracy')\n",
    "plt.title('Logistic Regression: Tuning C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on Query Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Balanced Accuracy on Query Set: 0.7506222669050479\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter= 200, C=lr_best_param[0], penalty='l2', solver='newton-cholesky')\n",
    "lr_model.fit(X_train_use.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1), y_train_use)\n",
    "\n",
    "preds = lr_model.predict(X_query.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "lr_accuracy = balanced_accuracy_score(y_query, preds)\n",
    "print(f\"Logistic Regression Balanced Accuracy on Query Set: {lr_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adp_dist</td>\n",
       "      <td>1.488055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>post_oracle</td>\n",
       "      <td>1.317316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pre_rf_x</td>\n",
       "      <td>0.883381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>post_nucleus_y</td>\n",
       "      <td>0.777680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pre_test_score</td>\n",
       "      <td>0.621819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>post_nucleus_x</td>\n",
       "      <td>0.476147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pre_nucleus_z</td>\n",
       "      <td>0.467766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RL_pre</td>\n",
       "      <td>0.449879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>area3</td>\n",
       "      <td>0.225926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>post_rf_y</td>\n",
       "      <td>0.225351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>post_skeletal_distance_to_soma</td>\n",
       "      <td>0.201238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pre_skeletal_distance_to_soma</td>\n",
       "      <td>0.198360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>axonal_coor_x</td>\n",
       "      <td>0.188446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dendritic_coor_y</td>\n",
       "      <td>0.169342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rf_distance</td>\n",
       "      <td>0.159499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pre_nucleus_y</td>\n",
       "      <td>0.153722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pre_rf_y</td>\n",
       "      <td>0.152049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>minicol_dist</td>\n",
       "      <td>0.143683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>post_test_score</td>\n",
       "      <td>0.141013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dendritic_coor_x</td>\n",
       "      <td>0.112480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>me_similarity</td>\n",
       "      <td>0.109464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>V1_pre</td>\n",
       "      <td>0.098587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RL_post</td>\n",
       "      <td>0.097221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AL_post</td>\n",
       "      <td>0.088338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dendritic_coor_z</td>\n",
       "      <td>0.079951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>nuclei_adp_dist</td>\n",
       "      <td>0.071715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pre_nucleus_x</td>\n",
       "      <td>0.068326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>post_nucleus_z</td>\n",
       "      <td>0.056067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>area2</td>\n",
       "      <td>0.054597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>area1</td>\n",
       "      <td>0.039627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>axonal_coor_y</td>\n",
       "      <td>0.036222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>V1_post</td>\n",
       "      <td>0.034115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AL_pre</td>\n",
       "      <td>0.029286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>axonal_coor_z</td>\n",
       "      <td>0.028692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fw_similarity</td>\n",
       "      <td>0.024883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>post_rf_x</td>\n",
       "      <td>0.020048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pre_oracle</td>\n",
       "      <td>0.011967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Features  Importances\n",
       "6                         adp_dist     1.488055\n",
       "13                     post_oracle     1.317316\n",
       "11                        pre_rf_x     0.883381\n",
       "21                  post_nucleus_y     0.777680\n",
       "10                  pre_test_score     0.621819\n",
       "20                  post_nucleus_x     0.476147\n",
       "19                   pre_nucleus_z     0.467766\n",
       "26                          RL_pre     0.449879\n",
       "34                           area3     0.225926\n",
       "16                       post_rf_y     0.225351\n",
       "7   post_skeletal_distance_to_soma     0.201238\n",
       "8    pre_skeletal_distance_to_soma     0.198360\n",
       "0                    axonal_coor_x     0.188446\n",
       "4                 dendritic_coor_y     0.169342\n",
       "25                     rf_distance     0.159499\n",
       "18                   pre_nucleus_y     0.153722\n",
       "12                        pre_rf_y     0.152049\n",
       "35                    minicol_dist     0.143683\n",
       "14                 post_test_score     0.141013\n",
       "3                 dendritic_coor_x     0.112480\n",
       "23                   me_similarity     0.109464\n",
       "28                          V1_pre     0.098587\n",
       "29                         RL_post     0.097221\n",
       "31                         AL_post     0.088338\n",
       "5                 dendritic_coor_z     0.079951\n",
       "36                 nuclei_adp_dist     0.071715\n",
       "17                   pre_nucleus_x     0.068326\n",
       "22                  post_nucleus_z     0.056067\n",
       "33                           area2     0.054597\n",
       "32                           area1     0.039627\n",
       "1                    axonal_coor_y     0.036222\n",
       "30                         V1_post     0.034115\n",
       "27                          AL_pre     0.029286\n",
       "2                    axonal_coor_z     0.028692\n",
       "24                   fw_similarity     0.024883\n",
       "15                       post_rf_x     0.020048\n",
       "9                       pre_oracle     0.011967"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_features = pd.DataFrame({\"Features\":X_train_use.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).columns, \n",
    "              \"Importances\":abs(lr_model.coef_[0])}).sort_values(by='Importances', ascending=False)\n",
    "lr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Logistic Regression Feature Importance')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_theme(palette='deep')\n",
    "plt.figure(dpi=250)\n",
    "g = sns.barplot(lr_features, y='Features', x='Importances', palette='mako')\n",
    "g.set_yticklabels(g.get_yticklabels(), fontsize=5)\n",
    "plt.title(\"Logistic Regression Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Accuracy Score: 0.7374963301796554\n",
      "Private Accuracy Score: 0.78139921176348\n"
     ]
    }
   ],
   "source": [
    "leaderboard_path = \"Data/leaderboard_data.csv\"\n",
    "sub_data = cleaner(leaderboard_path, feature_path, morph_path)\n",
    "sub_data = sub_data.select_dtypes('number')\n",
    "preds = lr_model.predict(sub_data.drop(columns=['ID', 'pre_nucleus_id', 'post_nucleus_id']).sort_index(axis=1))\n",
    "sub_data['connected_pred'] = preds==1\n",
    "submission_data = sub_data.filter(['ID','connected_pred'])\n",
    "submission_data.to_csv('final_submission_data_lr.csv',index=False)\n",
    "\n",
    "# evaluate against real leaderbaord data\n",
    "solutions = pd.read_csv('Data/solution_data.csv')\n",
    "leaderboard = pd.concat([solutions,submission_data], axis=1)\n",
    "public = leaderboard[leaderboard[\"Usage\"]==\"Public\"]\n",
    "print(\"Public Accuracy Score:\", balanced_accuracy_score(public[\"connected\"], public[\"connected_pred\"]))\n",
    "private = leaderboard[leaderboard[\"Usage\"]==\"Private\"]\n",
    "print(\"Private Accuracy Score:\", balanced_accuracy_score(private[\"connected\"], private[\"connected_pred\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
